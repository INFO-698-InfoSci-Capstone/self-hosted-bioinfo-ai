[
  {
    "processed_text": "Skip to content Overview ¶ HPC operates as a shared system with resources in high demand. Computational tasks must be executed as jobs on dedicated compute resources. These resources are granted to each user for a limited time per session, and sessions are organized by Slurm , an open source, fault-tolerant, and highly scalable cluster manager and task scheduler. Users can interact with Slurm via an internet browser through Open OnDemand , or via an SSH connection to one of the login nodes to start an interactive job or submit a batch job . Do not run jobs on Login Nodes A login node serves as a staging area where you can perform housekeeping work, edit scripts, and submit job requests for execution on one/some of the cluster’s compute nodes. It is important to know that the login nodes are not the location where scripts are run . Heavy computation on the login nodes slows the system down for all users and will not give you the resources or performance you need. Additionally, modules are not available on the login nodes. Tasks run on the login nodes that impact usability will be identified and cancelled by HPC infrastructure without warning. New to HPC? ¶ If you are new to the UArizona HPC system, or to HPC systems in general, we recommend reviewing our quick start guide before getting into the details of running jobs. You may also want to take a look at our workshops which cover topics including introduction to HPC , parallel computing , and containers , among other topics. Best Practices ¶ When creating a job request, please keep the following in mind: Don't ask for more resources than you really need. The scheduler will have an easier time finding a slot for the two hours you need rather than the 48 hours you request. When you run a job it will report back on the time used which you can use as a reference for future jobs. However don't cut the time too tight. If something like shared I/O activity slows it down and you run out of time, the job will fail. Additionally, please do not request more CPUs than you plan to use. Increasing the number of CPUs without a clear plan of how your software is going to use them will not result in faster computation. Unused CPUs in a job represent wasted system resources, and will cost more CPU-hours to your allocation than you actually needed to spend. Test your submission scripts. Start small. You can use an interactive session to help build your script and run tests in real time. Respect memory limits. If your application needs more memory than is available, your job could fail and leave the node in a state that requires manual intervention. The failure messages might reference '''OOM''' which means '''Out Of Memory''' Do not run loops automating a large number of job submissions. Executing large numbers of job submissions in rapid succession (e.g. in a scripted loop) can overload the system's scheduler and cause problems with overall system performance. Small numbers of automated jobs may be acceptable (e.g. less than 100), but a better alternative in almost all cases is to use job arrays instead. Hyperthreading is turned off. Running multiple threads per core is generally not productive. MKL is an exception to that if it is relevant to your workflow. Instead, running one thread per core and using multiple cores (i.e. \"multiprocessing\" rather than \"multithreading\") is a suggested alternative. Open OnDemand Usage Please be mindful of other users' needs and avoid monopolizing resources for extended periods when they are not actively being utilized. In practice, this means actively terminating sessions when you are finished rather than leaving the session open. Closing the browser tab does not terminate the session. This ensures fair access for all members of our community and promotes a collaborative environment, and ensures you are only charged for the time you actually used. Frequently Asked Questions ¶ Below is a FAQ that includes answers to common questions and misconceptions about running jobs on HPC. Some are general information about HPC systems, but some contain information specific to the UArizona HPC. We recommend reviewing this FAQ whether you are a new user getting started on our system, or an experienced user returning to our documentation. What is a Job? A single instance of allocated computing resources is known as a Job. Jobs can be in the format of a graphical Open OnDemand application instance, interactive terminal session, or batch submission (a script scheduled for execution at a later time), and require an active allocation of CPU-time under an associated PI account. See Time Allocations for more information. How much am I charged for a job? Each PI group is given a free monthly allocation of CPU hours on each cluster (see Time Allocations for more information). The amount charged to this allocation is equal to the number of CPUs used for a job multiplied by its total run time in hours. How many jobs can I run? Each user can submit a maximum of 1000 jobs per cluster, and there are further limits on memory, CPUs, and GPUs concurrently used per group. More information is available at Job Limits . If you intend to submit a large number of jobs, especially if they are similar, they should be submitted as array jobs . How much memory can I request? The total amount of memory assigned to a job depends on the number of CPUs assigned to a job. Memory is physically mounted to the CPUs, so there is a fixed amount available to the job equal to the memory of a single CPU multiplied by the number of CPUs. For example, if there is 64GB of memory and 16 cores the ratio is always 4GB per core. You can ask for less, but never more. What compute resources are available on each cluster? We have three clusters available for use: Puma, Ocelote, and El Gato. See Compute Resources for details on each. Are the three clusters Puma, Ocelote, and El Gato interchangeable? Generally no. Puma is our newest cluster and runs on the operating system Rocky Linux 9. Ocelote and El Gato are older and run on CentOS 7. Software compiled on Puma may not be compatible with the older clusters and vice versa. Because Ocelote and El Gato run on the same OS and share the same system libraries and software modules, workflows can generally run on either. I submitted a job request, but it isn't running yet. Why? Job requests go to our task scheduler, Slurm, which manages all incoming job requests and determines the optimal order to run them in order to maximize the efficiency of the system. The amount of wait time for your job depends on the number and size of jobs before your job in the queue; the amount of compute resources requested by your job; and the requested wall time for your job. Variation in wait times is a natural consequence of this system. Typically, our most advanced cluster, Puma, experiences longer wait times due to increased usage. Smaller or interactive jobs are recommended to run on Ocelote or El Gato. Why is my job running slower on the HPC than on my laptop? The strongest use cases for HPC are either running more jobs or larger jobs than your laptop. The advantages come from running jobs in parallel or parallelizing your code. A job that runs well on your laptop could be slower on HPC because you are waiting for other jobs to end or you haven't taken advantage of the parallel features of HPC. See Parallelization for more info. I need some software to run my analysis, can you install it for me? First, check our list of installed software by accessing an interactive terminal session , then using module commands to check for your software. If you do not see your desired software package there, then there are two options. You can attempt to install the software for yourself in one of your own directories. Or you can request HPC Consult to install the software as a system module. Many programs can be installed with option 1 by users without root privileges, especially packages that are available via package managers like Pip, Conda, and CRAN. Most software like this does not make sense to install as a system module since it is generally easy to install on a per-user basis. Other software that is not available through package managers might be able to be installed by downloading and compiling the source code in one of your folders. The complexity of this process varies greatly from software to software, so if you run into any trouble while doing this, feel free to contact HPC Consulting for support. For information on what's appropriate to install as a system module, see our software policies guide . Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/overview/",
      "title": "Overview - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "ba0afa27-44b4-4f2d-a9b1-276f8a0610bf",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/overview/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content CPUs and Memory ¶ CPU and Memory Correlation ¶ Before submitting your job to the scheduler, it's important to know that the number of CPUs you allocate to your job determines the amount of memory you receive. Each cluster has a fixed amount of memory per CPU based on the node type. Accepted values by cluster and node type are listed below: | Cluster | Standard Node | High-Memory Node | GPU Node | | --- | --- | --- | --- | | Puma | 5 GB | 32 GB | 5 GB | | Ocelote | 6 GB | 41 GB | 8 GB | | El Gato | 4 GB | - | - | For example, using the table above we can see on Puma standard nodes you get 5 GB for each CPU you request. This means a standard job using 4 CPUs gets 5 GB/CPU × 4 CPUs = 20 GB of total memory. The video below shows the relationship between memory and CPUs, specifically looking at one of our Puma nodes. HPC Core to Memory Relationship - YouTube University of Arizona UITS Research Technologies 98 subscribers HPC Core to Memory Relationship University of Arizona UITS Research Technologies Search Info Shopping Tap to unmute If playback doesn't begin shortly, try restarting your device. You're signed out Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer. CancelConfirm Share Include playlist An error occurred while retrieving sharing information. Please try again later. Watch later Share Copy link Watch on 0:00 0:00 / 5:36•Live • Watch on YouTube Determining Job Resources ¶ The following flowchart describes the process of determining the amount of memory and number of CPUs to allocate to a job. For simplicity, this shorthand will be used: N : number of CPUs desired M : total memory desired MpC : the default value of memory per CPU for fixed cluster and node type as listed in the table above If a decimal value is encountered, round up in all cases. The dotted line above indicates that setting mem/CPU in your job is not generally necessary. If you are requesting a standard node, this value is set for you by the scheduler. The only times you will need to set this value is: If you're requesting a non-standard node (e.g. a high memory or Ocelote GPU node) If you're requesting an OnDemand application session. There is a field where you will fill in your mem/CPU requirement. Note that there is no deterministic method of finding the exact amount of memory needed by a job in advance . A general rule of thumb is to overestimate it slightly and then scale down based on previous runs. Significant overestimation, however, can lead to inefficiency of system resources and unnecessary expenditure of CPU time allocations. Things to Watch out for ¶ Be careful when requesting memory and memory per CPU. Note that if you request invalid mem/cpu values, unpredictable results may occur: Memory and CPU Mismatches In batch scripts, if you request --memory instead of --mem-per-cpu , the scheduler will automatically increase your CPU allocation to align with your memory requirements in case of a mismatch. For instance, if you request one CPU and 50 GB of total memory, the scheduler will adjust your resource requirements by allocating 10 CPUs to match your memory request. Invalid Mem/CPU Options If you request a mem/CPU value that isn't valid, the scheduler won't outright reject your job. Instead, it will attempt to accommodate your request. This could involve your job being moved to a high memory node if you ask for more than the standard ratio. However, high memory nodes usually have considerably longer wait times compared to standard nodes, potentially resulting in a longer queue time than anticipated. Alternatively, you might end up with less memory allocated than you expected. For instance, there aren't any machines with a memory ratio exceeding 41 GB/CPU. Therefore, if you request 100 GB/CPU, your job will still be constrained by the physical limits of available memory. Was this page informative?",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/cpus_and_memory/",
      "title": "CPUs and Memory - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "0861f67c-961a-4b1d-92e6-2bc988451663",
      "viewport": [
        "width=device-width,initial-scale=1",
        "width=device-width, initial-scale=1"
      ],
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/cpus_and_memory/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Interactive Jobs ¶ Overview ¶ Interactive sessions are a way to gain access to a compute node from the command line. This is useful for checking and using available software modules , testing submission scripts , debugging code, compiling software, and running programs in real time. The term \"interactive session\" in this context refers to jobs run from within the command line on a terminal client. Opening a terminal in an interactive graphical desktop is also equivalent, but these sessions are fixed to the resources allocated to that Open OnDemand (OOD) session. As you'll see below, one has more control over their resources when requesting an interactive session via SSH in a terminal client. Clusters ¶ An interactive session can be requested on any of our three clusters: El Gato, Ocelote, and Puma. Ocelote and El Gato share the same operating system (CentOS 7), system libraries, and software modules. El Gato typically has shorter wait times, so if you're compiling software or running small test jobs for workflows to run on either El Gato or Ocelote, it may be advantageous to request a session on El Gato for more immediate access. Puma runs a newer operating system (Rocky Linux 9), has newer system libraries, and its own software modules. If you are using Puma for your production work, you will want to stick to requesting interactive sessions on Puma for testing and compiling. Workflows may not be transferrable between the older clusters and Puma, so make sure to check which software ecosystem you're using to ensure the most predictable results. How to Request an Interactive Session ¶ The interactive Command ¶ We have a built-in shortcut command that will allow you to quickly and easily request a session by simply entering: interactive The interactive command is essentially a convenient wrapper for a Slurm command called salloc . This can be thought of as similar to the sbatch command, but for interactive jobs rather than batch jobs . When you request a session using interactive, the full salloc command being executed will be displayed for reference. ``` (ocelote) [netid@junonia ~]$ interactive Run \"interactive -h for help customizing interactive use\" Submitting with /usr/local/bin/salloc --job-name=interactive --mem-per-cpu=4GB --nodes=1 --ntasks=1 --time=01:00:00 --account=windfall --partition=windfall salloc: Pending job allocation 531843 salloc: job 531843 queued and waiting for resources salloc: job 531843 has been allocated resources salloc: Granted job allocation 531843 salloc: Waiting for resource configuration salloc: Nodes i16n1 are ready for job [netid@i16n1 ~]$ ``` Notice in the example above how the command prompt changes once your session starts. When you're on a login node, your prompt will show junonia or wentletrap . Once you're in an interactive session, you'll see the name of the compute node you're connected to. Customizing Your Resources The command interactive when run without any arguments will allocate a windfall session using one CPU for one hour which isn't ideal for most use cases. You can modify this by including additional flags. To see the available options, you can use the help flag -h ``` (ocelote) [netid@junonia ~]$ interactive -h Usage: /usr/local/bin/interactive [-x] [-g] [-N nodes] [-m memory per core] [-n total number of tasks] [-Q optional qos] [-t hh::mm:ss] [-a account to charge] ``` The values shown in the output can be combined and each mean the following: | Flag | Explanation | Example | | --- | --- | --- | | -a | This is followed by your group's name and will switch you to using the standard partition. This is highly recommended to keep your sessions from being interrupted and to help them start faster. | -a my_group | | -t | The amount of time to reserve for your job in the format hhh:mm:ss . | -t 05:00:00 | | -n | Total number of tasks (CPUs) to allocate to your job. By default, these CPUs will be allocated on a single node. | -n 16 | | -N | Total number of nodes (physical computers) to allocate to your job. | -N 2 | | -m | Total amount of memory per CPU. See CPUs and Memory for more details and information on potential complications. | -m 5gb | | -Q | Used to access high priority or qualified hours. Only for groups with buy-in or special project hours . | High Priority: -Q user_qos_<PI NETID> Qualified: -Q qual_qos_<PI NETID> | | -g | Request one GPU. This flag takes no arguments. On Puma, you may be allocated either a v100 or a MIG slice . If you want more control over your resources, you can use salloc directly using GPU batch directives . | -g | | -x | Enable X11 forwarding . This flag takes no arguments. | -x | You may also create your own salloc commands using any desired Slurm directives for maximum customization. The salloc Command ¶ If interactive is insufficient to meet your resource requirements (e.g., if you need to request more than one GPU or a GPU MIG slice), you can use the Slurm command salloc to further customize your job. The command salloc expects Slurm directives as input arguments that it uses to customize your interactive session. For comprehensive documentation on using salloc , see Slurm's official documentation . Single CPU Example ``` salloc --account= --partition=standard --nodes=1 --ntasks=1 --time=1:00:00 --job-name=interactive ``` Single Node, Multi-CPU Example ``` salloc --account= --partition=standard --nodes=1 --ntasks=16 --time=1:00:00 --job-name=interactive ``` Multi-GPU Example (Puma) ``` salloc --account= --partition=gpu_standard --nodes=1 --ntasks=1 --time=1:00:00 --job-name=multi-gpu --gres=gpu:volta:2 ``` GPU MIG Slice Example ``` salloc --account= --partition=gpu_standard --nodes=1 --ntasks=1 --time=1:00:00 --job-name=mig-gpu --gres=gpu:nvidia_a100_80gb_pcie_2g.20gb ``` Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/interactive_jobs/",
      "title": "Interactive Jobs - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "18c18e90-d8f7-4b44-87a1-f88dfd1a62e9",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/interactive_jobs/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Slurm Environment Variables ¶ Every Slurm job has environment variables that are set by default. These can be used in a job to control job behavior, for example setting the names of output files or directories, setting CPU count for multithreaded processes, controlling input parameters, etc. | Variable | Purpose | Example Value | | --- | --- | --- | | $SLURM_ARRAY_JOB_ID | Job array's parent ID | 399124 | | $SLURM_ARRAY_TASK_COUNT | Total number of subjobs in an array | 4 | | $SLURM_ARRAY_TASK_ID | Job index number (unique for each job in an array) | 1 | | $SLURM_ARRAY_TASK_MAX | Maximum index for the job array | 7 | | $SLURM_ARRAY_TASK_MIN | Minimum index for the job array | 1 | | $SLURM_ARRAY_TASK_STEP | Job array's index step size | 2 | | $SLURM_CLUSTER_NAME | Which cluster your job is running on | elgato | | $SLURM_CONF | Points to the Slurm configuration file | /var/spool/slurm/d/conf-cache/slurm.conf | | $SLURM_CPUS_ON_NODE | Number of CPUs allocated to target node | 3 | | $SLURM_GPUS_ON_NODE | Number of GPUs allocated to the target node | 1 | | $SLURM_GPUS_PER_NODE | Number of GPUs per node. Only set if --gpus-per-node is specified | 1 | | $SLURM_JOB_ACCOUNT | Account being charged | groupname | | $SLURM_JOB_GPUS | The global GPU IDs of the GPUs allocated to the job. Only set in batch and interactive jobs. | 0 | | $SLURM_JOB_ID | Your Slurm Job ID | 399072 | | $SLURM_JOB_CPUS_PER_NODE | Number of CPUs per node. This can be a list if there is more than one node allocated to the job. The list has the same order as SLURM_JOB_NODELIST | 3,1 | | $SLURM_JOB_NAME | The job's name | interactive | | $SLURM_JOB_NODELIST | The nodes that have been assigned to your job | gpu[73-74] | | $SLURM_JOB_NUM_NODES | The number of nodes allocated to the job | 2 | | $SLURM_JOB_PARTITION | The job's partition | standard | | $SLURM_JOB_QOS | The job's QOS/Partition | qos_standard_part | | $SLURM_JOB_USER | The username of the person who submitted the job | netid | | $SLURM_JOBID | Same as SLURM_JOB_ID , your Slurm Job ID | 399072 | | $SLURM_MEM_PER_CPU | The memory/CPU ratio allocated to the job | 4096 | | $SLURM_NNODES | Same as SLURM_JOB_NUM_NODES – the number of nodes allocated to the job | 2 | | $SLURM_NODELIST | Same as SLURM_JOB_NODELIST , The nodes that have been assigned to your job | gpu[73-74] | | $SLURM_NPROCS | The number of tasks allocated to your job | 4 | | $SLURM_NTASKS | Same as SLURM_NPROCS , the number of tasks allocated to your job | 4 | | $SLURM_SUBMIT_DIR | The directory where sbatch was used to submit the job | /home/u00/netid | | $SLURM_SUBMIT_HOST | The hostname where sbatch was used to submit the job | wentletrap.hpc.arizona.edu | | $SLURM_TASKS_PER_NODE | The number of tasks to be initiated on each node. This can be a list if there is more than one node allocated to the job. The list has the same order as SLURM_JOB_NODELIST | 3,1 | | $SLURM_WORKING_CLUSTER | Valid for interactive jobs, will be set with remote sibling cluster's IP address, port and RPC version so that any sruns will know which cluster to communicate with. | elgato:foo:0000:0000:000 | Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/batch_jobs/environment_variables/",
      "title": "Environment Variables - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "0fa877b8-bdfe-4484-ab0d-faec3ed14a55",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/batch_jobs/environment_variables/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Essential Ideas To get the most out of ParaView, it is essential to understand not just the steps of doing things but also why those steps work. This will help you in debugging many of the issues you run across while using the program. Nodes ¶ Pretty much all of ParaView is structured in a system of nodes stored in the pipeline (see this with View → Pipeline Browser from the toolbar). Nodes are sorted into three main categories: Sources Sources are, as the name implies, the source for the ParaView pipeline. This is anything that you want to bring into your visualization including data, geometric shapes, annotations, etc. In short, these are the objects that all the other nodes base themselves on. Filters Filters are the main tool of analysis in ParaView. Anytime you want to add something to your data (such as showing velocities as arrows), modify your data in some way (like projecting onto an axis), or visualize with a graph, these are the nodes to use. Extractors Extractors, unlike sources or filters, do not add or modify data in any way. They simply act as a means of taking the data that you already have in ParaView and exporting it. Examples of this include saving images or exporting data as a spreadsheet . The general workflow for ParaView is importing your data with sources (this is what the Open button does), performing your analysis with filters, then exporting it either as an image or spreadsheet for further analysis. The Pipeline ¶ The pipeline is where all of your nodes will be stored. It is stored in a tree-like format, with the root of the tree always being builtin. Generally, sources and extractors will be children of the root and filters will be children of sources. Any filters that a source has as a child will affect that source object. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/essential_ideas/",
      "title": "Essential Ideas - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "1a3f6449-ec81-48cd-b0d3-3194fcfc4c93",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/essential_ideas/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content SLURM Job Dependencies Example ¶ Click here to download the example Overview ¶ Sometimes projects need to be split up into multiple parts where each step is dependent on the step (or several steps) that came before. SLURM dependencies are a way to automate this process. In this example, we'll create a number of three-dimensional plots using Python and will combine them into a gif as the last step. A job dependency is a good solution in this case since the job that creates the gif is dependent on all the images being present. Data structure ¶ We'll try to keep things in order by partitioning our data, output, and images in distinct directories. These directories and files can be downloaded by clicking the button at the top of the page. ``` (elgato) [user@wentletrap volcano]$ tree . ├── create_gif.slurm ├── data │ └── volcano.csv ├── generate_frames.slurm ├── images ├── output │ ├── archives │ └── slurm_files ├── submit-gif-job └── volcano.py ``` Scripts ¶ Python script ¶ The Python example script was pulled and modified from the Python graph gallery and the CSV file used to generate the image was downloaded from: https://raw.githubusercontent.com/holtzy/The-Python-Graph-Gallery/master/static/data/volcano.csv Below you'll notice one modification to the original script: n = int(sys.argv[1]) . We're going to execute this in an array job and will be importing the array indices (integers n where 1 ≤ n ≤ 360 ) into this script as arguments so that we can manipulate the viewing angle ( ax.view_init(30, 45 + n) ). Each frame will be slightly different and, when combined into a gif, will allow us to execute a full rotation of the 3D volcano plot. ``` !/usr/bin/env python3 from mpl_toolkits.mplot3d import Axes3D import matplotlib.pyplot as plt import pandas as pd import seaborn as sns import sys n = int(sys.argv[1]) # <- We'll import an array index to rotate our image Original example from: https://www.python-graph-gallery.com/3d/ CSV available from : https://raw.githubusercontent.com/holtzy/The-Python-Graph-Gallery/master/static/data/volcano.csv data = pd.read_csv(\"data/volcano.csv\") Transform it to a long format df=data.unstack().reset_index() df.columns=[\"X\",\"Y\",\"Z\"] And transform the old column name in something numeric df['X']=pd.Categorical(df['X']) df['X']=df['X'].cat.codes Make the plot fig = plt.figure() ax = fig.add_subplot(projection='3d') ax.set_axis_off() ax.plot_trisurf(df['Y'], df['X'], df['Z'], cmap=plt.cm.viridis, linewidth=0.2) ax.view_init(30, 45 + n) plt.savefig('images/image%s.png'%n,format='png',transparent=False) ``` Slurm Script to Generate Images ¶ This is the job where we will generate all of our images. In each step, we will pass our array index to our python script to determine the viewing angle of our plot. Script: generate_frames.slurm ``` !/bin/bash SBATCH --account=hpcteam SBATCH --partition=standard SBATCH --nodes=1 SBATCH --ntasks=1 SBATCH --time=00:05:00 SBATCH --job-name=generate_frames SBATCH -o output/slurm_files/%x-%A.out SBATCH -e output/slurm_files/%x-%A.err SBATCH --open-mode=append SBATCH --array=1-360 python3 volcano.py $SLURM_ARRAY_TASK_ID ``` SLURM script to combine frames into gif ¶ Once each frame has been generated, we'll use ffmpeg to combine our images into a gif and will clean up our workspace. The bash script shown in the next section is what will ensure that this script isn't run until the array has completed. Script: create_gif.slurm ``` !/bin/bash SBATCH --account=hpcteam SBATCH --partition=standard SBATCH --nodes=1 SBATCH --ntasks=1 SBATCH --time=00:10:00 SBATCH --job-name=make_gif SBATCH -o output/slurm_files/%x-%j.out SBATCH -e output/slurm_files/%x-%j.err module load ffmpeg ffmpeg -framerate 25 -i $PWD/images/image%d.png -r 30 -b 5000k volcano.mp4 ffmpeg -i volcano.mp4 -loop 0 -vf scale=400:240 volcano.gif rm volcano.mp4 cd images DATE_FORMAT=$(date +%m-%d-%Y.%H:%M:%S) tar czvf volcano-images-${DATE_FORMAT}.tar.gz image .png mv tar.gz ../output/archives rm -rf ./*.png ``` Script to Automate Job Submissions ¶ This simple bash script is what implements the SLURM job dependency magic. Each step is described in detail below. Script: submit-gif-job ``` !/bin/bash printf \"Submitting job to generate images\\n\" jobid=$(sbatch --parsable generate_frames.slurm) printf \"Job submitted with ID: $jobid\\n\\n\" printf \"Submitting job dependency. Combines images into a gif\\n\" sbatch --dependency=afterany:$jobid create_gif.slurm ``` Step-by-step: ¶ 1) jobid=$(sbatch --parsable generate_frames.slurm) In this case, we're capturing the job ID output from our array submission. Typically, when you submit a SLURM job without arguments, you get back something that looks like: ``` (elgato) [user@wentletrap ~]$ sbatch example.slurm Submitted batch job 448243 ``` The parsable option is what reduces this to simply the job ID and allows us to easily capture it: ``` (elgato) [user@wentletrap ~]$ sbatch --parsable example.slurm 448244 ``` As a general comment, when you run something like: ``` VAR=$(command) ``` You are running command and setting the variable VAR to the output. In the specifc case of our bash script, we've set the bash variable jobid to the output of our sbatch --parsable command. 2) sbatch --dependency=afterany:$jobid create_gif.slurm Now that we have the Job ID, we'll submit the next job with a dependency flag: --dependency=afterany:$jobid . The dependency option tells the scheduler that this job should not be run until the job with Job ID $jobid has completed. The afterany specifies that the exit status of the previous job does not matter. Other options are afterok (meaning only execute the dependent job if the previous job ended successfully) or afternotok (meaning only execute if the previous job terminated abnormally, e.g. was cancelled or failed). You might consider setting up multiple job dependencies that depend on the previous job's exit status. Submitting the Jobs ¶ Once we've gotten everything set up, it's time to execute our workflow. We can check our jobs once we've run our bash script. In this case, while the array job used to generate the different image frames is running, the make_gif job will sit in queue with the reason (Dependency) indicating that it is waiting to run until its dependency has been satisfied. ``` (elgato) [user@wentletrap volcano]$ bash submit-gif-job Submitting job to generate images Job submitted with ID: 447878 Submitting job dependency. Combines images into a gif file Submitted batch job 447879 (elgato) [user@wentletrap volcano]$ squeue --user user JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 447878_[9-360] standard generate user PD 0:00 1 (None) 447879 standard make_gif user PD 0:00 1 (Dependency) 447878_1 standard generate user R 0:02 1 cpu16 447878_2 standard generate user R 0:02 1 cpu37 447878_3 standard generate user R 0:02 1 cpu37 447878_4 standard generate user R 0:02 1 cpu37 447878_5 standard generate user R 0:02 1 cpu37 447878_6 standard generate user R 0:02 1 cpu37 447878_7 standard generate user R 0:02 1 cpu37 447878_8 standard generate user R 0:02 1 cpu37 ``` Once the job has completed, you should see something that looks like the following structure with output files: ``` (elgato) [user@wentletrap volcano]$ tree . ├── create_gif.slurm ├── data │ └── volcano.csv ├── generate_frames.slurm ├── images ├── output │ ├── archives │ │ └── volcano-images-10-25-2022.12:52:19.tar.gz │ ├── gifs │ │ └── volcano.gif │ └── slurm_files │ ├── generate_frames-447878.err │ ├── generate_frames-447878.out │ ├── make_gif-447879.err │ └── make_gif-447879.out ├── submit-gif-job └── volcano.py 6 directories, 11 files ``` Output ¶ If everything is successful, there should be a gif of a rotating volcano under ./output/gifs/volcano.gif Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/batch_jobs/job_dependencies/",
      "title": "Job Dependencies - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "1cb3da35-8772-4c5a-8180-240118d9df13",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/batch_jobs/job_dependencies/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Paraview Python Plugins If you've spent more than a minute with ParaView you're already aware that there are some things that could use a little more explanation. There's really just too much to cover in the depth that most beginners/intermediate users need. This page is meant to help shed a little bit of light on another subject that is shrouded in a deep layer of mystery, namely making plugins with Python. Unfortunately, this tutorial is not without sections that are less well understood, but hopefully in between those sections there's some value! For many purposes it might be useful to look through examples of scripts and plugins, so for access to an ever evolving scratch space of these kinds of files please refer to https://github.com/DevinBayly/paraview_code . Before going to much further just know this page will not document or explain things like the decoration options that create widgets for filter options. In fact, there are quite a few things related to the @ -prefixing of methods that won't be discussed until a later date. Due to this limitation it may be useful to layout upfront that much of the true instructional material is captured in comments on the source code from https://github.com/Kitware/ParaView/blob/cdb6fa76693885dd14af49d91d9b6177710f7267/Examples/Plugins/PythonAlgorithm/PythonAlgorithmExamples.py , and more generally from https://github.com/Kitware/ParaView/tree/cdb6fa76693885dd14af49d91d9b6177710f7267/Examples/Plugins . Example Plugin ¶ https://github.com/DevinBayly/paraview_code/blob/main/time_series_reader_plugin.py This plugin was designed to make it possible to point ParaView to a folder of Python NumPy array files that each store a point cloud at a particular moment in time. The benefit of using this plugin is that now in ParaView as we scrub through a timeline interface element the plugin will load new NumPy files and update the viewport. The sections that follow will attempt to explain what the different methods of the plugin do, and this will ideally provide departure points for reader's own usecases. First the entire source is shown and then the step by step sections will follow. ``` \"\"\"This module demonstrates various ways of adding VTKPythonAlgorithmBase subclasses as filters, sources, readers, and writers in ParaView\"\"\" This is module to import. It provides VTKPythonAlgorithmBase, the base class for all Python-based vtkAlgorithm subclasses in VTK and decorators used to 'register' the algorithm with ParaView along with information about UI. from paraview.util.vtkAlgorithm import * import uuid from pathlib import Path ------------------------------------------------------------------------------ A reader example. ------------------------------------------------------------------------------ def createModifiedCallback(anobject): import weakref weakref_obj = weakref.ref(anobject) anobject = None def _markmodified( args, *kwars): o = weakref_obj() if o is not None: o.Modified() return _markmodified To add a reader, we can use the following decorators @smproxy.source(name=\"PythonnumpyReader\", label=\"Python-based numpy Reader\") @smhint.xml(\"\"\" \"\"\") or directly use the \"@reader\" decorator. @smproxy.reader(name=\"Sama Lidar Numpy Reader\", label=\"Python-based Numpy pcd Reader for timeseries data\", extensions=\"npy\", file_description=\"Numpy files\") class PythonNumpyPCDReader(VTKPythonAlgorithmBase): \"\"\"A reader that reads a NumPy file. If the NumPy has a \"time\" column, then the data is treated as a temporal dataset\"\"\" def init (self): VTKPythonAlgorithmBase. init (self, nInputPorts=0, nOutputPorts=1, outputType='vtkPolyData') self._filename = None self._ndata = None self._timesteps = None print(\"starting\",uuid.uuid1()) from vtkmodules.vtkCommonCore import vtkDataArraySelection self._arrayselection = vtkDataArraySelection() self._arrayselection.AddObserver(\"ModifiedEvent\", createModifiedCallback(self)) def _get_raw_data(self, requested_time=None): import numpy if self._ndata is not None: if requested_time is not None: ##### load specific npy file from fnmes fname = self.fnames[int(requested_time)] self._ndata = numpy.load(fname) print(self._ndata.dtype) # self._ndata.dtype = numpy.dtype([(\"x\",numpy.float32),(\"y\",numpy.float32),(\"z\",numpy.float32),(\"intensity\",numpy.float32)]) return self._ndata return self._ndata if self._filename is None: # Note, exceptions are totally fine! raise RuntimeError(\"No filename specified\") # self._ndata = numpy.genfromtxt(self._filename, dtype=None, names=True, delimiter=',', autostrip=True) self.pth = Path(self._filename) self.fnames = list(self.pth.parent.rglob(\"*npy\")) self.fnames.sort() times = [i for i,e in enumerate(self.fnames)] self._ndata = 0 self._timesteps = times return self._get_raw_data(requested_time) def _get_timesteps(self): self._get_raw_data() return self._timesteps if self._timesteps is not None else None def _get_update_time(self, outInfo): executive = self.GetExecutive() timesteps = self._get_timesteps() if timesteps is None or len(timesteps) == 0: return None elif outInfo.Has(executive.UPDATE_TIME_STEP()) and len(timesteps) > 0: utime = outInfo.Get(executive.UPDATE_TIME_STEP()) print(\"using inner method get update time\",utime) dtime = timesteps[0] for atime in timesteps: if atime > utime: return dtime else: dtime = atime return dtime else: assert(len(timesteps) > 0) return timesteps[0] def _get_array_selection(self): return self._arrayselection @smproperty.stringvector(name=\"FileName\") @smdomain.filelist() @smhint.filechooser(extensions=\"npy\", file_description=\"Numpy pcd file\") def SetFileName(self, name): \"\"\"Specify filename for the file to read.\"\"\" print(name) if self._filename != name: self._filename = name self._ndata = None self._timesteps = None self.Modified() @smproperty.doublevector(name=\"TimestepValues\", information_only=\"1\", si_class=\"vtkSITimeStepsProperty\") def GetTimestepValues(self): print(\"getting time steps\") return self._get_timesteps() # Array selection API is typical with readers in VTK # This is intended to allow ability for users to choose which arrays to # load. To expose that in ParaView, simply use the # smproperty.dataarrayselection(). # This method **must** return a `vtkDataArraySelection` instance. @smproperty.dataarrayselection(name=\"Arrays\") def GetDataArraySelection(self): return self._get_array_selection() def RequestInformation(self, request, inInfoVec, outInfoVec): print(\"requesting information\") executive = self.GetExecutive() outInfo = outInfoVec.GetInformationObject(0) outInfo.Remove(executive.TIME_STEPS()) outInfo.Remove(executive.TIME_RANGE()) timesteps = self._get_timesteps() if timesteps is not None: for t in timesteps: outInfo.Append(executive.TIME_STEPS(), t) outInfo.Append(executive.TIME_RANGE(), timesteps[0]) outInfo.Append(executive.TIME_RANGE(), timesteps[-1]) return 1 def RequestData(self, request, inInfoVec, outInfoVec): print(\"requesting data\") from vtkmodules.vtkCommonDataModel import vtkPolyData from vtkmodules.numpy_interface import dataset_adapter as dsa import vtk data_time = self._get_update_time(outInfoVec.GetInformationObject(0)) output = dsa.WrapDataObject(vtkPolyData.GetData(outInfoVec, 0)) points = self._get_raw_data(data_time) #points = self._ndata vpoints = vtk.vtkPoints() vpoints.SetNumberOfPoints(points.shape[0]) intensity = vtk.vtkFloatArray() intensity.SetNumberOfComponents(1) intensity.SetName(\"Intensity\") intensity.SetNumberOfTuples(points.shape[0]) for i in range(points.shape[0]): vpoints.SetPoint(i, points[i][:3]) intensity.SetTuple1(i, points[i][3]) output.GetPointData().SetScalars(intensity) output.SetPoints(vpoints) vcells = vtk.vtkCellArray() for i in range(points.shape[0]): vcells.InsertNextCell(1) vcells.InsertCellPoint(i) output.SetVerts(vcells) if data_time is not None: output.GetInformation().Set(output.DATA_TIME_STEP(), data_time) return 1 ``` Breaking this down we will begin with a helper function from the top. ``` \"\"\"This module demonstrates various ways of adding VTKPythonAlgorithmBase subclasses as filters, sources, readers, and writers in ParaView\"\"\" This is module to import. It provides VTKPythonAlgorithmBase, the base class for all Python-based vtkAlgorithm subclasses in VTK and decorators used to 'register' the algorithm with ParaView along with information about UI. from paraview.util.vtkAlgorithm import * import uuid from pathlib import Path ------------------------------------------------------------------------------ A reader example. ------------------------------------------------------------------------------ def createModifiedCallback(anobject): import weakref weakref_obj = weakref.ref(anobject) anobject = None def _markmodified( args, *kwars): o = weakref_obj() if o is not None: o.Modified() return _markmodified ``` Starting off with a bit of a still unclear boilerplate helper function. Best guess is this function is used later in the class to make sure that ParaView tracks that our object (NumPy point cloud) has been modified, and the downstream rendering updates need to happen. This may be because ParaView has a lazy pipeline evaluation implementation, where filters in the processing pipeline only evaluate when changes have occurred. Now lets inspect the beginning of the actual class defining the NumPy array reader plugin. So this class can be named whatever we like, but it must inherit from the Python algorithm base. In my case I've filled in some of the values in the header so that the reader works with \"npy\" extension files. As with other inherited classes in Python it makes sense to initialize the parent class within the init method of the subclass. The lines creating member variables are used later on within the class methods, but aren't intended to be accessed by the user so they start with underscores. The print statement is also just a sanity check that we have started the initialization of the plugin prior reading in an actual file. It's nice for development's sake to have a message that looks slightly different if changes to the source are made and the plugin is reloaded. ``` To add a reader, we can use the following decorators @smproxy.source(name=\"PythonnumpyReader\", label=\"Python-based Numpy Reader\") @smhint.xml(\"\"\" \"\"\") or directly use the \"@reader\" decorator. @smproxy.reader(name=\"Sama Lidar Numpy Reader\", label=\"Python-based Numpy pcd Reader for timeseries data\", extensions=\"npy\", file_description=\"Numpy files\") class PythonNumpyPCDReader(VTKPythonAlgorithmBase): \"\"\"A reader that reads a NumPy file. If the NumPy has a \"time\" column, then the data is treated as a temporal dataset\"\"\" def init (self): VTKPythonAlgorithmBase. init (self, nInputPorts=0, nOutputPorts=1, outputType='vtkPolyData') self._filename = None self._ndata = None self._timesteps = None print(\"starting\",uuid.uuid1()) from vtkmodules.vtkCommonCore import vtkDataArraySelection self._arrayselection = vtkDataArraySelection() self._arrayselection.AddObserver(\"ModifiedEvent\", createModifiedCallback(self)) ``` This method is used by a different non private method, but is intended to respond to ParaView's requests to get data. These requests are usually the result of something in the pipeline before a filter changing, or in our case having a new timestamp selected from the animation window. We are importing NumPy here because we will be loading filenames and in some cases changing the data type of the array. If the point cloud (taken from a Lidar) was not just point positions but also intensity it becomes easier to access the intensity as a attribute for rendering if we make the array a named datatype array. The top of the method handles cases when we have already read the initial time point and are querying for new data. If we already have data in the member variable _ndata , and the requested _time was provided when the method was called then we use that information to look up another file from a list of filenames ( self.fnames ). NOTE, this code is written for a case when the timeline is using positive integer time steps. This means that we can use the requested _time as an index into the filenames list to load the requested timesteps point cloud. If you have floating point timeline values a different approach will be required. Later in the method is the code that is used when we are getting the raw data for the first time. We take the user selected file from the filebrowser and work out the path to the parent folder. Then we get a list of the contents of this folder that match the extension that we care about in this file reader. We can then fill in the member variables for _ndata and _timesteps . At this point we now can re-issue the call to this method and it will take a different path through loading data from the NumPy array out of a file. This allows us to still make use of the same code that all other invocations will use even though this time the method wasn't called in response to a change in the timeline. ``` def _get_raw_data(self, requested_time=None): import numpy if self._ndata is not None: if requested_time is not None: ##### load specific npy file from fnmes fname = self.fnames[int(requested_time)] self._ndata = numpy.load(fname) print(self._ndata.dtype) # self._ndata.dtype = numpy.dtype([(\"x\",numpy.float32),(\"y\",numpy.float32),(\"z\",numpy.float32),(\"intensity\",numpy.float32)]) return self._ndata return self._ndata if self._filename is None: # Note, exceptions are totally fine! raise RuntimeError(\"No filename specified\") # self._ndata = numpy.genfromtxt(self._filename, dtype=None, names=True, delimiter=',', autostrip=True) self.pth = Path(self._filename) self.fnames = list(self.pth.parent.rglob(\"*npy\")) self.fnames.sort() times = [i for i,e in enumerate(self.fnames)] self._ndata = 0 self._timesteps = times return self._get_raw_data(requested_time) ``` After the last method you get a break since this one is much shorter. When we looking to find out how many timesteps there are in the animation we need to know how many files are in the folder we are processing. The logic there is that we want a time step per file. In support of this we issue a call to the previous method _get_raw_data because that's what creates the member variable _timesteps . ``` def _get_timesteps(self): self._get_raw_data() return self._timesteps if self._timesteps is not None else None ``` This method for the most part is just recommended ParaView Python code straight from their section on Python programmable filters . Essentially it just returns back the updated time making sure its less than the values within the timesteps provided, otherwise it gives back the initial timestep. The _get_array_selection(self) method is included here too just because it's a one liner. I think this code might be dead as it's not actually getting set to anything in my case as far as I know. The _arrayselection is set in the init , but doesn't appear explicitly anywhere else. ``` def _get_update_time(self, outInfo): executive = self.GetExecutive() timesteps = self._get_timesteps() if timesteps is None or len(timesteps) == 0: return None elif outInfo.Has(executive.UPDATE_TIME_STEP()) and len(timesteps) > 0: utime = outInfo.Get(executive.UPDATE_TIME_STEP()) print(\"using inner method get update time\",utime) dtime = timesteps[0] for atime in timesteps: if atime > utime: return dtime else: dtime = atime return dtime else: assert(len(timesteps) > 0) return timesteps[0] def _get_array_selection(self): return self._arrayselection Array selection API is typical with readers in VTK This is intended to allow ability for users to choose which arrays to load. To expose that in ParaView, simply use the smproperty.dataarrayselection(). This method must return a vtkDataArraySelection instance. @smproperty.dataarrayselection(name=\"Arrays\") def GetDataArraySelection(self): return self._get_array_selection() ``` This is the next most important part where we are actually setting a filename using a widget. This method will receive values back after the user selects a file in the filebrowser. To trigger all this they must load the plugin and then use the \"open\" button. Once they select a file who's extension matches with this, the custom plugin reader will show up with any others that match the extension. Last I checked there weren't any others that were stepping forward to handle NumPy array input. This method just checks whether the same file has been loaded twice, and then sets itself to modified to trigger the subsequent loading and rendering steps. ``` @smproperty.stringvector(name=\"FileName\") @smdomain.filelist() @smhint.filechooser(extensions=\"npy\", file_description=\"numpy pcd file\") def SetFileName(self, name): \"\"\"Specify filename for the file to read.\"\"\" print(name) if self._filename != name: self._filename = name self._ndata = None self._timesteps = None self.Modified() ``` This is the public wrapper of the _get_timesteps() method which will be called after using the reader on a file that matches the extension supported. This method needs to be implemented for the algorithm class we are inheriting from to work correctly. ``` @smproperty.doublevector(name=\"TimestepValues\", information_only=\"1\", si_class=\"vtkSITimeStepsProperty\") def GetTimestepValues(self): print(\"getting time steps\") return self._get_timesteps() ``` This method is another of the ones that must be extended in the child class for the parent to work. This specifically is a method that responds to the changes in the timeline. For more information on this request for information please read lower down in this section https://docs.paraview.org/en/latest/ReferenceManual/pythonProgrammableFilter.html#understanding-the-programmable-modules . It will talk about how the RequestInformation is actually part of a \"pass in the pipeline's execution.\" Essentially this helps ParaView know that the reader plugin can provide more data at different timesteps. This method will also ensure that the animation timeline gets set to the right length as far as I know. ``` def RequestInformation(self, request, inInfoVec, outInfoVec): print(\"requesting information\") executive = self.GetExecutive() outInfo = outInfoVec.GetInformationObject(0) outInfo.Remove(executive.TIME_STEPS()) outInfo.Remove(executive.TIME_RANGE()) timesteps = self._get_timesteps() if timesteps is not None: for t in timesteps: outInfo.Append(executive.TIME_STEPS(), t) outInfo.Append(executive.TIME_RANGE(), timesteps[0]) outInfo.Append(executive.TIME_RANGE(), timesteps[-1]) return 1 ``` You've almost made it! The last method does almost all the heavy lifting. This again is covered in the section on programmable sources, and gives ParaView back something to show at the end of the execution pass. We load in several packages that are required to convert to VTK datatypes from NumPy. Then we get the time that we are supposed to load as a point cloud. We must have a variable representing the output on which to set our results so that ParaView can show them. Then we get the actual NumPy array representing the points in the point cloud for a particular data _time . Then we construct a vpoints variable which is an empty VTK point set with an expected number of points matching the number in our NumPy data. If our data set has an intensity attribute then we can create a separate float array that will be set as a scalar point data on the output. After that we need to set up the correct cell to be shown in ParaView. As I understand it, this part is standard VTK Python coding. Once the output has it's cells set we are done. ``` def RequestData(self, request, inInfoVec, outInfoVec): print(\"requesting data\") from vtkmodules.vtkCommonDataModel import vtkPolyData from vtkmodules.numpy_interface import dataset_adapter as dsa import vtk data_time = self._get_update_time(outInfoVec.GetInformationObject(0)) output = dsa.WrapDataObject(vtkPolyData.GetData(outInfoVec, 0)) points = self._get_raw_data(data_time) #points = self._ndata vpoints = vtk.vtkPoints() vpoints.SetNumberOfPoints(points.shape[0]) intensity = vtk.vtkFloatArray() intensity.SetNumberOfComponents(1) intensity.SetName(\"Intensity\") intensity.SetNumberOfTuples(points.shape[0]) for i in range(points.shape[0]): vpoints.SetPoint(i, points[i][:3]) intensity.SetTuple1(i, points[i][3]) output.GetPointData().SetScalars(intensity) output.SetPoints(vpoints) vcells = vtk.vtkCellArray() for i in range(points.shape[0]): vcells.InsertNextCell(1) vcells.InsertCellPoint(i) output.SetVerts(vcells) if data_time is not None: output.GetInformation().Set(output.DATA_TIME_STEP(), data_time) return 1 ``` Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/paraview_python_plugins/",
      "title": "Paraview Python Plugins - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "33aa8efc-e3a3-4490-a47e-b229daa9e5ee",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/paraview_python_plugins/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Roadmap ¶ Roadmap Overview OOD Remote Desktop Session Download and Configure Blender Running the Blender Scene Render from the Command Line Viewing Your Results Overview ¶ Blender is a tremendously useful open source 3D modeling program, and can be run on the HPC to create high fidelity visualizations of data and general 3D modeling. One of the benefits of running Blender on the HPC is you're able to transfer in a blender file containing your scene already configured and you can start the rendering process and leave it running overnight or during the week depending on the size of the rendering that needs to be done. Using command line rendering makes it possible to render from blender in a headless fashion, meaning no GPU is has to be allocated for your rendering. Another benefit of rendering on the supercomputer is large single frame Renderings can take up a significant amount of computer memory which HPC environments have to spare. OOD Remote Desktop Session ¶ Start up Open On Demand by going to ood.hpc.arizona.edu or reading up about OOD within our\\ documentation class=\"confluence-embedded-file-wrapper confluence-embedded-manual-size\"> Enter details of your request. Here we are requesting a 5 hour 16 core interactive session on elgato. Important to note that your own PI Group must be entered instead of visteam when the request clears the allocation queue you will be able to launch your interactive desktop Download and Configure Blender ¶ In this section we will download Blender, and unzip it, and then write a rendering python script At the remote desktop start screen open a terminal Navigate to an existing folder where you have space Download blender with this wget command from their clarkson mirror ``` wget \"../../all_images/mirror.clarkson.edu/blender/release/Blender2.93/blender-2.93.8-linux-x64.tar.xz\" ``` This lets us download the Long Term Support version of blender from their homepage ../../all_images/www.blender.org/download/lts/2-93/ Also make sure to uncompress the tar.xz at the end using this command ``` tar -xf blender-2.93.8-linux-x64.tar.xz ``` Here is the code that we will put into the render.py file ``` get the blender python library import bpy set the scene scene = bpy.context.scene set the output format as .tif scene.render.image_settings.file_format = \"TIFF\" specify where the rendered image/s will go and what their names should be scene.render.filepath = \"./frames/render\" set the engine to high performance CYCLES renderer scene.render.engine = \"CYCLES\" set the resolution percentage down for testing, turn this up to 100 when it's worked once scene.render.resolution_percentage = 25 how many cpu threads we should create, this is a good default for Elgato, but should be higher on Puma and Ocelote set it to the number of CPU cores you have in your allocation scene.render.threads = 15 scene.render.threads_mode = \"FIXED\" write a still frame render of the scene bpy.ops.render.render(write_still=1) ``` Running the Blender Scene Render from the Command Line ¶ Once you have created/edited the render.py using a command line editor like vi, use this command to start the headless rendering This will ensure that the blender file you have configured gets run in the background ( -b ) and that the render.py script is used as a python ( -P ) script ``` blender-2.93.8-linux-x64/blender -b -P render.py ``` With any luck your terminal will output that the process is initializing. This scene renders a landscape mesh of flagstaff using a heightmap in a tiff format which is why we see messages like TIFFFetchNormalTag , which shouldn't appear on your screen unless you are doing a similar task. This output shows the initialization of other blender systems that may or may not be part of the rendering that you are doing. Unused systems are unlikely to detract from the rendering performance and are simply listed for diagnostic purposes it seems. Once the initialization has completed the individual tiles of the larger image will begin rendering. This is where the massive multicore environments can really shine because a rendering thread can be dispatched for each core provided there's enough memory to support all of them running at the same time. There is also a time estimate which is usually an over estimate for the full duration of the rendering task. If you want to make sure that all the cpu cores you have allocated are in use use the htop -u <username> in a new terminal tab. When the process completes you will see blender quit after saving out an image to the folder you specify Viewing Your Results ¶ This is a view of the image produced by the workflow {\"serverDuration\": 21, \"requestCorrelationId\": \"ca405ed9ac024358b0ff2ea1e8cbddc2\"} Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/blender/command_line_rendering/",
      "title": "Blender Command Line Rendering - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "441c8cf0-8f02-4a11-8bf8-8c28bc10c7d5",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/blender/command_line_rendering/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Submitting Batch Jobs ¶ Submitting a Job ¶ To submit a batch job to the scheduler, use the command sbatch . This will place your job in line for execution and will return a job ID that you can use to track and monitor your job . As an example: ``` [netid@gpu66 hello_world]$ sbatch hello_world.slurm Submitted batch job 807387 [netid@gpu66 hello_world]$ squeue --job 807387 JOBID PARTITION NAME USER ST TIME NODES 807387 standard hello_wo netid PD 0:06 1 ``` The command squeue gives us detailed information about our batch jobs while they're in queue or running. Under the heading ST you can check the state of your job. In this case, it's pending ( PD ) which means it's waiting in line with other jobs. Once the job starts running, it's state will change to R , and when the job has completed running, squeue will return a blank line. Submitting Multiple Jobs ¶ Frequently, users need to submit multiple, related jobs. It may be tempting to do this using a bash loop, but there are several drawbacks to this method, primarily that it can affect the performance of the job scheduling software. Use Arrays instead of Loops for large numbers of jobs Users submitting large numbers (> 100s) of jobs using loops will be contacted and asked to adjust their workflows. Requests that persistently affect the performance of the job scheduler will be cancelled by HPC infrastructure. The best way to submit related jobs is to use job arrays. Jobs arrays allow users to submit multiple related jobs using a single script and single sbatch command. Each task within the array can have its own unique input parameters, making it ideal for running batch jobs with varied inputs or executing repetitive tasks efficiently. See Array Jobs for specifics on how to submit these sorts of workflows. Output Files ¶ Once your job completes, you should see an output file in the directory where you submitted the batch script. This output file captures anything that would have been printed to the terminal if you had run it interactively. By default, output filenames will be slurm-<jobid>.out . In the example above, this translates to filename slurm-807387.out . Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/batch_jobs/submitting_jobs/",
      "title": "Submitting Jobs - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "549b4b4f-8d73-4a72-89e7-d9fb0c09d400",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/batch_jobs/submitting_jobs/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Visualization Consulting ¶ To see a gallery of past projects please visit https://rtdatavis.github.io/ . Software ¶ Paraview ¶ Paraview is available as a module. You will use those in server mode where the calculations are done on HPC nodes and the rendering is back on your workstation. View our material on Visualization With\\ ParaView for more information. Essential Ideas for\\ ParaView Getting Started With\\ ParaView ParaView Cameras and\\ Keyframes Visualizing NetCDF\\ Files [Getting Started With The Paraview Terminal\\ (PvPython)](Paraview/Getting_Started_With_The_Paraview_Terminal_PvPython_ Graphs and Exporting\\ Data Paraview Headless Batch\\ Rendering Speeding up Filters & Rendering with Client Server\\ configurations Advanced Computational Geometry in Paraview with CGAL &\\ Vespa Demystifying Paraview Python Plugins (or at least a little less\\ mystery) Blender ¶ Blender is a tremendously useful open source 3D modeling program. Detailed instructions for Blender are available in several tutorials. Blender Command Line\\ Rendering Scaling up Blender\\ rendering Visit ¶ Step by step instructions for using VisIt are on this\\ page , and it is also available as a module on our system. GUIs Through Open OnDemand ¶ You can utilize the GUI version of an application using a Virtual Desktop or through one of our interactive applications using Open\\ OnDemand . When you choose an application from the Interactive Apps dropdown at the top of the page and enter the resources you need (like cores and wall time hours) you will have access to a session that will let you use a compute node interactively through a virtual desktop or software-specific GUI application. VPN - Virtual Private Network ¶ A VPN service (Virtual Private Network) is available for HPC, primarily for applications that cannot navigate the bastion host for visualization, and because the performance is frequently better than tunneling through the Bastion host. This is separate from the UA VPN. We suggest you first try the Desktop application of OnDemand for GUI applications as shown for Matlab immediately above. VPN - Virtual Private Network X11 Examples ¶ The alternative to using the Open OnDemand desktop is to use X11\\ tunneling , though it should be noted that in most cases the performance is not as good. The examples below use X11 forwarding to access the graphical interfaces for the listed software. Ansys Workbench ¶ In this example, we use the GUI for launching Ansys projects. Note: Ansys is also available as a GUI application through Open OnDemand. You can expect it to be much faster using OnDemand ``` $ ssh -X netid@hpc.arizona.edu $ shell -X $ interactive -x -a your_group # starts an interactive session for one hour. The -x option will enable image forwarding $ module load cuda11 # Provides the GLX functionality needed by some components like CFD Post $ module load ansys $ runwb2 ``` GLX - Useful for VisIt ¶ The functionality of GLX requires synchronization between your workstation and the compute nodes. Keep in mind that the login nodes do not have a graphics so you must request a compute using an interactive session. These steps provide a practical example for using the Visualization software called VisIt ``` $ ssh -X netid@hpc.arizona.edu $ shell -X $ interactive -a your_group -x # The -x flag provides the graphics tunneling $ module load cuda11 # Provides support for GLX. A GPU node is not required $ glxinfo # **See notes below if you get an error during this step $ glxspheres64 # This is a good test of rendering back on the workstation $ module load visit $ visit ``` \\ \\ You might get a \"Bad Value\" error on the Mac command line. This can by fixed by entering the following on the command line ``` $ defaults write org.macosforge.xquartz.X11 enable_iglx -bool true ``` On a Linux workstation, you might need to create a file /etc/X11/xorg/conf with the following section: ``` Section \"ServerFlags\" Option \"AllowIndirectGLX\" \"on\" Option \"IndirectGLX\" \"on\" EndSection ``` {\"serverDuration\": 17, \"requestCorrelationId\": \"0a7963d46a704cf6895bca5ac6e13934\"} Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/",
      "title": "Intro - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "56c4573e-4a65-4397-9e5e-1f98078f9b93",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Getting Started ParaView is a powerful data visualization software that many researchers can find useful for getting a visual understanding of their data. This guide will be a brief introduction to ParaView including how to install it and use its GUI interface. For more information on how to use the ParaView terminal (PvPython), see the tutorial here . This tutorial was created using Windows 10 but should apply to most if not all ParaView installations. Note: while programming experience with python is not required to follow along, it can be useful for working with more advanced ParaView features. Installation (for HPC) These instructions will get you setup with the Paraview GUI on our HPC systems. Feel free to copy and paste this code into an OOD Remote Desktop Terminal, and consult the lower explanations for details about each line. ``` singularity pull docker://ghcr.io/devinbayly/vtk:latest wget \"https://www.paraview.org/paraview-downloads/download.php?submit=Download&version=v5.11&type=binary&os=Linux&downloadFile=ParaView-5.11.0-MPI-Linux-Python3.9-x86_64.tar.gz\" -O paraview.tar.gz tar xf paraview.tar.gz singularity exec vtk_latest.sif ./ParaView-5.11.0-MPI-Linux-Python3.9-x86_64/bin/paraview ``` singularity pull docker://ghcr.io/devinbayly/vtk:latest This line of code will pull a container the packages provided by our visualization consultant wget \"https://www.paraview.org/paraview-downloads/download.php?submit=Download&version=v5.11&type=binary&os=Linux&downloadFile=ParaView-5.11.0-MPI-Linux-Python3.9-x86_64.tar.gz\" -O paraview.tar.gz This line will pull the paraview binary from their downloads, and rename it to paraview.tar.gz tar xf paraview.tar.gz This line will extract the contents of the gzipped tar file singularity exec vtk_latest.sif ./ParaView-5.11.0-MPI-Linux-Python3.9-x86_64/bin/paraview This line will execute the paraview binary and launch the GUI If you'd like to know more about what's happening here, we are technically using the container to package the dependencies needed by paraview, and using the program downloaded to our local folder launch to the gui. This decouples the paraview version from the dependencies, freeing you up to use whatever version suits you without having to download other singularity containers. Installation (for workstations) ParaView is available for Windows, Mac, and Linux and can be downloaded at this website: https://www.paraview.org/download/ Find the installation option that best applies to you. For most cases, the top option will work. Run the downloaded executable and follow the instructions to install it. Running ParaView There are two ways to work with ParaView, either through the GUI (visual editor) or through a terminal. This tutorial will focus on the GUI, if you wish to use the terminal, check out the PvPython tutorial here . ParaView GUI To run the ParaView GUI, find the Paraview executable and run it on your computer. Once open, close any popups and you will see the default layout If you plan on working with python, from the top drop-down you can click View→Python Shell. This python shell is equivalent to PvPython in the ParaView Terminal option. To get started with the terminal, first, add a shape. Try adding a sphere to the scene from the drop-down by clicking Sources→Geometric Shapes→Sphere. While nothing is added to the view here, you can see that in the pipeline tab on the left, a sphere has been added. To show this in the view, you can click the eye icon to the left of the new object. To maneuver in the view you can use your left mouse button to rotate, your right mouse button to zoom, and your middle mouse button to pan. Alternatively, shift+ right mouse button can be used to pan and the scroll wheel can be used to zoom. The view can be reset at any time with the Reset button shown below: Loading Data Let’s load some example data into ParaView. Click the “Open” button from the toolbar (shown below), go to File → Open, or press Crtl + O. On the left side under Favorites, there should be a directory called Examples. From that, click can.ex2 and hit OK. No data should appear yet but information about the data should appear in the properties tab. In this, we can select what information from our data set we want to load. For this example, we can leave all the default settings. Press Apply to confirm the settings and you should see the example data appear in the view. Changing The Visualization This visualization can be played with the green play button at the top of the screen. Currently, the colors just show the two different shapes. Let’s say we want to visualize the acceleration of all the points of the shape. In the toolbar, find the drop-down that currently says vtkBlockColors and switch it to vel. Play the animation again to visualize the result. While this does provide some useful information, it may not be the best way of visualizing velocity. Instead, let's add some vectors to see how each point is moving. Start by adding a Glyph to your data. With your data selected click the Glyph button from the toolbar. Next, set both the Orientation Array and Scale Array to vel and hit apply. You should notice that the arrows do get added but they are way too large. Change this by editing your scale factor until it looks right (0.0005 worked for me). And there you have it! You can play the animation and watch as the arrows change with time. Later tutorials will work with specific data types as well as other features included in ParaView but after this, you should have a base understanding of how to navigate the scene and load some basic data. Visualization on HPC youtube video {\"serverDuration\": 18, \"requestCorrelationId\": \"ab72a95d9f4b40179c6ccfe7f323516a\"} Was this page informative? Thanks for your feedback! Thanks for your feedback! Have suggestions for improvements? Contact us . Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/Paraview/Getting_Started_With_ParaView",
      "title": "Getting Started - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "1fc7199f-9480-4548-bb58-ff7373497b60",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.34",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/Paraview/Getting_Started_With_ParaView",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Visualizing NetCDF Files This tutorial will help you visualize your NetCDF Files in ParaView. If you are unfamiliar with ParaView, check out Getting Started With ParaView GUI . Opening The Data ¶ Once you have ParaView running. Click the open button and find your data. For this tutorial, I will be using the ParaView example data in Examples → netCDF . A window should appear showing you different options for opening your file. For most purposes, the NetCDF Reader will work. In the properties window, check “Replace Fill Value With Nan” and press apply. You should see something like the image below Finally, to visualize the data, select the color drop down from the toolbar (it should say “Solid Color”) and change it to tos . Now, the data should appear on the sphere, allowing you to rotate around it and press play at the top for further visualization. If instead of the sphere you would prefer a flat view of the data, you can uncheck Spherical Coordinates from the properties menu, press apply, then reselect the new tos from the color drop down. Finally, for a perfectly flat visualization, click the button above the view that says 3D. This will shift it to a 2D view. Then press the button to the right of the 2D button that says Adjust Camera and click the button with the -Z on it. This should give you a perfectly flat view of your map. And there you have a visualization of simple netCDF data. Below I will show some useful tips for visualizing more specific types of data. Three Dimensional Data ¶ For data that you would like to view in three dimensions rather than the 2D plane or sphere surface, under the properties window, change Dimensions to a property that has three separate values (often latitude, longitude, and another). Viewing 3D data can be a problem oftentimes on 2D screens. Likely the best ways to view this data is with ParaView's cropping tools (Green boxes on the left of the toolbar) or using the Nvidia IndeX plugin for volumetric rendering. Plotting Data ¶ For plotting data from NetCDF files, you can reference Graphs and Exporting Data . Be sure to check the Common Issues section at the bottom of the page as NetCDF files will often have holes or null values in the data. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/visualizing_netcdf_files/",
      "title": "Visualizing NetCDF Files - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "5ab72c4a-5d9f-425e-925b-b84b6fabd7ff",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/visualizing_netcdf_files/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Speeding up Filters & Rendering with Client Server configurations For Both of these configurations it is recommended that you start up a remote desktop and launch paraview according to the instructions on the previous documented page Getting Started With ParaView. Using the Embedded graphics library paraview binary Steps To launch a EGL server This workflow uses offscreen rendering and a gpu to speed up interactive workflows. First request an allocation on Ocelote that has 1 gpu Please remember to replace the <your account> with your own account name salloc -A <your account> -p standard --gres=gpu:1 -t 3:00:00 -N 1 -n 16 The following lines will download a version of paraview that we can use for offscreen headless rendering. The command will actually rename the export file in order to make it easier to uncompress it. wget \"https://www.paraview.org/paraview-downloads/download.php?submit=Download&version=v5.11&type=binary&os=Linux&downloadFile=ParaView-5.11.1-egl-MPI-Linux-Python3.9-x86_64.tar.gz\" -O paraview_egl.tar.gz Here we uncompress the tar file tar xf paraview_egl.tar.gz The folder we get from this will probably include the full paraview version name so we will use a * pattern match character to go into the bin folder where the pvserver program we need to use resides cd ParaView-5.11.1*/bin/ Here's where the exciting part occurs. We will use the pvserver program to start up a process which listens for connections from the graphical interface and fulfills requests on its behalf. The extra benefit is that it will use GPU hardware accelerated rendering to get back results to the viewport even faster. We will also be making use of the embedded graphics library EGL, to perform these renderings without the need for an x-server display. The flag --displays is used to specify the gpu card index that we want to make use of, starting from the number 0. So if we had another card we wished to use instead we would use --displays=1 . The potential to leverage multiple gpus is available but requires a different compiled version of paraview with a specific nvidia license so it will not be covered here. ./pvserver --displays=0 For completeness here are the commands all together for copying in pasting into a terminal on the HPC ``` wget \"https://www.paraview.org/paraview-downloads/download.php?submit=Download&version=v5.11&type=binary&os=Linux&downloadFile=ParaView-5.11.1-egl-MPI-Linux-Python3.9-x86_64.tar.gz\" -O paraview_egl.tar.gz cd ParaView-5.11.1*/bin/ tar xf paraview_egl.tar.gz ./pvserver --displays=0 ``` Take note of the address that is written to the screen at this point it will look something like cs://<node_name>.<cluster_name>.arizona.edu:11111 and this will be used when we connect via the gui client Steps to launch a client to connect to the server For a first draft of this page we will rely on the documentation provided on the paraview read the docs page to understand the process of connecting from the graphical user interface. https://docs.paraview.org/en/latest/ReferenceManual/parallelDataVisualization.html#configuring-a-server-connection Once the connection has been made, it is possible to determine the extent to which the gpu is utilized by putting the running ./pvserver in the background with ctrl-z and then typing bg . At this point you can now type nvidia-smi -l 2 to get a log of the activity on the gpu in a text table. Using multinode MPI and osmesa for This process is very similar to the above, but we will be asking for a slightly different allocation. Use the following command on Elgato for testing having two full nodes of 16 cpu cores to speed up filtering and rendering with paraview. After this tutorial feel free to test out using different machines or node and core counts. The rule being that the total number of cpu cores is specified in the -n flag and the total number of nodes is specified with the -N flag. For more information consult the basic Slurm scheduling pages. Please remember to replace the with your own account name interactive -a <your account> -N 2 -n 32 -t 1:00:00 When the first of the multiple nodes is allocated go and download the latest paraview osmesa. wget \"https://www.paraview.org/paraview-downloads/download.php?submit=Download&version=v5.11&type=binary&os=Linux&downloadFile=ParaView-5.11.1-osmesa-MPI-Linux-Python3.9-x86_64.tar.gz\" -O paraview_mesa.tar.gz The next step is to untar it tar xf paraview_mesa.tar.gz Then at this point you are ready to navigate into the bin folder cd Paraview*/bin Now use the provided mpiexec binary to start up a pvserver backed up by all our allocated cores ./mpiexec -n 32 ./pvserver Once again this is an offscreen rendering system able to run without needing to have displays or anything. A side benefit is since this is an entire osmesa build, no missing opengl dependencies cause problems. Again we need to pay attention to the output of this command so that we can see the server's connection url to use. For instance if the mpi nodes are gpu1&gpu2 it will look like cs://gpu2.elgato.hpc.arizona.edu:11111 At this point we are now all set in order to start up the paraview gui in the remote desktop allocation. Again, please refer back to the Getting Started with Paraview documentation for instructions on this TODO replace this with the link in confluence. Once the gui is open, configure a new connection, and put in just the server address ie gpu2.elgato.hpc.arizona.edu for the host, then supply the port as 11111 After about 10-30 seconds you will be connected and all of the filters and rendering will be accelerated by parallel multinode-mpi. To double check whether your cores are being fully utilised consider using the following. First open a separate terminal ssh into each of the mpi nodes and run htop to see their individual cpu core utilization. As you interact with the paraview gui client the multinode-mpi server will undergo periods of low and high activity observable in the htop display. Please consult the video below for any additional questions or reach out to the vislab-consult@list.arizona.edu **Note that this video is using less mpi resources because waiting on two full elgato nodes wasn't allocating very quickly and made it harder to record the video. Just remember instead of -n 8 you can use -n 32 and then go out for a quick snack.** Was this page informative? Thanks for your feedback! Thanks for your feedback! Have suggestions for improvements? Contact us . Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/Paraview/Speeding_up_Filters_Rendering_with_Client_Server_configurations",
      "title": "Speeding up Filters & Rendering with Client Server configurations - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "26f78472-3ba4-4b32-96b5-4dd587576521",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.34",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/Paraview/Speeding_up_Filters_Rendering_with_Client_Server_configurations",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Visualizing NetCDF Files This tutorial will help you visualize your NetCDF Files in ParaView. If you are unfamiliar with ParaView, check out the getting started tutorial here . Opening The Data Once you have ParaView running. Click the open button and find your data. For this tutorial, I will be using the ParaView example data in Examples → netCDF. A window should appear showing you different options for opening your file. For most purposes, the NetCDF Reader will work. In the properties window, check “Replace Fill Value With Nan” and press apply. You should see something like the image below Finally, to visualize the data, select the color drop down from the toolbar (it should say “Solid Color”) and change it to tos. Now, the data should appear on the sphere, allowing you to rotate around it and press play at the top for further visualization. If instead of the sphere you would prefer a flat view of the data, you can uncheck Spherical Coordinates from the properties menu, press apply, then reselect the new tos from the color drop down. Finally, for a perfectly flat visualization, click the button above the view that says 3D. This will shift it to a 2D view. Then press the button to the right of the 2D button that says Adjust Camera and click the button with the -Z on it. This should give you a perfectly flat view of your map. And there you have a visualization of simple netCDF data. Below I will show some useful tips for visualizing more specific types of data. Three Dimensional Data For data that you would like to view in three dimensions rather than the 2D plane or sphere surface, under the properties window, change Dimensions to a property that has three separate values (often latitude, longitude, and another). Viewing 3D data can be a problem oftentimes on 2D screens. Likely the best ways to view this data is with ParaView's cropping tools (Green boxes on the left of the toolbar) or using the Nvidia IndeX plugin for volumetric rendering. Plotting Data For plotting data from NetCDF files, you can reference Graphs and\\ Exporting\\ Data . Be sure to check the Common Issues section at the bottom of the page as NetCDF files will often have holes or null values in the data. {\"serverDuration\": 19, \"requestCorrelationId\": \"d8d4a00d919041b1971a6082298c5da4\"} Was this page informative? Thanks for your feedback! Thanks for your feedback! Have suggestions for improvements? Contact us . Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/Paraview/Visualizing_NetCDF_Files",
      "title": "Visualizing NetCDF Files - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "3fbef96b-af2c-4b11-9e83-87f73458eed5",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.34",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/Paraview/Visualizing_NetCDF_Files",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Parallelization ¶ Overview ¶ You might be surprised to learn that if you move code from a local computer to a supercomputer, it will not automatically run faster and may even run slower. This is because the power of a supercomputer comes from the volume of resources available (compute nodes, CPUs, GPUs, etc.) and not the clockspeed of the processors themselves. Performance boosts come from optimizing your code to make use of the additional processors available on HPC, a practice known as parallelization. Parallelization enables jobs to 'divide-and-conquer' independent tasks within a process when multiple threads are available. In practice, this typically means running a job with multiple CPUs on the HPC. On your local machine, running apps like your web browser is natively parallelized, meaning you don't have to worry about having so many tabs open. However, on the HPC, parallelization must almost always be explicitly configured and called from your job. This process is highly software-dependent, so please research the proper method for running your program of choice in parallel. If you are interested in learning more about parallelization, please check out our Parallel Computing Workshop Scaling ¶ A classic example of the advantages of parallel computing is multiplying an N×N matrix by a scalar, which takes N2 floating-point operations (flops). On one CPU, this will take an amount of time t=N2/f where f is the clock frequency of the CPU. On an integer M number of CPUs, this computation will instead take t′=tM. Most analyses involve computations which are not always as independent as matrix multiplication, leading to less than perfect speedup times. Amdahl's Law (strong scaling) The behavior of predicted speedup time for an analysis of fixed size is known as Strong Scaling . It depends on the fraction of code in a particular program which is parallelizable. Gustafson's Law (weak scaling) When scaling up an analysis by a factor of N, and running it on N times as many processors, the theoretical limit is an equivalent runtime. In practice, the runtime is typically longer. Most advancements in research are made from increasing the individual problem size, rather than decreasing time to execution, and therefore benefits more from so-called \"weak\" scaling. Single versus Multi-Node Parallelism ¶ Single Node Each node on HPC has multiple CPUs. These can be utilized simultaneously in shared-memory parallelism. Multi-Node Multiple nodes can be accessed simultaneously on HPC, but memory is distributed rather than shared. In this case, additional software is needed in order to facilitate communication between processes, such as OpenMPI or Intel MPI. Please be aware of the type of parallelism used in your program. Some software is configured only for shared-memory parallelism and will not be able to use processors on multiple nodes. Implementation ¶ Some software is configured to be parallel out of the box, and other software needs explicit configuration. Check with your provider to determine whether parallelism is available, and which kind. For example, Python is natively serial, but libraries are available to enable either shared-memory or distributed-memory parallelism. Providing a detailed guide on how to code parallel processing for all software available on the HPC is beyond the scope of this documentation. Instead, please refer to the following online guides: Python multiprocessing - enables shared-memory parallelism. Reference . import mpi4py - enables distributed-memory parallelism. Reference . R R provides the parallel library for multiprocessing. Reference . MATLAB MATLAB provides the Parallel Computing Toolbox. Reference . MPI We provide several versions and implementations of MPI as system modules: ``` [netid@cpu39 ~]$ module avail mpi ---------------- /opt/ohpc/pub/moduledeps/gnu8 ---------------- mpich/3.3.1 openmpi3/3.1.4 (L) openmpi4/4.1.1 ``` Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/parallelization/",
      "title": "Parallelization - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "5d067cc3-8643-4da9-a408-6c1d076cfdd9",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/parallelization/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Essential Ideas To get the most out of ParaView, it is essential to understand not just the steps of doing things but also why those steps work. This will help you in debugging many of the issues you run across while using the program. Nodes Pretty much all of ParaView is structured in a system of nodes stored in the pipeline (see this with View → Pipeline Browser from the toolbar). Nodes are sorted into three main categories: Sources Sources are, as the name implies, the source for the ParaView pipeline. This is anything that you want to bring into your visualization including data, geometric shapes, annotations, etc. In short, these are the objects that all the other nodes base themselves on. Filters Filters are the main tool of analysis in ParaView. Anytime you want to add something to your data (such as showing velocities as arrows), modify your data in some way (like projecting onto an axis), or visualize with a graph, these are the nodes to use. Extractors Extractors, unlike sources or filters, do not add or modify data in any way. They simply act as a means of taking the data that you already have in ParaView and exporting it. Examples of this include saving images or exporting data as a spreadsheet. The general workflow for ParaView is importing your data with sources (this is what the Open button does), performing your analysis with filters, then exporting it either as an image or spreadsheet for further analysis. The Pipeline The pipeline is where all of your nodes will be stored. It is stored in a tree-like format, with the root of the tree always being builtin:. Generally, sources and extractors will be children of the root and filters will be children of sources. Any filters that a source has as a child will affect that source object. {\"serverDuration\": 17, \"requestCorrelationId\": \"90643f41f1134bacadaac032883f9481\"} Was this page informative? Thanks for your feedback! Thanks for your feedback! Have suggestions for improvements? Contact us . Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/Paraview/Essential_Ideas_for_ParaView",
      "title": "Essential Ideas - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "6bdb2b26-d048-4465-ad15-5e48d1a06f79",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.34",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/Paraview/Essential_Ideas_for_ParaView",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Rendering a single frame headlessly: ¶ Pre Reqs ¶ This tutorial assumes you have followed steps from Blender Command\\ Line\\ Rendering to get a copy of blender and a folder to work in. Download the blender file test-headless.blend to your hpc workspace It also assumes an understanding of singularity commands for working with containers Containers , and Introduction to Containers on\\ HPC Attribution ¶ This article forms the base and background of this technique https://linuxmeerkat.wordpress.com/2014/10/17/running-a-gui-application-in-a-docker-container/ Summary ¶ The example that follows aims at producing a rendered frame from a default blender scene using `Xvfb` within a preconfigured singularity container. In each section there will be screen shot footage on the right and a code block provided for each command entered to make copying and reproducing the steps easier. image credit ( https://www.youtube.com/watch?v=FNhUnPWzVaw ) The first step here is to start an interactive job and get our copy of the singularity image from the github container repository. The code used are ``` interactive -a -n 4 ``` With your account specified here instead of the visteam account. Then navigate to a folder where you would like do do your work. You will then need a singularity container configured for Xvfb ``` singularity pull -F docker://ghcr.io/devinbayly/xvfb:test ``` This retrieves the docker image from github container repository and converts it into a singularity image. The -F is for forcing the command to overwrite an existing image if you have already pulled this in the past. This helps make sure that you work with the newest image if there were changes made. Now we go inside the container and start the Xvfb program to generate a virtual display named 99 with a screen 0 associated of 1024x720 resolution with a depth of 16 images. ``` singularity shell xvfb_test.sif Xvfb :99 -screen 0 1024x720x16 &> xvfb.log & ``` This will ensure the display remains as a background task and sends messages to the log file. To check whether this is working we can grep for the X process. ``` ps -aux | grep X ``` But of course this also reveals any of the other users who are also trying to create displays, so I've blurred that out. Part of picking :99 is to avoid using the same name as other folks on a node. We are now ready to try to run a headless blender render. The first screenshot shows the result of jumping the gun and running the command without mentioning which display to use. A more complete command that achieves the result is provided as follows. ``` DISPLAY=:99.0 blender-3.3.0-linux-x64/blender -b test-headless.blend -f 1 ``` This will make the blender command use the correct display for our rendering of a single frame from the blender scene. There's an exhausting, ahem I mean exhaustive treatment of the command line rendering flags here https://docs.blender.org/manual/en/latest/advanced/command_line/render.html if you are curious. The output of the command will be lines suggesting the scene is loaded and that samples are being taken and rendered to the image. The final line shows where the resulting png frame ends up. If you would like to watch the steps in video form feel free. Your browser does not support the HTML5 video element {panel Scaling Up ¶ The process of using many cpu cores with blender is as follows. The image for this step is pretty small, but you may click on it to view it in a larger window. That or you can trust me that the code blocks below are all you need. The contents of the file on the left simply capture what was done in the interactive session in the previous step. So create a file called run_blender.sh with these contents. ``` !/bin/bash Xvfb :99 -screen 0 1024x720x16 &> xvfb.log & DISPLAY=:99.0 blender-3.3.0-linux-x64/blender -b test-headless.blend -o ./test_$1_ -f 1 ``` We don't need to use the singularity shell command because that will be part of our slurm batch submission script. Also the -o ./test_$1_ lets us name the output images with a templated pattern using a number passed to the script. The file on the right is our slurm batch submission script named batch.sh . Read more about them in Jobs and\\ Scheduling . The content of this one is: ``` !/bin/bash SBATCH --output=Sample_SLURM_Job-%a.out SBATCH --ntasks=4 SBATCH --nodes=1 SBATCH --time=00:15:00 SBATCH --partition=standard SBATCH --account= SBATCH --array 0-64 SLURM Inherits your environment. cd $SLURM_SUBMIT_DIR not needed singularity exec xvfb_test.sif bash run_blender.sh ${SLURM_ARRAY_TASK_ID} ``` The primary change you will need to make is to write your own account in the place where it says --account=<your account name> . Besides that you will notice that the variable ${SLURM_ARRAY_TASK_ID} is being passed into the shell script so that each separate task will create a frame png output. To run this batch file we use: ``` sbatch batch.sh ``` Once the command to run the batch.sh is provided we see that all our 64 tasks with 4 cores each get kicked off. To view the results you may return to the open on demand panel and select one of the many finished pngs. I'm hopeful that you will find better ways to apply this to visualizing data besides leveraging 256 cores to render a gray cube on a gray background. If you would like to watch the steps unfold instead of looking at the still frames feel free to use this video. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/blender/scaling_up_blender_rendering/",
      "title": "Scaling Up Blender Rendering - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "73801fb7-d148-49ae-8944-23c9ce9b5c01",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/blender/scaling_up_blender_rendering/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content VisIt Goal Using OOD Client/Server Configuration Pre-requisite Installations Steps on the HPC Setting up ssh keys for passwordless authentication to the HPC. Installing the VisIt server Copying the launcher script into server folder Steps for Linux Connecting to the HPC VPN. Installing visit local machine Installing the HPC host profiles running visit Filling in host profile Support related to this document Goal By the end of this document you will know how to start up VisIt client server work sessions with Open On\\ Demand or between your local machine and the compute nodes of the HPC in client server configuration. Using OOD This method makes use of a remote desktop that the user can start up on the HPC. This benefits from putting the user in a familiar graphical environment so things like starting visit are the same here as on their local workstation. Open a browser and go to ood.hpc.arizona.edu When prompted fill out the authentication Now select Interactive Desktop from the Interactive Apps Here you can see the values that were set to run a 1 hour visit session on Ocelote with a gpu. It should be noted that this workflow can also be performed with cpu only allocations too. You will need to fill out your PI where the highlighter is over visteam . Once you have hit Launch and your allocation is granted you will see this on the page. Now you will click on the Launch Interactive Desktop button. A new tab will be created that shows a remote desktop in the window. Here you will right click on the background and select Open In Terminal . Then you will need to change directory to wherever Visit 3.2.1 was installed. If you haven't done this then follow this script to install it in the directory you are in Download the redhat EL7 w/ Mesa option by running these commands replacing the <PIname> with your PI's name ``` mkdir visit_setup cd visit_setup wget \"https://github.com/visit-dav/visit/releases/download/v3.2.1/visit3_2_1.linux-x86_64-rhel7-wmesa.tar.gz\" tar xf visit3_2_1.linux-x86_64-rhel7-wmesa.tar.gz cd visit3_2_1.linux-x86_64 ``` Once you are located in the visit3_2_1.linux-x86_64 folder navigate to the bin folder and run ./visit -debug 5 You will now see Visit starting in the remote desktop. Use the Open button to bring up a data set you want to visualize and export a movie from. Here the aneurysm dataset is selected, if you'd like to use it you may download it here https://visit-dav.github.io/largedata/datarchives/aneurysm . Then choose to add a simple plot. For this data one of the interesting parts is the velocity field and how it changes over time. Once you have selected the velocity field make sure to draw it to the adjacent Visit window. Now that we have the first frame drawn in the viewing window, let's render all the frames of the simulation out as a movie. Select File to show the drop down menu of options. From the options select Save Movie which will open a wizard window to step you through the export configuration. Select next with the default option to create a New simple movie In the next window select the format drop down and select one of the options. A benefit to exporting individual frames as still images is it provides greater flexibility over quality options for the conversion to video which is outlined later. Here PNG will be used Click the -> arrow to queue this format to the export options. It is possible to make several outputs from a single export pass by clicking the -> arrow multiple times for a range of formats/options. Clicking next shows a window with parameters about the video to be made. These options are default set to create a frame for each time step in the video, but if a particular range is of interest you may set the First Frame and Last Frame to customize. Finally we will specify where the outputs are supposed to go. Use the drop down button at the far right to open a file browser window to aid you in finding the optimal destination if it isn't the default one shown. You may change the Base Name to specify an alternate prefix for each rendered frame which is followed by a frame number padded with 4 0's. Since we have already configured a visit instance with out data and plot we will use the currently allocated processors . More advanced workflows may require the other options but they are not documented here as of yet. Once you select finish a new window opens which shows the terminal output of the process, as well as a progress bar window that helps you see how the export is unfolding. Once this finishes you may return to the terminal, and navigate to the folder where the individual frames were output. Use the ls command to make sure you are in the correct location. You must also load in a few modules in order to process the images into a video from the command line. ``` module load gnu8 module load ffmpeg ``` This is the final step. You will run ffmpeg to concatenate all the images together into a video whose format you may specify. To let ffmpeg know we are using all the images that match a pattern that starts with the word movie, then has 4 digits, then file type mp4 we use this string movie%04d.png . The -r specifies the rate in fps. The -f notifies ffmpeg that we are using the image2 filter to convert an image->something else. ``` ffmpeg -f image2 -i movie%04d.png -r 10 output.mp4 ``` All done! Client/Server Configuration This setup differs from OOD in that there is no remote rendering of the visit window contents, and allows for rapid response from interaction with the menus because the client runs natively on the local machine. The trade off is that certain capabilities are c`c Pre-requisite Installations VPN system like Cisco Anyconnect Steps on the HPC Setting up ssh keys for passwordless authentication to the HPC. The documentation for this step exists on confluence at this link System\\ Access#SSHKeys but you must complete these steps for shell.hpc.arizona.edu not the bastion hpc.arizona.edu host. Note that performing a ssh <netid>@shell.hpc.arizona.edu and replacing the <netid> with your netid is only possible when we are on the hpc vpn. Installing the VisIt server Download the redhat EL7 w/ Mesa option by running these commands replacing the <PIname> with your PI's name ``` cd /groups/ mkdir visit_setup cd visit_setup wget \"https://github.com/visit-dav/visit/releases/download/v3.1.4/visit3_1_4.linux-x86_64-rhel7-wmesa.tar.gz\" tar xf visit3_1_4.linux-x86_64-rhel7-wmesa.tar.gz cd visit3_1_4.linux-x86_64 ``` Copying the launcher script into server folder The contents of the visit installation are laid out as follows. Navigate to the bin folder within 3.1.4 and copy in the customlauncher code ``` cd 3.1.4/bin wget \"https://gist.githubusercontent.com/DevinBayly/310c8689c6221fd379aad34243441dda/raw/a8f5071bb8b1e96127e1ea01b2e8667940849f1a/customlauncher\" -O customlauncher ``` If you'd like to inspect the code a copy is available here as well. Customlauncher Steps for Linux Connecting to the HPC VPN. For more information about this step please consult the respective confluence page VPN - Virtual Private\\ Network . Here's an example of the process shown in screenshots: At this point you need to use either `push` for duo factor auth on the smart device, or `phone` for a call or `sms` for a collection of one time passcodes. It's also worth noting that on linux openconnect http://www.infradead.org/openconnect/ is a great tool for the job. Installing visit local machine https://visit-dav.github.io/visit-website/ https://visit-dav.github.io/visit-website/releases-as-tables/#series-31 Installing the HPC host profiles download the host_uahpc2.xml file and save it to the visit directory in your home folder. Here's what that looks like in my case: host_uahpc2.xml Once downloaded, transfer it to `~/.visit/hosts` running visit Navigate to the visit3.1.4 folder who's contents should appear like this Enter the `bin` folder and start visit in debug mode ``` cd bin ./visit -debug 5 ``` This mode will be helpful for tracking down any errors that come up when configuring the client server session. Filling in host profile There are the values that need to be changed in the host profile to support your user on the HPC. Open the host profiles from the options tab and then select UAHPC2 On the Host Settings page we will update the following entries that are highlighted. The first one is the path to the visit installation on the HPC. Here you must get the absolute path to the VisIt installation on the HPC which should be /groups/<PIname>/visit_setup/visit3_1_4.linux-x86_64 replacing <PIname> with your PI's name. If you installed VisIt somewhere else you will use a different absolute path to that location. Second you must replace baylyd with your own netid Then we will change the options in the Launch Profiles by selecting that tab in the Host Profiles window. Then select the parallel settings tab ensuring to replace visteam with your own PI Once that's complete you may attempt to open a session Select the UAHPC2 host Then navigate to a data set you wish to visualize in the client server session In my case I'll be using the sample aneurysm data which can be downloaded here https://visit-dav.github.io/largedata/datarchives/aneurysm You will then have the option to select a compute launcher configuration. I'm going with elgato . Again check to make sure that the Bank field has your PI's netid in it. After selecting OK you will see a progress bar: This is displayed while the slurm batch allocation that was automatically generated for you waits in a queue. When it is approved and launched the progress window goes away. You can double check that the VisIt session is running successfully on the HPC by logging into hpc.arizona.edu and running squeue -u <netid> . You will see an entry like this who's name is visit.<netid> At this point you can request for VisIt to perform operations and graph different aspects of your data. If your data is timeseries you may also hit the play button and it will step through your data visualizing each step. Support related to this document If you are interested in this workflow but need support you can send an email to vislab-consult@list.arizona.edu for technical support a Data & Visualization consultant. {\"serverDuration\": 18, \"requestCorrelationId\": \"79d8fd98c6fa410395673f3939c1f8ca\"} Was this page informative? Thanks for your feedback! Thanks for your feedback! Have suggestions for improvements? Contact us . Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/VisIt",
      "title": "VisIt - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "786cb90a-6222-4a79-9a44-dee0c6fff402",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.34",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/VisIt",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Introduction to Batch Jobs ¶ What are Batch Jobs? ¶ Batch jobs differ from interactive jobs and graphical jobs because they do not require user input while running. Instead, the user writes a script containing the instructions (code) that is sent to a compute node via the scheduler (Slurm). This allows your workflow to run automatically without you needing to be physically present. Here are a few benefits of using batch jobs: No Need to Stay Logged In : You don’t have to remain logged into the HPC system for your work to continue. This avoids potential issues like your terminal timing out, local internet disruptions, or your computer going to sleep—all of which could prematurely end your analysis, especially for long-running jobs. Submit Many Jobs at Once : Some workflows require running hundreds or thousands of analyses. For example, you might want to run the same script with different input values multiple times. Doing this interactively could be cumbersome or even impossible. Batch jobs can easily handle this use case . Batch Job Workflow and Analogy ¶ Think of a batch job like a researcher who wants something custom-made at a factory. There are a few steps they need to take: Get the Address : First, they need to know where the factory is so they can contact the right person to make their request. Provide Instructions : Next, they need to write instructions, or schematics, for the person who will do the manufacturing. Send the Instructions : Finally, they need to send these instructions to the factory so the builder can receive them and start working. We'll continue with this analogy, breaking down each step in more detail below. Batch Scripts ¶ When you want to run analyses in batch mode, you need to inform the system of two things: What Resources You Need : This includes specifying the number of CPUs, the amount of memory, GPUs, and other resources necessary to run your work. These instructions guide the system in selecting the appropriate compute resources and target compute nodes for your workflow. Continuing with our factory analogy, these resources can be thought of as the postal address on the outside of the envelope. The Instructions to Execute : This is a list of commands that the compute node will follow once your workflow starts. These commands are written in Bash and include everything you would normally type on the command line if you were running your work interactively. For example, you would cd to the relevant working directory, module load any required software, source virtual environments, and so forth. In our analogy, these commands are like the schematics inside the envelope that you're sending to the factory. Batch Script Structure ¶ A batch script is a text file that is written with three sections: | | | --- | | <br>#!/bin/bash<br> | | <br>#SBATCH --option=value<br> | | <br>[code here]<br> | The \" shebang \" will always be the line #!/bin/bash . This tells the system to interpret your file as a Bash script. Our HPC systems use Bash for all our environments, so it should be used in your scripts to get the most consistent, predictable results. The directives section will have multiple lines, all of which start with #SBATCH . These lines are interpreted as directives by the scheduler and are how you request resources on the compute nodes, set your output filenames, set your job name, request emails, etc. A list of directives is shown in Batch Directives . The code section in your script is a set of bash commands that tells the system how to run your analyses. This includes any module load commands you'd need to run to access your software, software commands to execute your analyses, directory changes, etc. An example batch script might look like the following: ``` !/bin/bash -------------------- Directives Section -------------------- SBATCH --job-name=hello_world SBATCH --account= SBATCH --partition=standard SBATCH --nodes=1 SBATCH --ntasks=1 SBATCH --time=00:01:00 -------------------- Code Section -------------------- module load python/3.9 cd ~/hello_world python3 hello_world.py ``` hello_world.py In the example above, we'll assume the hello_world.py script that's being executed is located in the user's home directory with the contents ``` !/usr/bin/env python3 print(\"Hello world!\") ``` Finding your group name Not sure what your group name is? Check out our allocations documentation . Submit Your Job ¶ After writing your batch script, the next step is to submit it to the scheduler. The scheduler is software designed to coordinate jobs efficiently across the system. It reads the batch directives from the second section of your batch script (described above) and uses those resource requests to determine where to send your workflow. Continuing with our factory analogy, the scheduler acts like the postman, serving as the intermediary between you and the compute nodes (the factory) and handling all communication and resource scheduling. To submit your batch job to the scheduler, use the sbatch command. This will return a Job ID, which works like a tracking number when you send a package via the post office. You can use your Job ID to monitor your analysis as it runs and retrieve resource usage data once it’s completed. An example of using sbatch is shown below: ``` (puma) [netid@wentletrap ~]$ sbatch hello_world.slurm Submitted batch job 10233657 ``` We can then check on its status using the command squeue . ``` (puma) [netid@wentletrap ~]$ squeue --job 10233657 JOBID PARTITION NAME PRIORITY USER ACCOUNT ST CPUS MIN_M NOD NODELIST(REASON) TIME_LEFT 10233657 standard hello_worl 5001 netid hpcteam PD 1 5G 1 (Priority) 1:00 ``` The ST shown above stands for \"State\" and indicates that the job is currently pending ( PD ). Once the job starts running, the state will change to R . When the job is complete, squeue will come back blank. The amount of time your job spends waiting before it starts running often is determined by various factors which include: Job Size . This includes the number of CPUs, nodes, GPUs, etc. The more resources you request, the longer it may take before your job starts running. Job Duration . Jobs with shorter runtimes will often start faster than long-running jobs. Node Type . Our clusters have many standard nodes, fewer GPU nodes, and very few high memory nodes. The high memory nodes in particular may have very long wait times. If you do not need a lot of memory for your job (e.g., on Puma, less than 470 GB), it may be more efficient to run your work on a standard node. If your job has been stuck in the queue for a very long time, ensure that you have not accidentally targeted a high memory node . Cluster Usage . The more jobs running on the cluster, the longer the wait times may be. To check how busy the cluster is, try running the command nodes-busy . Maintenance . Quarterly maintenance cycles impact queue times. We will send announcements typically a week in advance of any planned maintenance cycles and will include announcement banners in our documentation. See our maintenance section for more information. Retrieve Your Results ¶ Once your job starts running, a file will be generated in the directory where you submitted your batch script. This file logs the job's standard output (stdout) as it runs—essentially, what would have been printed to the terminal if you had run the job interactively. The file is updated in real-time, so you can log into the cluster at any time to check your job's progress. By default, this file is named slurm-<jobid>.out , but you can customize the filename using batch directives if desired. Let’s look at the output for our example job: ``` (puma) [netid@wentletrap ~]$ cat slurm-10233657.out Hello world! ``` As we can see, the script ran successfully, and Hello world! was printed to the terminal. Conclusion ¶ Batch jobs are a powerful way to automate your workflows on the HPC system, allowing you to efficiently use resources and manage long-running tasks without needing to stay logged in. By understanding how to write and submit batch scripts, you can run complex analyses, monitor their progress, and retrieve their results. If you're just getting started, here are a few additional tips: Start Small : Test your scripts with smaller jobs to ensure everything is working as expected before scaling up to larger, more resource-intensive tasks. Explore More : Take some time to explore additional batch directives and experiment with different resource requests to optimize your jobs. Ask for Help : Don’t hesitate to reach out to our HPC support team if you have any questions or run into issues. We're here to help you get the most out of the system. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/batch_jobs/intro/",
      "title": "Intro to Batch - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "7d0032b5-247b-435d-a166-af1cae1eee51",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/batch_jobs/intro/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Monitoring Jobs and Resources ¶ Monitoring Jobs ¶ Tip Some of these functions are specific to the UArizona HPC, and may not work if invoked on other systems. Every job has a unique ID associated with it that can be used to track its status, view resource allocations, and resource usage. Below is a list of some helpful commands you can use for job monitoring. | Command | Purpose | Example | | --- | --- | --- | | squeue --job=<jobid> | Retrieves a running or pending job's status. | squeue --job=12345 | | squeue --me | Retrieves all your running and pending jobs | | | scontrol show jobs <jobid> | Retrieve detailed information on a running or pending job | scontrol show job 12345 | | scancel <jobid> | Cancel a running or pending job | scancel 12345 | | job-history <jobid> | Retrieves a running or completed job's history in a user-friendly format | job-history 12345 | | seff <jobid> | Retrieves a completed job's memory and CPU efficiency | seff 12345 | | past-jobs | Retrieves past jobs run by user. Can be used with option -d <n> to search for jobs run in the past <n> days | past-jobs -d 5 | | job-limits <group_name> | View your group's job resource limits and current usage. | job-limits mygroup | Slurm Reason Codes ¶ Sometimes, if you check a pending job there is a message under the field Reason indicating why your job may not be running. Some of these codes are non-intuitive so a human-readable translation is provided below: | Reason | Explanation | | --- | --- | | AssocGrpCpuLimit | Your job is not running because your group CPU limit has been reached 1 | | AssocGrpMemLimit | Your job is not running because your group memory limit has been reached 1 | | AssocGrpCPUMinutesLimit | Either your group is out of CPU hours or your job will exhaust your group's CPU hours. | | AssocGrpGRES | Your job is not running because your group GPU limit has been reached 1 | | Dependency | Your job depends on the completion of another job. It will wait in queue until the target job completes. | | QOSGrpCPUMinutesLimit | This message indicates that your high priority or qualified hours allocation has been exhausted for the month. | | QOSMaxWallDurationPerJobLimit | Your job's time limit exceeds the max allowable and will never run 1 | | Nodes_required_for_job_are_DOWN,_DRAINED_or_reserved_or_jobs_in_higher_priority_partitions | This very long message simply means your job is waiting in queue until there is enough space for it to run | | Priority | Your job is waiting in queue until there are enough resources for it to run. | | QOSMaxCpuPerUserLimit | Your job is not running because your per-user CPU limit has been reached 1 | | ReqNodeNotAvail, Reserved for maintenance | Your job's time limit overlaps with an upcoming maintenance window. Run \"uptime_remaining\" to see when the system will go offline. If you remove and resubmit your job with a shorter walltime that does not overlap with maintenance, it will likely run. Otherwise, it will remain pending until after the maintenance window. | | Resources | Your job is waiting in queue until the required resources are available. | Monitoring System Resources ¶ We have a number of system commands that can be used to view the availability of resources on each cluster. This may be helpful in determining which cluster to use for your analyses | Command | Purpose | | --- | --- | | nodes-busy | Display a visualization of each node on a cluster and overall usage. Use nodes-busy --help for more detailed options. | | cluster-busy | Display a visual overview of each cluster's usage | | system-busy | Display a text-based summary of a cluster's usage | For more information on resource limitations, see: Job Limits . ↩ ↩ ↩ ↩ ↩ Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/monitoring_jobs_and_resources/",
      "title": "Monitoring Jobs and Resources - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "7f03971a-a2ed-46ad-a561-ba046bca3f60",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/monitoring_jobs_and_resources/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Cameras and Keyframes When working with ParaView, it can be helpful to generate images or videos to show to others. Fortunately, ParaView has many built-in features that allow you to do just that. Basic Image Capture The simplest way to capture an image from ParaView is to click the camera icon to the left just above the 3D view (the one with four corners around it). Clicking it will open a window asking where you would like to save the image just as you would any file. Holding Ctrl, Shift, or Alt will take the photo and copy it to your clipboard to paste it somewhere rather than saving it to your files. The photo will always be whatever you currently see in your viewport. To change your view you can use the left, middle, and right mouse buttons to move around or click the camera with the pencil on it located above the viewport to enter numeric values. Animation Capture Rather than repeatedly clicking the save image button, ParaView offers ways of storing animations of your data. The simplest way to do this is using extractors. To get a better understanding of how extractors work, check out this page for more information. From the toolbar, select Extractors → Image then select PNG or JPG depending on your preference. PNG is generally higher quality with a bigger file size while JPG is lower quality with a smaller file size. After this, you're all set to save your images. Position your camera wherever you'd like then do File → Save Extracts. This will ask you to select a folder to save them to, then save the files to that folder. By default, it will save every frame of your scene as an image. You can control this and other properties in the Properties tab of the extractor. Animation With A Moving Camera If you would like the camera to move through your animation, this can be occasionally tedious but nonetheless doable. You’ll first need to open the animation view from View → Animation View, click the dropdown next to the blue plus sign in the animation view, select Camera, select the movement option that works best for you, then press the blue plus. I've outlined the different movement options below: Orbit Orbit allows you to define a circle that the camera will follow over the animation. The three parameters are as follows. Center: A vector representing the point the circle surrounds (and where the camera looks at). Normal: A vector that will be perpendicular to the circle (aka the axis that you want to rotate around). Origin: A vector indicating the starting point of the camera. Follow Path Follow Path will create an orbit around the selected object's starting position. This is a simpler way of creating an orbit but it does not provide as many options. Follow Data Follow Data has the camera track the movement of the selected object. The camera's starting position will be wherever it is when this is added to the animation view. The tracking is done by following the average position of all the points and thus isn't affected by uniform scaling or rotation. Interpolate Camera Path (Spline/Linear) This option is the most versatile but also most difficult to use option. It allows you to create a custom path for your camera based on where it should be at certain times. After adding this to the animation view, double-click the new Camera animation target. This will bring up a page where you can add all the points of your path. As a workflow for this, I recommend moving to the frame and position you want to capture, adding a keyframe in the editor, changing its time value to the current frame, double click on the position, and selecting Use Current. This is still tedious, just slightly less so that individually filling in values. The difference between spline and linear has to do with where the camera is between keyframes. If you choose linear, it will transition with straight lines between the points. If you choose spline, it will give a more flowing path. Capturing Graphs and Other Visualizations The process for capturing graphs and other types of visualizations is very similar to the process for the viewport. For a single frame, a camera icon is shown in the top left of the graph view. For an animation, add a PNG/JPG extractor as you would with the viewport but make sure that the plot is selected rather than the viewport (click on it, a blue border should appear). To see what image capture is capturing what, click between your viewport and graph(s). If the blue eye icon appears to the left of the capture device in the pipeline browser, it is capturing the selected view. Once everything is set up, go to File → Save Extracts as before. If you'd like to give this a try, you can use the can2 example from Getting Started With\\ ParaView to try to create this video: The data in the histogram is the magnitude of the displacement vectors (displ) Note that as ParaView exports individual images per frame, the use of other image processing software such as Adobe Premiere is needed to make a video. {\"serverDuration\": 18, \"requestCorrelationId\": \"88acd6ef20f84d1c8ee07134a26132e0\"} Was this page informative? Thanks for your feedback! Thanks for your feedback! Have suggestions for improvements? Contact us . Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/Paraview/ParaView_Cameras_and_Keyframes",
      "title": "Cameras and Keyframes - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "898099f8-89fe-45e9-ba10-cd75076e2bc1",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.34",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/Paraview/ParaView_Cameras_and_Keyframes",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Speeding up Filters & Rendering with Client Server configurations For Both of these configurations it is recommended that you start up a remote desktop and launch paraview according to the instructions on the previous documented page Getting Started With ParaView. Using the Embedded graphics library paraview binary Steps To launch a EGL server ¶ This workflow uses offscreen rendering and a gpu to speed up interactive workflows. First request an allocation on Ocelote that has 1 gpu Please remember to replace the <your account> with your own account name salloc -A <your account> -p standard --gres=gpu:1 -t 3:00:00 -N 1 -n 16 The following lines will download a version of paraview that we can use for offscreen headless rendering. The command will actually rename the export file in order to make it easier to uncompress it. wget \"https://www.paraview.org/paraview-downloads/download.php?submit=Download&version=v5.11&type=binary&os=Linux&downloadFile=ParaView-5.11.1-egl-MPI-Linux-Python3.9-x86_64.tar.gz\" -O paraview_egl.tar.gz Here we uncompress the tar file tar xf paraview_egl.tar.gz The folder we get from this will probably include the full paraview version name so we will use a * pattern match character to go into the bin folder where the pvserver program we need to use resides cd ParaView-5.11.1*/bin/ Here's where the exciting part occurs. We will use the pvserver program to start up a process which listens for connections from the graphical interface and fulfills requests on its behalf. The extra benefit is that it will use GPU hardware accelerated rendering to get back results to the viewport even faster. We will also be making use of the embedded graphics library EGL, to perform these renderings without the need for an x-server display. The flag --displays is used to specify the gpu card index that we want to make use of, starting from the number 0. So if we had another card we wished to use instead we would use --displays=1 . The potential to leverage multiple gpus is available but requires a different compiled version of paraview with a specific nvidia license so it will not be covered here. ./pvserver --displays=0 For completeness here are the commands all together for copying in pasting into a terminal on the HPC ``` wget \"https://www.paraview.org/paraview-downloads/download.php?submit=Download&version=v5.11&type=binary&os=Linux&downloadFile=ParaView-5.11.1-egl-MPI-Linux-Python3.9-x86_64.tar.gz\" -O paraview_egl.tar.gz cd ParaView-5.11.1*/bin/ tar xf paraview_egl.tar.gz ./pvserver --displays=0 ``` Take note of the address that is written to the screen at this point it will look something like cs://<node_name>.<cluster_name>.arizona.edu:11111 and this will be used when we connect via the gui client Steps to launch a client to connect to the server For a first draft of this page we will rely on the documentation provided on the paraview read the docs page to understand the process of connecting from the graphical user interface. https://docs.paraview.org/en/latest/ReferenceManual/parallelDataVisualization.html#configuring-a-server-connection Once the connection has been made, it is possible to determine the extent to which the gpu is utilized by putting the running ./pvserver in the background with ctrl-z and then typing bg . At this point you can now type nvidia-smi -l 2 to get a log of the activity on the gpu in a text table. Using multinode MPI and osmesa for ¶ This process is very similar to the above, but we will be asking for a slightly different allocation. Use the following command on Elgato for testing having two full nodes of 16 cpu cores to speed up filtering and rendering with paraview. After this tutorial feel free to test out using different machines or node and core counts. The rule being that the total number of cpu cores is specified in the -n flag and the total number of nodes is specified with the -N flag. For more information consult the basic Slurm scheduling pages. Please remember to replace the with your own account name interactive -a <your account> -N 2 -n 32 -t 1:00:00 When the first of the multiple nodes is allocated go and download the latest paraview osmesa. wget \"https://www.paraview.org/paraview-downloads/download.php?submit=Download&version=v5.11&type=binary&os=Linux&downloadFile=ParaView-5.11.1-osmesa-MPI-Linux-Python3.9-x86_64.tar.gz\" -O paraview_mesa.tar.gz The next step is to untar it tar xf paraview_mesa.tar.gz Then at this point you are ready to navigate into the bin folder cd Paraview*/bin Now use the provided mpiexec binary to start up a pvserver backed up by all our allocated cores ./mpiexec -n 32 ./pvserver Once again this is an offscreen rendering system able to run without needing to have displays or anything. A side benefit is since this is an entire osmesa build, no missing opengl dependencies cause problems. Again we need to pay attention to the output of this command so that we can see the server's connection url to use. For instance if the mpi nodes are gpu1&gpu2 it will look like cs://gpu2.elgato.hpc.arizona.edu:11111 At this point we are now all set in order to start up the paraview gui in the remote desktop allocation. Again, please refer back to the Getting Started with Paraview documentation for instructions on this TODO replace this with the link in confluence. Once the gui is open, configure a new connection, and put in just the server address ie gpu2.elgato.hpc.arizona.edu for the host, then supply the port as 11111 After about 10-30 seconds you will be connected and all of the filters and rendering will be accelerated by parallel multinode-mpi. To double check whether your cores are being fully utilised consider using the following. First open a separate terminal ssh into each of the mpi nodes and run htop to see their individual cpu core utilization. As you interact with the paraview gui client the multinode-mpi server will undergo periods of low and high activity observable in the htop display. Please consult the video below for any additional questions or reach out to the vislab-consult@list.arizona.edu **Note that this video is using less mpi resources because waiting on two full elgato nodes wasn't allocating very quickly and made it harder to record the video. Just remember instead of -n 8 you can use -n 32 and then go out for a quick snack.** Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/speeding_up_filters_and_rendering_with_client_server_configurations/",
      "title": "Speeding up Filters & Rendering with Client Server configurations - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "9ce5ce56-3ef6-4a0c-bdc3-bab098da038d",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/speeding_up_filters_and_rendering_with_client_server_configurations/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Cameras and Keyframes When working with ParaView, it can be helpful to generate images or videos to show to others. Fortunately, ParaView has many built-in features that allow you to do just that. Basic Image Capture ¶ The simplest way to capture an image from ParaView is to click the camera icon to the left just above the 3D view (the one with four corners around it). Clicking it will open a window asking where you would like to save the image just as you would any file. Holding Ctrl, Shift, or Alt will take the photo and copy it to your clipboard to paste it somewhere rather than saving it to your files. The photo will always be whatever you currently see in your viewport. To change your view you can use the left, middle, and right mouse buttons to move around or click the camera with the pencil on it located above the viewport to enter numeric values. Animation Capture ¶ Rather than repeatedly clicking the save image button, ParaView offers ways of storing animations of your data. The simplest way to do this is using extractors. To get a better understanding of how extractors work, check out Essential Ideas for more information. From the toolbar, select Extractors → Image then select PNG or JPG depending on your preference. PNG is generally higher quality with a bigger file size while JPG is lower quality with a smaller file size. After this, you're all set to save your images. Position your camera wherever you'd like then do File → Save Extracts . This will ask you to select a folder to save them to, then save the files to that folder. By default, it will save every frame of your scene as an image. You can control this and other properties in the Properties tab of the extractor. Animation With A Moving Camera ¶ If you would like the camera to move through your animation, this can be occasionally tedious but nonetheless doable. You’ll first need to open the animation view from View → Animation View , click the dropdown next to the blue plus sign in the animation view, select Camera, select the movement option that works best for you, then press the blue plus. I've outlined the different movement options below: Orbit ¶ Orbit allows you to define a circle that the camera will follow over the animation. The three parameters are as follows. Center: A vector representing the point the circle surrounds (and where the camera looks at). Normal: A vector that will be perpendicular to the circle (aka the axis that you want to rotate around). Origin: A vector indicating the starting point of the camera. Follow Path ¶ Follow Path will create an orbit around the selected object's starting position. This is a simpler way of creating an orbit but it does not provide as many options. Follow Data ¶ Follow Data has the camera track the movement of the selected object. The camera's starting position will be wherever it is when this is added to the animation view. The tracking is done by following the average position of all the points and thus isn't affected by uniform scaling or rotation. Interpolate Camera Path (Spline/Linear) ¶ This option is the most versatile but also most difficult to use option. It allows you to create a custom path for your camera based on where it should be at certain times. After adding this to the animation view, double-click the new Camera animation target. This will bring up a page where you can add all the points of your path. As a workflow for this, I recommend moving to the frame and position you want to capture, adding a keyframe in the editor, changing its time value to the current frame, double click on the position, and selecting Use Current. This is still tedious, just slightly less so that individually filling in values. The difference between spline and linear has to do with where the camera is between keyframes. If you choose linear, it will transition with straight lines between the points. If you choose spline, it will give a more flowing path. Capturing Graphs and Other Visualizations ¶ The process for capturing graphs and other types of visualizations is very similar to the process for the viewport. For a single frame, a camera icon is shown in the top left of the graph view. For an animation, add a PNG/JPG extractor as you would with the viewport but make sure that the plot is selected rather than the viewport (click on it, a blue border should appear). To see what image capture is capturing what, click between your viewport and graph(s). If the blue eye icon appears to the left of the capture device in the pipeline browser, it is capturing the selected view. Once everything is set up, go to File → Save Extracts as before. If you'd like to give this a try, you can use the can2 example from Getting Started With ParaView GUI to try to create this video: The data in the histogram is the magnitude of the displacement vectors (displ). Note that as ParaView exports individual images per frame, the use of other image processing software such as Adobe Premiere is needed to make a video. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/cameras_and_keyframes/",
      "title": "Cameras and Keyframes - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "a2b524df-b86e-4813-ad08-d974437f219e",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/cameras_and_keyframes/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Getting Started with Paraview Terminal Welcome to this tutorial on PvPython, ParaView’s terminal that holds all the same functionality as ParaView’s GUI interface, but allows for an interface more familiar to terminal users. If you haven’t already, check out Getting Started for the installation and basic functionality of the software. This tutorial will follow the same procedure as the GUI tutorial, of course, except that it uses the terminal. A note on using the terminal: PvPython is not particularly intuitive and not particularly well documented. If you're interested in creating scripts I would recommend reading this entire article (especially How To Find Python Commands ) before getting started. ParaView Terminal ¶ To get started with PvPython, find the pvpython executable on your computer. The first thing you should do after opening the terminal is run ``` from paraview.simple import * ``` This will give you all the base functionality. To access methods outside the simple package, those need to be imported separately. Basic Example ¶ Let's start off by creating a sphere and rendering it from the terminal. First, you'll want to create a sphere object with the following: ``` sphere = Sphere() ``` This creates the sphere but we also need to “show” it to make it visible in the render. Do this by calling ``` Show() ``` Finally, to view the sphere call the command ``` Render() ``` This will bring up a window in which you can see your sphere. Notice that if you hover over this window it may say Not Responding . That is expected, the window will close when you close PvPython and will update whenever Render is called in the terminal. If you want, you can call Interact() to be able to interact with the view, including moving the camera, moving the window, and closing the window. You may notice the sphere has a pretty low resolution. Let's try making it rounder by updating its parameters. Spheres have two parameters that contribute to their resolution: ThetaResolution and PhiResolution . ThetaResolution is the number of points you have for each horizontal plane (lines of latitude) and PhiResolution is the number of points in each vertical slice (lines of longitude). Let's try increasing both of these to get a better sphere. ``` sphere.ThetaResolution = 20 sphere.PhiResolution = 20 Render() # Changes wont update untill Render() is called ``` There you go! The sphere should get a bit smoother. Feel free to mess around with different parameters and Render() them to see the result. A list of parameters can be seen by calling dir(sphere) . How To Find Python Commands ¶ While some PvPython commands are intuitive or well documented, there are many commands that are difficult if not impossible to find information about. If this is the case and you're trying to find a way to do something you know you can do in the GUI version, you can use the following steps. In the ParaView GUI, click Tools → Start Trace from the toolbar. Click OK on the popup. Next, do whatever action you would like the PvPython command for. Once you're done, click Tools → End Trace and a window should pop up giving you the python commands for all the actions you did while the trace was running. This is also a useful way to build scripts without the need for a lot of coding. It is important to note however that the trace generates a lot of redundant or unnecessary code. If you want your program running as fast as possible, it's best to go through and only keep what you need. Running PvPython or a PvPython script within ParaView GUI ¶ To do this, all you need to do is click View → Python Shell from the toolbar. This will open a PvPython shell the same as the one in the terminal. At the bottom of this window, there is a button labeled Run Script . Clicking this will allow you to select a PvPython script to run within ParaView. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/getting_started_with_paraview_terminal/",
      "title": "Getting Started with Paraview Terminal - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "a6ae70f6-0b61-474c-90d4-d25dcc652525",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/getting_started_with_paraview_terminal/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Job Limits ¶ Job Limits ¶ To ensure equitable access and efficient use of our clusters, we impose restrictions on the total compute resources available to individual users, jobs, and HPC groups. These limits vary based on the specific cluster and partition in use. The table below outlines the maximum resources that can be concurrently used at both the user and group levels. Open OnDemand jobs have different limits Note that the below limits apply to interactive terminal jobs and batch jobs . Open OnDemand jobs have further restrictions that are detailed in the Open OnDemand Job Limit section below. High priority job limits Buy-in groups are limited to using the resources they purchased. If you are a member of a buy-in group, you can check your specific limits by following the instructions in the Checking Limits and Usage below. Available hardware If you're looking for more information on the physical resources available for jobs, see our Compute Resources page for more information. | | Puma | Ocelote | El Gato | | --- | --- | --- | --- | | Global Limits | | Maximum Simultaneous User Jobs | 1000 | 1000 | 1000 | | Maximum Job Walltime | 10 days | 10 days | 10 days | | Standard Job Resources | | Maximum Memory Per Group | 16996 GB | 10000 GB | 10000 GB | | Maximum CPUs per Group | 3290 | 1024 | 1024 | | Maximum GPUs per Group | 4 | 10 | N/A | | Windfall Job Resources | | Maximum Memory per User | Unlimited | Unlimited | Unlimited | | Maximum CPUs per User | 6000 | 6000 | 6000 | | Maximum GPUs per Job | Unlimited | Unlimited | N/A | | High Priority Job Resources | | Maximum CPUs per Group | Number Purchased | N/A | N/A | | Maximum GPUs per Group | Number Purchased | N/A | N/A | Open OnDemand Jobs Limits ¶ Open OnDemand graphical jobs are more limited in size and duration than batch or interactive terminal jobs. Jobs that need more resources or longer runtimes can be submitted as batch jobs . Single node resources Note that the maximum CPUs and GPUs listed in the table below are based on the resources available by node. For information, see Compute Resources . | | Puma | Ocelote | El Gato | | --- | --- | --- | --- | | Maximum Nodes per Job | 1 | 1 | 1 | | Maximum CPUs per Job | 94 | 28 (Standard/GPU nodes) 48 (High memory node) | 16 | | Maximum GPUs per Job | 4 | 2 | N/A | | Maximum Walltime per Job | 4 days | 4 days | 4 days | Checking Limits and Usage ¶ You can view job limits for a specific group by using the job-limits <group_name> command in a terminal session. The output will display the current usage of your group and personal jobs, so the results may vary. Below is an example output from this command for a sample user who is a member of a buy-in group. The buy-in resources available to them are indicated in the High Priority field. If you are not a member of a buy-in group, this field will not appear. Please note that the output of job-limits is specific to the queried group, meaning the User Limits displayed apply only to jobs submitted by that user using the group's account. Puma Ocelote El Gato ``` (puma) [ @wentletrap ~]$ job-limits Group Limits: <group> Job Type | Memory | CPU | GPU | Job Number |Running/Limit |Running/Limit | Running/Limit |Submitted/Limit Standard | -/16998G | -/3290 | -/gres/gpu=4 | -/- High Priority | -/16998G | -/94 | -/gres/gpu=0 | -/- User Limits: <your_netid> Job Type | Memory | CPU | GPU | Job Number* |Running/Limit |Running/Limit | Running/Limit |Submitted/Limit Windfall | -/- | -/6000 | -/- | -/1000 Standard | -/16998G | -/3290 | -/gres/gpu=4 | High Priority | -/16998G | -/94 | -/gres/gpu=0 | *Max jobs across all groups and partitions. Individual Job Limits Job Type | Memory | CPU | GPU | Time Windfall | 16998G | 6000 | | 10-00:00:00 Standard | 16998G | 3290 | gres/gpu=4 | 10-00:00:00 High Priority | 16998G | 94 | gres/gpu=0 | 10-00:00:00 ``` ``` (ocelote) [ @wentletrap ~]$ job-limits Group Limits: <group> Job Type | Memory | CPU | GPU | Job Number | Running/Limit | Running/Limit | Running/Limit |Submitted/Limit Standard | -/10T | -/1024 | -/gres/gpu=10 | -/- User Limits: <your_netid> Job Type | Memory | CPU | GPU | Job Number* | Running/Limit | Running/Limit | Running/Limit |Submitted/Limit Windfall | -/- | -/6000 | -/- | -/1000 Standard | -/10T | -/1024 | -/gres/gpu=10 | *Max jobs across all groups and partitions. Individual Job Limits Job Type | Memory | CPU | GPU | Time Windfall | 8064G | 6000 | | 10-00:00:00 Standard | 8064G | 1024 | gres/gpu=10 | 10-00:00:00 ``` ``` (elgato) [ @wentletrap ~]$ job-limits Group Limits: <group> Job Type | Memory | CPU | GPU | Job Number | Running/Limit | Running/Limit | Running/Limit |Submitted/Limit Standard | -/10T | -/1024 | -/- | -/- User Limits: <your_netid> Job Type | Memory | CPU | GPU | Job Number* | Running/Limit | Running/Limit | Running/Limit |Submitted/Limit Windfall | -/- | -/6000 | -/- | -/1000 Standard | -/10T | -/1024 | -/- | *Max jobs across all groups and partitions. Individual Job Limits Job Type | Memory | CPU | GPU | Time Windfall | 8064G | 6000 | | 10-00:00:00 Standard | 8064G | 1024 | | 10-00:00:00 ``` Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/job_limits/",
      "title": "Job Limits - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "a8af14d8-8002-4d18-88a0-91fff7bc4c4d",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/job_limits/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Headless Batch Rendering When producing a rendering video we can speed things up tremendously by recognizing that each frame is independent of every other. This allows us to perform the rendering of each frame of a video output in parallel. In this tutorial you will learn to prepare a scene in ParaView, and save its \"state\" so that Slurm array jobs can then render all the frames in parallel, taking a fraction of the time it would otherwise take. At the end of the process you will use FFmpeg to stitch all the frames together into a movie. Making an animation with ParaView GUI ¶ Follow the steps in Getting Started With ParaView GUI to launch ParaView. Generate a simple geometric shape, such as a cone, for this tutorial. You can select one from the Sources → Geometric Shapes drop down menu. Select Apply to see the geometry in the viewport on the right. Select the Animation View option from the View drop down menu. A pane titled Animation View will appear in the bottom of the window. The Animation View pane will show an option to add an orbit camera animation track. Click the blue cross to add it. For more information on cameras, see Cameras And Keyframes . After adding a Camera , double click on it. This will open a window titled Animation Keyframes , containing a table. Double click the cell that says Path . This will open up a window titled Key Frame Interpolation . Selecting the Camera Position element will add a yellow line added to the viewport containing the cone geometry. You can move the yellow line from the middle of the geometric shape with the middle mouse button. Select the Up Direction element in the Key Frame Interpolation window and change the values to 0, 0, 1. Without this the camera will face sideways. Click Ok , and return to the Animation View . Set both the Number of Frames and End Time to 1000. This will be important later when we use Slurm array jobs to produce a rendering video, as it will allow Slurm to identify frames by whole numbers. Click the Play button and see the camera orbiting around the geometric shape in the viewport. If this looks correct, save the ParaView state file by either clicking the Save State button, or selecting Save State from the File drop down menu. Parallel Rendering with Slurm Array Jobs ¶ The ParaView state file represents the data corresponding to the geometric shape and the animated camera perspective. We will render it in parallel with Slurm array jobs and create a video. Start an interactive session on the HPC, and pull an Apptainer container with a headless version of ParaView 5.11.0 in it: ``` apptainer pull oras://ghcr.io/e-eight/containers/paraview-headless ``` This will create a file called paraview-headless_latest.sif . Headless means that the program does not have access to a display to make graphical outputs. This works because we will use ParaView only to render the frames, not to view the movie. If the pulling the container fails for any reason, you can create one from an Apptainer definition file containing the following: ``` Bootstrap: docker From: debian:bookworm-slim %post apt update -y apt install -y libgomp1 curl wget libglu1-mesa-dev freeglut3-dev mesa-common-dev libxcursor cd /opt wget \"https://www.paraview.org/paraview-downloads/download.php?submit=Download&version=v5.11&type=binary&os=Linux&downloadFile=ParaView-5.11.0-osmesa-MPI-Linux-Python3.9-x86_64.tar.gz\" -O paraview.tar.gz tar xf paraview.tar.gz rm paraview.tar.gz apt-get clean && rm -rf /var/lib/apt/lists/ %environment export PATH=/opt/ParaView-5.11.0-osmesa-MPI-Linux-Python3.9-x86_64/bin:$PATH ``` See Containers to learn more about Apptainer containers, and how to create them. We will now construct one Slurm script and one Python script that we will use for our rendering. The Slurm script will submit an array job. For more information, on Slurm array jobs, see Array Jobs . The Python script will contain all the logic for rendering the frames with ParaView. Slurm Script ¶ In the following script replace visteam with your allocation account. headless_batch.slurm ``` !/bin/bash SBATCH --output=logs/%x-%a.out SBATCH --error=logs/%x-%a.err SBATCH --job-name=paraview-headless SBATCH --ntasks=1 SBATCH --nodes=1 SBATCH --time=00:30:00 SBATCH --partition=standard SBATCH --account=visteam pvsm_pth=$1 apptainer exec paraview-headless_latest.sif pvpython render.py --pvsm \"$pvsm_pth\" --frame ${SLURM_ARRAY_TASK_ID} ``` As it is written this will allocate 1 CPU task for the task of headlessly rendering a single frame with an upper time limit of 30 mins. Note that there is a ${SLURM_ARRAY_TASK_ID} environment variable in use but no #SBATCH --array= line. This is because it is often nice to have the option to specify the size of the array job at run time as we will see below. This will simply start the Apptainer container for each array job and execute a script that is a wrapper around the ParaView pvpython program. For more information on the pvpython program, see Getting Started with ParaView Terminal . This next file is where many of the interesting bits actually are. Python Script ¶ This script was initially generated with the trace utility built into ParaView. With the trace utility you can make a Python script by recording interactions with the ParaView GUI. For more information, see Trace Recorder . The comments in the script explain what the code does. render.py ``` state file generated using paraview version 5.11.0 from paraview.simple import * import paraview import os import argparse from pathlib import Path import time this is for helping us determine the run time of each frame's render start = time.perf_counter() get frame number to render and the path to the pvsm file to load parser = argparse.ArgumentParser() parser.add_argument(\"--pvsm\", type=Path) parser.add_argument(\"--frame\") args = parser.parse_args() load the pvsm file this brings in our geometric shape, as well as the animation track that we created with the orbiting camera paraview.simple.LoadState(args.pvsm.name) paraview.compatibility.major = 5 paraview.compatibility.minor = 11 import the simple module from the paraview disable automatic camera reset on 'Show' paraview.simple._DisableFirstRenderCameraReset() ---------------------------------------------------------------- setup views used in the visualization ---------------------------------------------------------------- timekeeper = GetTimeKeeper() renderView1 = GetActiveViewOrCreate(\"RenderView\") ensure axes are hidden renderView1.OrientationAxesVisibility = 0 renderView1.CenterAxesVisibility = 0 ---------------------------------------------------------------- setup color maps and opacity mapes used in the visualization note: the Get..() functions create a new object, if needed ---------------------------------------------------------------- print(GetSources()) find out how many frames are in our animation anim = GetAnimationScene() print(\"animation length is\",anim.EndTime) if name == ' main ': figure out what the output folder is for the frame render_folder = Path(f\"{Path(args.pvsm.stem)}_renders\") we will make this folder if it doesn't already exist render_folder.mkdir(parents=True,exist_ok =True) get the argument provided indicating which frame we are supposed to render frame = int(args.frame) find out the maximum array job id max_array_id = int(os.environ[\"SLURM_ARRAY_TASK_MAX\"]) print(\"starting render\") as long as we aren't above the last frame number continue to run while (frame < int(anim.EndTime)): print(\"running frame,\",frame) move our animation track head to the frame we want to render anim.AnimationTime = float(frame) establish the HD and 4K file paths for our still frame render png_pth_hd = Path(f\"{render_folder}/frame_{frame:06d}_HD.png\") png_pth_4k = Path(f\"{render_folder}/frame_{frame:06d}_4K.png\") if there's already renders for this frame go ahead and skip if png_pth_hd.exists() and png_pth_4k.exists(): print(\"skipping, already exists\") frame+=max_array_id continue otherwise go ahead and render them out using the correct resolutions SaveScreenshot(f\"{png_pth_hd}\",renderView1,ImageResolution=[1920,1080]) SaveScreenshot(f\"{png_pth_4k}\",renderView1,ImageResolution=[3840,2160]) print(\"saved\") move on to the next frame that might need rendering using the number of array jobs as our offset this ensures that we can use smaller sets of array jobs and still render all the frames we need. frame+=max_array_id get an end time for performance measuring end = time.perf_counter() print(f\"elapsed {end-start}\") ``` Submitting Array Jobs ¶ Submit a Slurm job array with the following, replacing <state_file> with the path to the ParaView state file that you had saved earlier: ``` sbatch --array=0-900 headless_batch.sh ``` The --array option allows us to dynamically specify the number of array jobs that we want to run, 0-900 means we specified a job array with array indices 0 to 900. After submitting the job, it might take a while to start, depending on the resource availability of the cluster. You can check the status of the job with squeue -u <netid> , replacing <netid> with your NetID. Assuming you have not submitted any other jobs and this job has started, you will see an output like the following: ``` JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 2125119_[101-900] standard paraview sohampal PD 0:00 1 (None) 2125119_4 standard paraview sohampal R 0:01 1 cpu39 2125119_5 standard paraview sohampal R 0:01 1 cpu43 2125119_6 standard paraview sohampal R 0:01 1 cpu43 2125119_7 standard paraview sohampal R 0:01 1 cpu45 2125119_8 standard paraview sohampal R 0:01 1 cpu45 2125119_9 standard paraview sohampal R 0:01 1 cpu45 2125119_10 standard paraview sohampal R 0:01 1 cpu45 2125119_11 standard paraview sohampal R 0:01 1 cpu45 2125119_12 standard paraview sohampal R 0:01 1 cpu45 2125119_13 standard paraview sohampal R 0:01 1 cpu45 2125119_14 standard paraview sohampal R 0:01 1 cpu45 2125119_15 standard paraview sohampal R 0:01 1 cpu45 2125119_16 standard paraview sohampal R 0:01 1 cpu45 2125119_17 standard paraview sohampal R 0:01 1 cpu45 2125119_18 standard paraview sohampal R 0:01 1 cpu45 2125119_19 standard paraview sohampal R 0:01 1 cpu45 2125119_20 standard paraview sohampal R 0:01 1 cpu45 ... ``` The output has been truncated to fit in to the space. You can see the jobs with array indices 101 to 900 are yet to start, while jobs with lower array indices are running. It will take close to 14 seconds per frame for a simple animated scene like this, and since there are more frames to render than there are jobs to run, some jobs will take longer than others because they will render more than one frame. Perform either an HD or a 4K render of your material, instead of both, if the render times become too long. Rendering Movie ¶ From an interactive session load the ffmpeg module: ``` module load ffmpeg ``` Stitch together all the frames to generate a video, replacing <video_name> with the desired name for your video file: ``` ffmpeg -i frame_%06d_HD.png -r 10 -c:v copy ``` You can automate the creation of this video with Slurm job dependencies. For more information, see Job Dependencies . This is the highest possible quality video that you can use in other video editing software without any codec. It shows the entire render in sequence even though all the frames were generated in parallel. Below is a gif created from the video file for a cone geometry. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/headless_batch_rendering/",
      "title": "Headless Batch Rendering - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "ac76f7ee-6542-478b-9087-40eee404d619",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/headless_batch_rendering/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Batch Directives ¶ Tip For a full list of directives, see Slurm's official documentation . The first section of a batch script (after the shebang) always contains the Slurm Directives, which specify the resource requests for your job. The scheduler parses these in order to allocate CPUs, memory, walltime, etc. to your job request. Allocations and Partitions ¶ The partitions, or queues, on the UArizona HPC which determine the priority of your jobs and resources available to them are shown in the table below. With the exception of Windfall, these consume your monthly allocation. See our allocations documentation for more detailed information on each. The syntax to request each of the following is shown below: Buy-in users must use the --qos directive If you're a member of a buy-in group and are trying to use your high priority hours, ensure you are including a --qos directive. When this directive is missing, you will recieve the error: ``` sbatch: error: QOSGrpSubmitJobsLimit sbatch: error: Batch job submission failed: Job violates accounting/QOS policy (job submit limit, user's size and/or time limits) ``` | Partition | Request Syntax | Comments | | --- | --- | --- | | Standard | <br>#SBATCH --account=<PI GROUP><br>#SBATCH --partition=standard<br> | Request a CPU-only node using standard hours. | | Standard GPU | <br>#SBATCH --account=<PI GROUP><br>#SBATCH --partition=gpu_standard<br>#SBATCH --gres=gpu:<options><br> | Request GPU resources using standard hours. See the GPUs section below for details on the gres directive. | | Windfall | <br>#SBATCH --partition=windfall<br> | Unlimited access. Preemptible. Do not include an --account flag when requesting this partition. | | Windfall GPU | <br>#SBATCH --partition=gpu_windfall<br>#SBATCH --gres=gpu:<options><br> | Request GPU resources using windfall. See the GPUs section below for details on the gres directive. Do not include an --account flag when requesting this partition. | | High Priority | <br>#SBATCH --account=<PI GROUP><br>#SBATCH --partition=high_priority<br>#SBATCH --qos=user_qos_<PI GROUP><br> | Request a CPU-only node with high priority resources. Only available to buy-in groups . | | High Priority GPU | <br>#SBATCH --account=<PI GROUP><br>#SBATCH --partition=gpu_high_priority<br>#SBATCH --qos=user_qos_<PI GROUP><br>#SBATCH --gres=gpu:<options><br> | Request GPU resources with high priority hours. Only available to buy-in groups . See the GPUs section below for details on the gres directive. | | Qualified | <br>#SBATCH --account=<PI GROUP><br>#SBATCH --partition=standard<br>#SBATCH --qos=qual_qos_<PI GROUP><br> | Available to groups with an activate special project . | | Qualified GPU | <br>#SBATCH --account=<PI GROUP><br>#SBATCH --partition=gpu_standard<br>#SBATCH --qos=qual_qos_<PI GROUP><br>#SBATCH --gres=gpu:<options><br> | Request GPU resources with qualified hours. Available to groups with an activate special project . See the GPUs section below for details on the gres directive. | CPUs ¶ Each job must specify the requested number of CPUs with the --ntasks directive. This can be done in one of two ways: If your application is making use of MPI or is executing simultaneous distinct processes, you can request <N> CPUs with ``` SBATCH --ntasks= ``` If you are using a multithreaded application, then you can request <N> CPUs with: ``` SBATCH --ntasks=1 SBATCH --cpus-per-task= ``` Nodes ¶ Single vs. Multi-Node Programs In order for your job to make use of more than one node, it must be able to make use of something like MPI. If your application is not MPI-enabled, always set --nodes=1 The term node refers to the number of physical computers allocated to your job. The syntax to allocate <N> nodes to a job is: ``` SBATCH --nodes= ``` Time ¶ The syntax for requesting time for your job is HHH:MM:SS or DD-HHH:MM:SS . The maximum amount of time that can be requested is 10 days for a batch job. More details in Job Limits . ``` SBATCH --time=HHH:MM:SS ``` Memory and High Memory Nodes ¶ Memory and CPUs are connected More detailed information on memory and CPU requests can be found on our CPUs and Memory page . Include Units If you exclude gb from your memory request, Slurm will default to mb . Memory is an optional flag. By default, the scheduler will allocate you the standard CPU/memory ratio available on the cluster. Memory can either be requested with the --mem or --mem-per-cpu flags. The --mem flag indicates the amount of Memory per node to allocate to your job. If you are running multi-node MPI jobs with this flag, the total amount of memory you will receive will be mem × nodes The general syntax for requesting <N> GB of memory per node is ``` SBATCH --mem= gb ``` or, to request <N> GB of memory per CPU: ``` SBATCH --mem-per-cpu= gb ``` High Memory Node Requests To request a high memory node, you will need the additional flag --constraint=hi_mem . It is recommended to use the exact directives below to avoid unexpected behavior. | Cluster | Command | | --- | --- | | Ocelote | <br>#SBATCH --mem-per-cpu=41gb<br>#SBATCH --constraint=hi_mem<br> | | Puma | <br>#SBATCH --mem-per-cpu=32gb<br>#SBATCH --constraint=hi_mem<br> | GPUs ¶ GPU partitions must be used GPU jobs will need to use GPU-specific partitions. See the partitions section at the top of this page for details. GPU options are per node When using --gres=gpu:N , keep in mind that the total number of GPUs the job is allocated is N per node. GPUs are an optional resource that may be requested with the --gres directive. For an overview of the specific GPU resources available on each cluster, see our resources page . | Cluster | Directive | Target | | Puma | <br>#SBATCH --gres=gpu:1<br> | Request a single GPU. This will either target one Volta GPU (v100) or one A100 MIG slice , depending on availability. Only one GPU should be selected with this method to avoid being allocated multiple MIG slices. | | <br>#SBATCH --gres=gpu:nvidia_a100_80gb_pcie_2g.20gb<br> | Target one A100 MIG slice. | | <br>#SBATCH --gres=gpu:volta:N<br> | Request N V100 GPUs where 1≤ N ≤4 | | Ocelote | <br>#SBATCH --gres=gpu:N<br>#SBATCH --mem-per-cpu=8gb<br> | Request N GPUs, where 1≤ N ≤2. This will target either one or two Pascals (p100s) | Job Arrays ¶ Array jobs in Slurm allow users to submit multiple similar tasks as a single job. Each task within the array can have its own unique input parameters, making it ideal for running batch jobs with varied inputs or executing repetitive tasks efficiently. The flag for submitting array jobs is: ``` SBATCH --array= - ``` where <N> and <M> are integers. For detailed information on job arrays, see our job array tutorial . Job Dependencies ¶ Slurm job dependencies allow users to submit to a series of jobs that depend on each other using the flag and options: ``` --dependency= ``` For example, say job B depends on the successful completion of job A . Job B can be submitted as a dependency of job A using the following method: ``` [netid@junonia ~]$ sbatch A.slurm Submitted batch job 1939000 [netid@junonia ~]$ sbatch --afterok:1939000 B.slurm ``` This tells the scheduler to hold job B until job A completes. The afterok is the dependency <type> , in this case it ensures that job B runs only if job A completes successfully. The different options for <type> are show below: | Dependency Type | Meaning | | --- | --- | | after | Job can begin after the specified job(s) have started | | afterany | Job can begin after the specified job(s) have terminated. Job(s) will start regardless of whether the specified jobs failed or ran successfully | | afterok | Job can begin after the specified job(s) have completed successfully. If the specified job(s) fail, the dependency will never run. | | afternotok | Job can begin after the specified job(s) have failed. If the specified job(s) complete successfully, the dependency will never run. | Output Filenames ¶ The default output filename for a slurm job is slurm-<jobid>.out . If desired, this can be customized using the directives ``` SBATCH -o output_filename.out SBATCH -e output_filename.err ``` Filenames take patterns that allow for job information substitution. A list of filename patterns is shown below. | Variable | Meaning | Example Slurm Directive(s) | Sample Output | | --- | --- | --- | --- | | %A | A job array's main job ID | <br>#SBATCH --array=1-2<br>#SBATCH -o %A.out<br>#SBATCH --open-mode=append<br> | 12345.out | | %a | A job array's index number | <br>#SBATCH --array=1-2<br>#SBATCH -o %A_%a.out<br> | 12345_1.out 12345_2.out | | %J | Job ID plus stepid | <br>#SBATCH -o %J.out<br> | 12345.out | | %j | Job ID | <br>#SBATCH -o %j.out<br> | 12345.out | | %N | Hostname of the first compute node allocated to the job | <br>#SBATCH -o %N.out<br> | r1u11n1.out | | %u | Username | <br>#SBATCH -o %u.out<br> | netid.out | | %x | Job name | <br>#SBATCH --job-name=JobName<br>#SBATCH -o %x.out<br> | JobName.out | Additional Directives ¶ | Command | Purpose | | --- | --- | | <br>#SBATCH --job-name=JobName<br> | Optional: Specify a name for your job. This will not automatically affect the output filename. | | <br>#SBATCH -e output_filename.err<br>#SBATCH -o output_filename.out<br> | Optional: Specify output filename(s). If -e is missing, stdout and stderr will be combined. | | <br>#SBATCH --open-mode=append<br> | Optional: Append your job's output to the specified output filename(s). | | <br>#SBATCH --mail-type=BEGIN|END|FAIL|ALL<br> | Optional: Request email notifications. Beware of mail bombing yourself. | | <br>#SBATCH --mail-user=email@address.xyz<br> | Optional: Specify email address. If this is missing, notifications will go to your UArizona email address by default. | | <br>#SBATCH --export=VAR<br> | Optional: Export a comma-delimited list of environment variables to a job. | | <br>#SBATCH --export=all<br> | Optional: Export your working environment to your job. This is the default. | | <br>#SBATCH --export=none<br> | Optional: Do not export working environment to your job. | Examples and Explanations ¶ The below examples are complete sections of Slurm directives that will produce valid requests. Other directives can be added (like output files), but they are not strictly necessary to submit a valid request. For simplicity, the Puma cluster is assumed when discussing memory and GPU resources. Note that these examples do not include the shebang #!bin/bash statement, which should be at the top of every Slurm script. Also, note that the order of directives does not matter. Single CPU Single Node Single GPU Node Multi-Node High-Memory Node ``` SBATCH --job-name=hello_world SBATCH --account=your_group SBATCH --partition=standard SBATCH --nodes=1 SBATCH --ntasks=1 SBATCH --time=01:00:00 ``` This example requests one CPU on one node for one hour. Easy! ``` SBATCH --job-name=hello_world SBATCH --account=your_group SBATCH --partition=standard SBATCH --nodes=1 SBATCH --ntasks=10 SBATCH --time=01:00:00 ``` 10 CPUs are now requested. The default value of mem-per-cpu is assumed, therefore giving this job 50 GB of total memory. Specifying this value by including #SBATCH --mem-per-cpu=5gb will not change the behavior of the above request. The example below will produce an equivalent request as above: ``` SBATCH --job-name=hello_world SBATCH --account=your_group SBATCH --partition=standard SBATCH --nodes=1 SBATCH --ntasks=1 SBATCH --mem=50gb SBATCH --time=01:00:00 ``` On Puma, up to 94 CPUs or 470 GB of memory can be requested. NEW! July 31, 2024 Partitions update Beginning July 31, GPU jobs must use a GPU partition. See the partitions section at the top of this page for details. ``` SBATCH --job-name=hello_world SBATCH --account=your_group SBATCH --partition=gpu_standard SBATCH --nodes=1 SBATCH --ntasks=10 SBATCH --time=01:00:00 SBATCH --gres=gpu:1 ``` Note the gres=gpu:1 option and gpu_standard partition. When requesting a multi-node job, up to 94 --ntasks-per-node can be requested on Puma. The numbers below are chosen for illustrative purposes and can be replaced with your choice, up to system limitations. It should be noted that there is no advantage to requesting multiple nodes when the total number of CPUs needed is less than or equal to the number of CPUs on one node. ``` SBATCH --job-name=Multi-Node-MPI-Job SBATCH --account=your_group SBATCH --partition=standard SBATCH --ntasks=30 SBATCH --nodes=3 SBATCH --ntasks-per-node=10 SBATCH --time=01:00:00 ``` When requesting a high memory node, include both the --mem-per-cpu and --constraint directives. ``` SBATCH --job-name=High-Mem-Job SBATCH --account=your_group SBATCH --partition=standard SBATCH --nodes=1 SBATCH --ntasks=94 SBATCH --mem-per-cpu=32gb SBATCH --constraint=hi_mem SBATCH --time=01:00:00 ``` Groups and users are subject to limitations on resource usage. For more information, see job limits . ↩ Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/batch_jobs/batch_directives/",
      "title": "Batch Directives - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "b4b049d4-fed4-40da-843d-7612748b61a3",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/batch_jobs/batch_directives/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Headless Batch Rendering When producing rendering a video we can speed things up tremendously by recognizing that each frame is independent of every other. This allows us to perform the rendering of each frame of a video output in parallel. In this tutorial you will learn to prepare a scene in Paraview, and save its \"state\" so that a slurm array batch processing job can then render all the frames in a fraction of the time. At the end of the process we will use ffmpeg to stitch all the frames together into a movie that we can use to share our visualization results. Part one: Making an animation in the Paraview Graphical User Interface First follow the steps to launch Paraview that are explained in Getting\\ Started With\\ ParaView , and Visualization With\\ ParaView . Go ahead and generate a simple geometry that we are going to use for visualizing. Here I'm going with a cone shape selected from the Sources drop down menu. Make sure to select apply or you won't see your geometry in the viewport on the right. Next, make sure the animation view is visible by using the drop down menu . We are going to add an orbit camera animation track which is also talked about in greater detail here ParaView Cameras and\\ Keyframes . Don't forget to select the blue cross to add the track. Then double click on the Camera and double click the cell that says Path . This opens up a window with another panel, and once you select the positions element you will see a yellow line added to the viewport containing the cone geometry. Use the middle mouse button to move the track slightly of from the mid line of the cone Then select the Up Direction element and make sure that we have 0,0,1 or the camera will be facing sideways Click Ok, and return to the animation view. On here we are going to set the number of frames to 1000 and the end time to 1000. This distinction is important for allowing our parallel tasks to use whole numbers to identify which frame to render. At this point you should be able to click the play button and see the camera orbiting around the cone in the viewport. If this looks correct to you save the paraview state file with the name cone_orbit.pvsm with the file drop down menu. Part two: Slurm Array Batch Parallel Rendering We are ready to render now that we have the paraview state file representing our data and the animated camera perspective we want to make into a video. The first step is to get the singularity container that has a headless version of paraview in it. This means that we don't have to have a display connected for the program to be able to make graphical outputs. The code below is ``` singularity pull -F docker://ghcr.io/devinbayly/paraview_headless:latest ``` There will probably be more output if you haven't done this container pull before since it will need to fetch the individual layers and add them to the sif file that is created in the current directory. We will now construct 3 separate scripts that we will use for our rendering. The first one is the slurm array job batch submission script. For more information about these kinds of allocation requests read up on Running Jobs with\\ Slurm , and Job Examples . In this code you will need to replace your allocation account where it says visteam , but that is the only critical modification. headless_batch.sh ``` !/bin/bash SBATCH --output=logs/Sample_SLURM_Job-%a.out SBATCH --job-name=paraview_headless SBATCH --ntasks=1 SBATCH --nodes=1 SBATCH --time=00:30:00 SBATCH --partition=standard SBATCH --account=visteam mkdir -p logs pvsm_pth=$1 singularity exec paraview_headless_latest.sif /bin/bash render_headless.sh \"$pvsm_pth\" ${SLURM_ARRAY_TASK_ID} ``` As it is written this will allocate 1 cpu task for the task of headlessly rendering a single frame with an upper time limit of 30 mins. Note that there is a ${SLURM_ARRAY_TASK_ID} environment variable in use but no #SBATCH --array= line. This is because it is often nice to have the option to specify the size of the array job at run time as we will see below. This will simply start our headless singularity container for each array job and execute a script that is a wrapper around the paraview pvpython program. render_headless.sh ``` !/bin/bash /opt/ParaView-5.11.0-RC2-osmesa-MPI-Linux-Python3.9-x86_64/bin/pvpython render.py --pvsm \"$1\" --frame \"$2\" ``` This file is really only here to allow us to encapsulate a rather long command which uses the path to the paraview pvpython program inside the container to run the render.py python script with our paraview state file's path, and our array job task ID as a frame number to render. This next file is where many of the interesting bits actually are. It should also be noted that this program was initially generated with the \"trace\" utility built into paraview. This allows us to make a python script by recording interactions within the gui. https://www.paraview.org/Wiki/ParaView_and_Python#Trace_Recorder . Since most of the important explanations for this code are context sensitive I will switch to #comments in code block below. render.py ``` state file generated using paraview version 5.11.0-RC1 from paraview.simple import * import paraview import os import argparse from pathlib import Path import time import uuid this is for helping us determine the run time of each frame's render start = time.perf_counter() get frame number to render and the path to the pvsm file to load parser = argparse.ArgumentParser() parser.add_argument(\"--pvsm\") parser.add_argument(\"--frame\") args = parser.parse_args() load the pvsm file this brings in our cone geometry, as well as the animation track that we created with the orbiting camera paraview.simple.LoadState(args.pvsm) paraview.compatibility.major = 5 paraview.compatibility.minor = 11 import the simple module from the paraview disable automatic camera reset on 'Show' paraview.simple._DisableFirstRenderCameraReset() ---------------------------------------------------------------- setup views used in the visualization ---------------------------------------------------------------- timekeeper = GetTimeKeeper() renderView1 = GetActiveViewOrCreate(\"RenderView\") ensure axes are hidden renderView1.OrientationAxesVisibility = 0 renderView1.CenterAxesVisibility = 0 ---------------------------------------------------------------- setup color maps and opacity mapes used in the visualization note: the Get..() functions create a new object, if needed ---------------------------------------------------------------- print(GetSources()) find out how many frames are in our animation anim = GetAnimationScene() print(\"animation length is\",anim.EndTime) if name == ' main ': figure out what the output folder is for the frame render_folder = Path(f\"{Path(args.pvsm.stem)}_renders\") we will make this folder if it doesn't already exist render_folder.mkdir(parents=True,exist_ok =True) get the argument provided indicating which frame we are supposed to render frame = int(args.frame) find out the maximum array job id max_array_id = int(os.environ[\"SLURM_ARRAY_TASK_MAX\"]) print(\"starting render\") as long as we aren't above the last frame number continue to run while (frame < int(anim.EndTime)): print(\"running frame,\",frame) move our animation track head to the frame we want to render anim.AnimationTime = float(frame) establish the HD and 4K file paths for our still frame render png_pth_hd = Path(f\"{render_folder}/frame_{frame:06d}_HD.png\") png_pth_4k = Path(f\"{render_folder}/frame_{frame:06d}_4K.png\") if there's already renders for this frame go ahead and skip if png_pth_hd.exists() and png_pth_4k.exists(): print(\"skipping, already exists\") frame+=max_array_id continue Otherwise go ahead and render them out using the correct resolutions SaveScreenshot(f\"{png_pth_hd}\",renderView1,ImageResolution=[1920,1080]) SaveScreenshot(f\"{png_pth_4k}\",renderView1,ImageResolution=[3840,2160]) print(\"saved\") move on to the next frame that might need rendering using the number of array jobs as our offset. This ensures that we can use smaller sets of array jobs and still render all the frames we need. frame+=max_array_id get an end time for performance measuring end = time.perf_counter() print(f\"elapsed {end-start}\") ``` The following command actually kicks off our rendering job. It allows us to dynamically specify the number of array jobs that will be run, and we will pass in the slurm batch script as the second argument followed by the name of the paraview state file. This state file's name will be used to generate a folder where our images are getting saved. We are using a vertical tmux split so that we can watch the queue of these jobs and see if they are actually running. It will take close to 14seconds perframe for a simple animated scene like this, and since we have more frames to render than we have jobs to run, some jobs will take longer than others because they will render more than one frame. Note, you don't have to perform both an HD and a 4K render of your material either if the render times are becoming too long. ``` sbatch --array=0-900 headless_batch.sh cone_orbit.pvsm ``` Before moving on to the next step ensure that all tasks have cleared because even one or two missing frames will cause errors in the next video production step Making a video from the frames is a common task in visualization and is mentioned in other pages ( VisIt Workflows on UA\\ HPC , Blender Command Line\\ Rendering ). First ensure you have access to ffmpeg: ``` module load ffmpeg ``` Then use each frame as input to generate a video with no codec for the highest possible quality in other editing softwares ``` ffmpeg -i frame_%06d_HD.png -r 10 -c:v copy cone_orbit_HD.mov ``` You now should have an HD movie showing the entire render in sequence even though all the frames were generated in parallel. Enjoy! {\"serverDuration\": 18, \"requestCorrelationId\": \"200b544cc55b4e009a74f8b47a92c00e\"} Was this page informative? Thanks for your feedback! Thanks for your feedback! Have suggestions for improvements? Contact us . Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/Paraview/Paraview_Headless_Batch_Rendering",
      "title": "Headless Batch Rendering - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "b96f2e36-6c8d-4f6b-8f5a-26efd46223c4",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.34",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/Paraview/Paraview_Headless_Batch_Rendering",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Index If you need advanced Computational Geometry support for your visualization and analysis, then we have you covered! Follow these steps to get up and running with an OOD remote desktop and a custom compiled paraview container providing cgal vespa. ``` pull the container apptainer pull docker://ghcr.io/devinbayly/cgal_vespa:latest ``` When this is complete you must run the container and start paraview ``` apptainer exec cgal_vespa_latest.sif /opt/paraview_build/bin/paraview ``` When paraview starts you need to import the Vespa paraview plugin which will be at this system path ``` /usr/local/lib/paraview-5.11/plugins/VESPAPlugin/VESPAPlugin.so ``` Then the following filters can help for estimating volumes contained within areas of interest in imaging data threshold extract surface tetrahedralize Alpha Wrap (CGAL Vespa) Connectivity Compute Connected Surface Properties Refer to the screen recording shared here for other details, as well as the demo_state.pvsm which you can just load in the custom paraview container to get started. {\"serverDuration\": 17, \"requestCorrelationId\": \"6bc9cdeba83c4a7bb82f184a96d6b19b\"} Was this page informative? Thanks for your feedback! Thanks for your feedback! Have suggestions for improvements? Contact us . Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/Paraview/Advanced_Computational_Geometryin_Paraview_with_CGAL_Vespa",
      "title": "Index - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "c85a8ccc-330f-4f39-a1c7-02f2ca7a3038",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.34",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/Paraview/Advanced_Computational_Geometryin_Paraview_with_CGAL_Vespa",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Paraview Python Plugins Demystified If you've spent more than a minute with Paraview you're already aware that there are some things that could use a little more explanation. There's really just too much to cover in the depth that most beginners/intermediate users need. This page is meant to help shed a little bit of light on another subject that is shrouded in a deep layer of mystery, namely making plugins with Python. Unfortunately, this tutorial is not without sections that are less well understood, but hopefully in between those sections there's some value! For many purposes it might be useful to look through examples of scripts and plugins, so for access to an ever evolving scratch space of these kinds of files please refer to https://github.com/DevinBayly/paraview_code . Before going to much further just know this page will not document or explain things like the decoration options that create widgets for filter options. In fact, there are quite a few things related to the @prefixing of methods that won't be discussed until a later date. Due to this limitation it may be useful to layout upfront that much of the true instructional material is captured in comments on the source code from this location. https://github.com/Kitware/ParaView/blob/cdb6fa76693885dd14af49d91d9b6177710f7267/Examples/Plugins/PythonAlgorithm/PythonAlgorithmExamples.py and more generally https://github.com/Kitware/ParaView/tree/cdb6fa76693885dd14af49d91d9b6177710f7267/Examples/Plugins . Example Plugin 1 https://github.com/DevinBayly/paraview_code/blob/main/time_series_reader_plugin.py This plugin was designed to make it possible to point paraview to a folder of python numpy array files that each store a point cloud at a particular moment in time. The benefit of using this plugin is that now in paraview as we scrub through a timeline interface element the plugin will load new numpy files and update the viewport. The sections that follow will attempt to explain what the different methods of the plugin do, and this will ideally provide departure points for reader's own usecases. First the entire source is shown and then the step by step sections will follow ``` \"\"\"This module demonstrates various ways of adding VTKPythonAlgorithmBase subclasses as filters, sources, readers, and writers in ParaView\"\"\" This is module to import. It provides VTKPythonAlgorithmBase, the base class for all python-based vtkAlgorithm subclasses in VTK and decorators used to 'register' the algorithm with ParaView along with information about UI. from paraview.util.vtkAlgorithm import * import uuid from pathlib import Path ------------------------------------------------------------------------------ A reader example. ------------------------------------------------------------------------------ def createModifiedCallback(anobject): import weakref weakref_obj = weakref.ref(anobject) anobject = None def _markmodified( args, *kwars): o = weakref_obj() if o is not None: o.Modified() return _markmodified To add a reader, we can use the following decorators @smproxy.source(name=\"PythonnumpyReader\", label=\"Python-based numpy Reader\") @smhint.xml(\"\"\" \"\"\") or directly use the \"@reader\" decorator. @smproxy.reader(name=\"Sama Lidar Numpy Reader\", label=\"Python-based numpy pcd Reader for timeseries data\", extensions=\"npy\", file_description=\"numpy files\") class PythonNumpyPCDReader(VTKPythonAlgorithmBase): \"\"\"A reader that reads a numpy file. If the numpy has a \"time\" column, then the data is treated as a temporal dataset\"\"\" def init (self): VTKPythonAlgorithmBase. init (self, nInputPorts=0, nOutputPorts=1, outputType='vtkPolyData') self._filename = None self._ndata = None self._timesteps = None print(\"starting\",uuid.uuid1()) from vtkmodules.vtkCommonCore import vtkDataArraySelection self._arrayselection = vtkDataArraySelection() self._arrayselection.AddObserver(\"ModifiedEvent\", createModifiedCallback(self)) def _get_raw_data(self, requested_time=None): import numpy if self._ndata is not None: if requested_time is not None: ##### load specific npy file from fnmes fname = self.fnames[int(requested_time)] self._ndata = numpy.load(fname) print(self._ndata.dtype) # self._ndata.dtype = numpy.dtype([(\"x\",numpy.float32),(\"y\",numpy.float32),(\"z\",numpy.float32),(\"intensity\",numpy.float32)]) return self._ndata return self._ndata if self._filename is None: # Note, exceptions are totally fine! raise RuntimeError(\"No filename specified\") # self._ndata = numpy.genfromtxt(self._filename, dtype=None, names=True, delimiter=',', autostrip=True) self.pth = Path(self._filename) self.fnames = list(self.pth.parent.rglob(\"*npy\")) self.fnames.sort() times = [i for i,e in enumerate(self.fnames)] self._ndata = 0 self._timesteps = times return self._get_raw_data(requested_time) def _get_timesteps(self): self._get_raw_data() return self._timesteps if self._timesteps is not None else None def _get_update_time(self, outInfo): executive = self.GetExecutive() timesteps = self._get_timesteps() if timesteps is None or len(timesteps) == 0: return None elif outInfo.Has(executive.UPDATE_TIME_STEP()) and len(timesteps) > 0: utime = outInfo.Get(executive.UPDATE_TIME_STEP()) print(\"using inner method get update time\",utime) dtime = timesteps[0] for atime in timesteps: if atime > utime: return dtime else: dtime = atime return dtime else: assert(len(timesteps) > 0) return timesteps[0] def _get_array_selection(self): return self._arrayselection @smproperty.stringvector(name=\"FileName\") @smdomain.filelist() @smhint.filechooser(extensions=\"npy\", file_description=\"numpy pcd file\") def SetFileName(self, name): \"\"\"Specify filename for the file to read.\"\"\" print(name) if self._filename != name: self._filename = name self._ndata = None self._timesteps = None self.Modified() @smproperty.doublevector(name=\"TimestepValues\", information_only=\"1\", si_class=\"vtkSITimeStepsProperty\") def GetTimestepValues(self): print(\"getting time steps\") return self._get_timesteps() # Array selection API is typical with readers in VTK # This is intended to allow ability for users to choose which arrays to # load. To expose that in ParaView, simply use the # smproperty.dataarrayselection(). # This method **must** return a `vtkDataArraySelection` instance. @smproperty.dataarrayselection(name=\"Arrays\") def GetDataArraySelection(self): return self._get_array_selection() def RequestInformation(self, request, inInfoVec, outInfoVec): print(\"requesting information\") executive = self.GetExecutive() outInfo = outInfoVec.GetInformationObject(0) outInfo.Remove(executive.TIME_STEPS()) outInfo.Remove(executive.TIME_RANGE()) timesteps = self._get_timesteps() if timesteps is not None: for t in timesteps: outInfo.Append(executive.TIME_STEPS(), t) outInfo.Append(executive.TIME_RANGE(), timesteps[0]) outInfo.Append(executive.TIME_RANGE(), timesteps[-1]) return 1 def RequestData(self, request, inInfoVec, outInfoVec): print(\"requesting data\") from vtkmodules.vtkCommonDataModel import vtkPolyData from vtkmodules.numpy_interface import dataset_adapter as dsa import vtk data_time = self._get_update_time(outInfoVec.GetInformationObject(0)) output = dsa.WrapDataObject(vtkPolyData.GetData(outInfoVec, 0)) points = self._get_raw_data(data_time) #points = self._ndata vpoints = vtk.vtkPoints() vpoints.SetNumberOfPoints(points.shape[0]) intensity = vtk.vtkFloatArray() intensity.SetNumberOfComponents(1) intensity.SetName(\"Intensity\") intensity.SetNumberOfTuples(points.shape[0]) for i in range(points.shape[0]): vpoints.SetPoint(i, points[i][:3]) intensity.SetTuple1(i, points[i][3]) output.GetPointData().SetScalars(intensity) output.SetPoints(vpoints) vcells = vtk.vtkCellArray() for i in range(points.shape[0]): vcells.InsertNextCell(1) vcells.InsertCellPoint(i) output.SetVerts(vcells) if data_time is not None: output.GetInformation().Set(output.DATA_TIME_STEP(), data_time) return 1 ``` Breaking this down we will begin with a helper function from the top ``` \"\"\"This module demonstrates various ways of adding VTKPythonAlgorithmBase subclasses as filters, sources, readers, and writers in ParaView\"\"\" This is module to import. It provides VTKPythonAlgorithmBase, the base class for all python-based vtkAlgorithm subclasses in VTK and decorators used to 'register' the algorithm with ParaView along with information about UI. from paraview.util.vtkAlgorithm import * import uuid from pathlib import Path ------------------------------------------------------------------------------ A reader example. ------------------------------------------------------------------------------ def createModifiedCallback(anobject): import weakref weakref_obj = weakref.ref(anobject) anobject = None def _markmodified( args, *kwars): o = weakref_obj() if o is not None: o.Modified() return _markmodified ``` Starting off with a bit of a still unclear boilerplate helper function. Best guess is this function is used later in the class to make sure that paraview tracks that our object (numpy point cloud) has been modified, and the downstream rendering updates need to happen. This may be because paraview has a lazy pipeline evaluation implementation, where filters in the processing pipeline only evaluate when changes have occurred. Now lets inspect the beginning of the actual class defining the numpy array reader plugin. So this class can be named whatever we like, but it must inherit from the python algorithm base. In my case I've filled in some of the values in the header so that the reader works with \"npy\" extension files. As with other inherited classes in python it makes sense to initalize the parent class within the subclasses `init` method. The lines creating member variables are used later on within the class methods, but aren't intended to be accessed by the user so they start with underscores. The print statement is also just a sanity check that we have started the initialization of the plugin prior reading in an actual file. It's nice for development's sake to have a message that looks slightly different if changes to the source are made and the plugin is reloaded. ``` To add a reader, we can use the following decorators @smproxy.source(name=\"PythonnumpyReader\", label=\"Python-based numpy Reader\") @smhint.xml(\"\"\" \"\"\") or directly use the \"@reader\" decorator. @smproxy.reader(name=\"Sama Lidar Numpy Reader\", label=\"Python-based numpy pcd Reader for timeseries data\", extensions=\"npy\", file_description=\"numpy files\") class PythonNumpyPCDReader(VTKPythonAlgorithmBase): \"\"\"A reader that reads a numpy file. If the numpy has a \"time\" column, then the data is treated as a temporal dataset\"\"\" def init (self): VTKPythonAlgorithmBase. init (self, nInputPorts=0, nOutputPorts=1, outputType='vtkPolyData') self._filename = None self._ndata = None self._timesteps = None print(\"starting\",uuid.uuid1()) from vtkmodules.vtkCommonCore import vtkDataArraySelection self._arrayselection = vtkDataArraySelection() self._arrayselection.AddObserver(\"ModifiedEvent\", createModifiedCallback(self)) ``` This method is used by a different non private method, but is intended to respond to paraview's requests to get data. These requests are usually the result of something in the pipeline before a filter changing, or in our case having a new timestamp selected from the animation window. We are importing numpy here because we will be loading filenames and in some cases changing the data type of the array. If the point cloud (taken from a lidar) was not just point positions but also intensity it becomes easier to access the intensity as a attribute for rendering if we make the array a named datatype array. The top of the method handles cases when we have already read the initial time point and are querying for new data. If we already have data in the member variable _ndata, and the requested_time was provided when the method was called then we use that information to look up another file from a list of filenames (self.fnames). NOTE, this code is written for a case when the timeline is using positive integer time steps. This means that we can use the requested_time as an index into the filenames list to load the requested timesteps point cloud. If you have floating point timeline values a different approach will be required. Later in the method is the code that is used when we are getting the raw data for the first time. We take the user selected file from the filebrowser and work out the path to the parent folder. Then we get a list of the contents of this folder that match the extension that we care about in this file reader. We can then fill in the member variables for _ndata and _timesteps. At this point we now can re-issue the call to this method and it will take a different path through loading data from the numpy array out of a file. This allows us to still make use of the same code that all other invocations will use even though this time the method wasn't called in response to a change in the timeline. ``` def _get_raw_data(self, requested_time=None): import numpy if self._ndata is not None: if requested_time is not None: ##### load specific npy file from fnmes fname = self.fnames[int(requested_time)] self._ndata = numpy.load(fname) print(self._ndata.dtype) # self._ndata.dtype = numpy.dtype([(\"x\",numpy.float32),(\"y\",numpy.float32),(\"z\",numpy.float32),(\"intensity\",numpy.float32)]) return self._ndata return self._ndata if self._filename is None: # Note, exceptions are totally fine! raise RuntimeError(\"No filename specified\") # self._ndata = numpy.genfromtxt(self._filename, dtype=None, names=True, delimiter=',', autostrip=True) self.pth = Path(self._filename) self.fnames = list(self.pth.parent.rglob(\"*npy\")) self.fnames.sort() times = [i for i,e in enumerate(self.fnames)] self._ndata = 0 self._timesteps = times return self._get_raw_data(requested_time) ``` After the last method you get a break since this one is much shorter. When we looking to find out how many timesteps there are in the animation we need to know how many files are in the folder we are processing. The logic there is that we want a time step per file. In support of this we issue a call to the previous method _get_raw_data because that's what creates the member variable _timesteps . ``` def _get_timesteps(self): self._get_raw_data() return self._timesteps if self._timesteps is not None else None ``` This method for the most part is just recommended paraview python code straight from their section about python programmable filters. https://docs.paraview.org/en/latest/ReferenceManual/pythonProgrammableFilter.html#reading-a-csv-file-series . Essentially it just returns back the updated time making sure its less than the values within the timesteps provided, otherwise it gives back the initial timestep. The _get_array_selection(self) method is included here too just because it's a one liner. I think this code might be dead as it's not actually getting set to anything in my case as far as I know. The _arrayselection is set in the init, but doesn't appear explicitly anywhere else. ``` def _get_update_time(self, outInfo): executive = self.GetExecutive() timesteps = self._get_timesteps() if timesteps is None or len(timesteps) == 0: return None elif outInfo.Has(executive.UPDATE_TIME_STEP()) and len(timesteps) > 0: utime = outInfo.Get(executive.UPDATE_TIME_STEP()) print(\"using inner method get update time\",utime) dtime = timesteps[0] for atime in timesteps: if atime > utime: return dtime else: dtime = atime return dtime else: assert(len(timesteps) > 0) return timesteps[0] def _get_array_selection(self): return self._arrayselection # Array selection API is typical with readers in VTK # This is intended to allow ability for users to choose which arrays to # load. To expose that in ParaView, simply use the # smproperty.dataarrayselection(). # This method **must** return a `vtkDataArraySelection` instance. @smproperty.dataarrayselection(name=\"Arrays\") def GetDataArraySelection(self): return self._get_array_selection() ``` This is the next most important part where we are actually setting a filename using a widget. This method will receive values back after the user selects a file in the filebrowser. To trigger all this they must load the plugin and then use the \"open\" button. Once they select a file who's extension matches with this, the custom plugin reader will show up with any others that match the extension. Last I checked there weren't any others that were stepping forward to handle numpy array input. This method just checks whether the same file has been loaded twice, and then sets itself to modified to trigger the subsequent loading and rendering steps. ``` @smproperty.stringvector(name=\"FileName\") @smdomain.filelist() @smhint.filechooser(extensions=\"npy\", file_description=\"numpy pcd file\") def SetFileName(self, name): \"\"\"Specify filename for the file to read.\"\"\" print(name) if self._filename != name: self._filename = name self._ndata = None self._timesteps = None self.Modified() ``` This is the public wrapper of the _get_timesteps() method which will be called after using the reader on a file that matches the extension supported. This method needs to be implemented for the algorithm class we are inheriting from to work correctly. ``` @smproperty.doublevector(name=\"TimestepValues\", information_only=\"1\", si_class=\"vtkSITimeStepsProperty\") def GetTimestepValues(self): print(\"getting time steps\") return self._get_timesteps() ``` This method is another of the ones that must be extended in the child class for the parent to work. This specifically is a method that responds to the changes in the timeline. For more information on this request for information please read lower down in this section https://docs.paraview.org/en/latest/ReferenceManual/pythonProgrammableFilter.html#understanding-the-programmable-modules . It will talk about how the RequestInformation is actually part of a \"pass in the pipeline's execution.\" Essentially this helps paraview know that the reader plugin can provide more data at different timesteps. This method will also ensure that the animation timeline gets set to the right length as far as I know. ``` def RequestInformation(self, request, inInfoVec, outInfoVec): print(\"requesting information\") executive = self.GetExecutive() outInfo = outInfoVec.GetInformationObject(0) outInfo.Remove(executive.TIME_STEPS()) outInfo.Remove(executive.TIME_RANGE()) timesteps = self._get_timesteps() if timesteps is not None: for t in timesteps: outInfo.Append(executive.TIME_STEPS(), t) outInfo.Append(executive.TIME_RANGE(), timesteps[0]) outInfo.Append(executive.TIME_RANGE(), timesteps[-1]) return 1 ``` You've almost made it! The last method does almost all the heavy lifting. This again is covered in the section on programmable sources, and gives paraview back something to show at the end of the execution pass. We load in several packages that are required to convert to vtk datatypes from numpy. Then we get the time that we are supposed to load as a point cloud. We must have a variable representing the output on which to set our results so that paraview can show them. Then we get the actual numpy array representing the points in the point cloud for a particular data_time Then we construct a vpoints variable which is an empty vtk point set with an expected number of points matching the number in our numpy data. If our data set has an intensity attribute then we can create a separate float array that will be set as a scalar point data on the output. After that we need to set up the correct cell to be shown in paraview. As I understand it, this part is standard vtk python coding. Once the output has it's cells set we are done. ``` def RequestData(self, request, inInfoVec, outInfoVec): print(\"requesting data\") from vtkmodules.vtkCommonDataModel import vtkPolyData from vtkmodules.numpy_interface import dataset_adapter as dsa import vtk data_time = self._get_update_time(outInfoVec.GetInformationObject(0)) output = dsa.WrapDataObject(vtkPolyData.GetData(outInfoVec, 0)) points = self._get_raw_data(data_time) #points = self._ndata vpoints = vtk.vtkPoints() vpoints.SetNumberOfPoints(points.shape[0]) intensity = vtk.vtkFloatArray() intensity.SetNumberOfComponents(1) intensity.SetName(\"Intensity\") intensity.SetNumberOfTuples(points.shape[0]) for i in range(points.shape[0]): vpoints.SetPoint(i, points[i][:3]) intensity.SetTuple1(i, points[i][3]) output.GetPointData().SetScalars(intensity) output.SetPoints(vpoints) vcells = vtk.vtkCellArray() for i in range(points.shape[0]): vcells.InsertNextCell(1) vcells.InsertCellPoint(i) output.SetVerts(vcells) if data_time is not None: output.GetInformation().Set(output.DATA_TIME_STEP(), data_time) return 1 ``` {\"serverDuration\": 19, \"requestCorrelationId\": \"a3e9db9e220446d8b97041e2bc9c2d8c\"} Was this page informative? Thanks for your feedback! Thanks for your feedback! Have suggestions for improvements? Contact us . Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/Paraview/Demystifying_Paraview_Python_Plugins_or_at_least_a_little_less_mystery_",
      "title": "Paraview Python Plugins Demystified - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "ca7f665d-5be4-4335-afac-e1c29c1234ad",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.34",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/Paraview/Demystifying_Paraview_Python_Plugins_or_at_least_a_little_less_mystery_",
      "statusCode": 200
    }
  },
  {
    "processed_text": "CGAl Vespa If you need advanced Computational Geometry support for your visualization and analysis, then we have you covered! Follow these steps to get up and running with an OOD remote desktop and a custom compiled paraview container providing cgal vespa. ``` pull the container apptainer pull docker://ghcr.io/devinbayly/cgal_vespa:latest ``` When this is complete you must run the container and start paraview ``` apptainer exec cgal_vespa_latest.sif /opt/paraview_build/bin/paraview ``` When paraview starts you need to import the Vespa paraview plugin which will be at this system path ``` /usr/local/lib/paraview-5.11/plugins/VESPAPlugin/VESPAPlugin.so ``` Then the following filters can help for estimating volumes contained within areas of interest in imaging data threshold extract surface tetrahedralize Alpha Wrap (CGAL Vespa) Connectivity Compute Connected Surface Properties Refer to the screen recording shared here for other details, as well as the demo_state.pvsm which you can just load in the custom paraview container to get started. {\"serverDuration\": 17, \"requestCorrelationId\": \"6bc9cdeba83c4a7bb82f184a96d6b19b\"} Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/cgal_vespa/",
      "title": "CGAl Vespa - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "cd119f12-c008-4d02-a593-4188ebaa6a68",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/cgal_vespa/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Graphs and Exporting Data While visualizations in the terminal are ParaView's strength, it also includes tools to plot data for different types of visualizations. Creating Graphs In Paraview For this, I’m going to use the same example as the introductory tutorial . The first thing you want to do is select the source you want to work with. Next, from the toolbar select Filters → Data Analysis then select whichever plot works best for you. For this example, I’m going to select Plot Data Over Time. For some options, a warning will pop up stating that it could take a long time to plot the data. This is certainly the case for large data sets but if you're using the example, it should take only a second. Click apply in the properties tab and an indicator in the bottom right should appear as your data gets processed. Once your data is fully processed, a graph should appear, splitting the viewport in two. Keep in mind that what is visible is view specific so selecting the 3D viewport will cause an eye to appear next to the base source in the pipeline (indicating that it is what's being shown in this view), and likewise selecting the graph view will remove the eye on the source and put it on the new graph filter. To demonstrate, compare the two images above. In the first, I clicked on the viewport giving it a blue border to indicate it's selected and also placing the eye icon on can.ex2 in the pipeline (because it's what's being shown in the viewport). Then in the second image, once I click on the graph, the blue border changes and the eye icon moves to PlotDataOverTime1. From this point, you can use the properties menu to modify the values of the graph as you see fit, making sure to have the graph selected in the Pipeline Browser. After changes are made, many will require you to press Apply at the top of the properties menu for them to take effect. Exporting Data To A Spreadsheet While ParaView does have many data management tools and graphs, it's certainly not as many as other data analysis software. To use the data in other programs, you'll need to export it to a format of your choice, probably the most simple of which is a CSV (a standard format for spreadsheets). Select your source whose data you'd like to export, then, from the toolbar, go to Extractors → Data → CSV. Make the desired changes in the properties tab, press Apply, then go to File → Save Extracts. At this point select where you would like to save the data and it should generate CSVs in the specified folder. Common Issues If your data has outliers or null values, many plotting functions have difficulties correctly reading the data. If this is the case, apply a Threshold filter (Filters → Common → Threshold), scale the data to whatever range you want, then apply the graph to the new threshold filter. Note that you'll have to remove the old graph, add the threshold, then re-add the graph to the threshold. For example (before and after adding Threshold): {\"serverDuration\": 17, \"requestCorrelationId\": \"67ff7a936cfe4a31b5acaa7c171a21e5\"} Was this page informative? Thanks for your feedback! Thanks for your feedback! Have suggestions for improvements? Contact us . Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/Paraview/Graphs_and_Exporting_Data",
      "title": "Graphs and Exporting Data - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "d4ddb357-8839-4781-bb17-ef72c0cf0516",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.5.34",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/Paraview/Graphs_and_Exporting_Data",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content VisIt ¶ Goal Using OOD Client/Server Configuration Pre-requisite Installations Steps on the HPC Setting up ssh keys for passwordless authentication to the HPC. Installing the VisIt server Copying the launcher script into server folder Steps for Linux Connecting to the HPC VPN. Installing visit local machine Installing the HPC host profiles running visit Filling in host profile Support related to this document Goal ¶ By the end of this document you will know how to start up VisIt client server work sessions with Open On\\ Demand or between your local machine and the compute nodes of the HPC in client server configuration. Using OOD ¶ This method makes use of a remote desktop that the user can start up on the HPC. This benefits from putting the user in a familiar graphical environment so things like starting visit are the same here as on their local workstation. Open a browser and go to ood.hpc.arizona.edu When prompted fill out the authentication Now select Interactive Desktop from the Interactive Apps Here you can see the values that were set to run a 1 hour visit session on Ocelote with a gpu. It should be noted that this workflow can also be performed with cpu only allocations too. You will need to fill out your PI where the highlighter is over visteam . Once you have hit Launch and your allocation is granted you will see this on the page. Now you will click on the Launch Interactive Desktop button. A new tab will be created that shows a remote desktop in the window. Here you will right click on the background and select Open In Terminal . Then you will need to change directory to wherever Visit 3.2.1 was installed. If you haven't done this then follow this script to install it in the directory you are in Download the redhat EL7 w/ Mesa option by running these commands replacing the <PIname> with your PI's name ``` mkdir visit_setup cd visit_setup wget \"https://github.com/visit-dav/visit/releases/download/v3.2.1/visit3_2_1.linux-x86_64-rhel7-wmesa.tar.gz\" tar xf visit3_2_1.linux-x86_64-rhel7-wmesa.tar.gz cd visit3_2_1.linux-x86_64 ``` Once you are located in the visit3_2_1.linux-x86_64 folder navigate to the bin folder and run ./visit -debug 5 You will now see Visit starting in the remote desktop. Use the Open button to bring up a data set you want to visualize and export a movie from. Here the aneurysm dataset is selected, if you'd like to use it you may download it here https://visit-dav.github.io/largedata/datarchives/aneurysm . Then choose to add a simple plot. For this data one of the interesting parts is the velocity field and how it changes over time. Once you have selected the velocity field make sure to draw it to the adjacent Visit window. Now that we have the first frame drawn in the viewing window, let's render all the frames of the simulation out as a movie. Select File to show the drop down menu of options. From the options select Save Movie which will open a wizard window to step you through the export configuration. Select next with the default option to create a New simple movie In the next window select the format drop down and select one of the options. A benefit to exporting individual frames as still images is it provides greater flexibility over quality options for the conversion to video which is outlined later. Here PNG will be used Click the -> arrow to queue this format to the export options. It is possible to make several outputs from a single export pass by clicking the -> arrow multiple times for a range of formats/options. Clicking next shows a window with parameters about the video to be made. These options are default set to create a frame for each time step in the video, but if a particular range is of interest you may set the First Frame and Last Frame to customize. Finally we will specify where the outputs are supposed to go. Use the drop down button at the far right to open a file browser window to aid you in finding the optimal destination if it isn't the default one shown. You may change the Base Name to specify an alternate prefix for each rendered frame which is followed by a frame number padded with 4 0's. Since we have already configured a visit instance with out data and plot we will use the currently allocated processors . More advanced workflows may require the other options but they are not documented here as of yet. Once you select finish a new window opens which shows the terminal output of the process, as well as a progress bar window that helps you see how the export is unfolding. Once this finishes you may return to the terminal, and navigate to the folder where the individual frames were output. Use the ls command to make sure you are in the correct location. You must also load in a few modules in order to process the images into a video from the command line. ``` module load gnu8 module load ffmpeg ``` This is the final step. You will run ffmpeg to concatenate all the images together into a video whose format you may specify. To let ffmpeg know we are using all the images that match a pattern that starts with the word movie, then has 4 digits, then file type mp4 we use this string movie%04d.png . The -r specifies the rate in fps. The -f notifies ffmpeg that we are using the image2 filter to convert an image->something else. ``` ffmpeg -f image2 -i movie%04d.png -r 10 output.mp4 ``` All done! Client/Server Configuration ¶ This setup differs from OOD in that there is no remote rendering of the visit window contents, and allows for rapid response from interaction with the menus because the client runs natively on the local machine. The trade off is that certain capabilities are c`c Pre-requisite Installations ¶ VPN system like Cisco Anyconnect Steps on the HPC ¶ Setting up ssh keys for passwordless authentication to the HPC. ¶ The documentation for this step exists on confluence at this link System\\ Access#SSHKeys but you must complete these steps for shell.hpc.arizona.edu not the bastion hpc.arizona.edu host. Note that performing a ssh <netid>@shell.hpc.arizona.edu and replacing the <netid> with your netid is only possible when we are on the hpc vpn. Installing the VisIt server ¶ Download the redhat EL7 w/ Mesa option by running these commands replacing the <PIname> with your PI's name ``` cd /groups/ mkdir visit_setup cd visit_setup wget \"https://github.com/visit-dav/visit/releases/download/v3.1.4/visit3_1_4.linux-x86_64-rhel7-wmesa.tar.gz\" tar xf visit3_1_4.linux-x86_64-rhel7-wmesa.tar.gz cd visit3_1_4.linux-x86_64 ``` Copying the launcher script into server folder ¶ The contents of the visit installation are laid out as follows. Navigate to the bin folder within 3.1.4 and copy in the customlauncher code ``` cd 3.1.4/bin wget \"https://gist.githubusercontent.com/DevinBayly/310c8689c6221fd379aad34243441dda/raw/a8f5071bb8b1e96127e1ea01b2e8667940849f1a/customlauncher\" -O customlauncher ``` If you'd like to inspect the code a copy is available here as well. Customlauncher Steps for Linux ¶ Connecting to the HPC VPN ¶ Connect to the HPC VPN, preferably with Cisco AnyConnect. For detailed information on connecting to the HPC VPN, see VPN - Virtual Private Network . Note that The HPC VPN is needed to connect directly to a compute node. This differs from the standard UArizona VPN or campus network which are not sufficient. The HPC VPN is vpn.hpc.arizona.edu . Installing visit ¶ local machine ¶ https://visit-dav.github.io/visit-website/ https://visit-dav.github.io/visit-website/releases-as-tables/#series-31 Installing the HPC host profiles ¶ download the host_uahpc2.xml file and save it to the visit directory in your home folder. Here's what that looks like in my case: host_uahpc2.xml Once downloaded, transfer it to `~/.visit/hosts` running visit ¶ Navigate to the visit3.1.4 folder who's contents should appear like this Enter the `bin` folder and start visit in debug mode ``` cd bin ./visit -debug 5 ``` This mode will be helpful for tracking down any errors that come up when configuring the client server session. Filling in host profile ¶ There are the values that need to be changed in the host profile to support your user on the HPC. Open the host profiles from the options tab and then select UAHPC2 On the Host Settings page we will update the following entries that are highlighted. The first one is the path to the visit installation on the HPC. Here you must get the absolute path to the VisIt installation on the HPC which should be /groups/<PIname>/visit_setup/visit3_1_4.linux-x86_64 replacing <PIname> with your PI's name. If you installed VisIt somewhere else you will use a different absolute path to that location. Second you must replace baylyd with your own netid Then we will change the options in the Launch Profiles by selecting that tab in the Host Profiles window. Then select the parallel settings tab ensuring to replace visteam with your own PI Once that's complete you may attempt to open a session Select the UAHPC2 host Then navigate to a data set you wish to visualize in the client server session In my case I'll be using the sample aneurysm data which can be downloaded here https://visit-dav.github.io/largedata/datarchives/aneurysm You will then have the option to select a compute launcher configuration. I'm going with elgato . Again check to make sure that the Bank field has your PI's netid in it. After selecting OK you will see a progress bar: This is displayed while the slurm batch allocation that was automatically generated for you waits in a queue. When it is approved and launched the progress window goes away. You can double check that the VisIt session is running successfully on the HPC by logging into hpc.arizona.edu and running squeue -u <netid> . You will see an entry like this who's name is visit.<netid> At this point you can request for VisIt to perform operations and graph different aspects of your data. If your data is timeseries you may also hit the play button and it will step through your data visualizing each step. Support related to this document ¶ If you are interested in this workflow but need support you can send an email to vislab-consult@list.arizona.edu for technical support a Data & Visualization consultant. {\"serverDuration\": 18, \"requestCorrelationId\": \"79d8fd98c6fa410395673f3939c1f8ca\"} Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/visit/",
      "title": "VisIt - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "dd3cf8d7-57e3-4569-a496-741d06c0269f",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/visit/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Open OnDemand ¶ Open OnDemand (OOD), which is an NSF-funded open-source HPC portal, is available for users and provides web browser access for graphically interfacing with HPC. This service is available at https://ood.hpc.arizona.edu/ . Why use Open OnDemand? ¶ Since there are many ways to access HPC, including the command line terminal, why use Open OnDemand? Here are a few of the main reasons: GUI software available Many commonly used research software packages, such as RStudio and ANSYS, have graphical interfaces that streamline analysis workflows. Open OnDemand allows for easy access to these applications without the complications of server setup or image/port forwarding. User friendly Open OnDemand includes an interactive desktop application which mimics what you might find on your local workstation. This environment may be more intuitive to navigate when getting familiarized with the HPC. Standardized batch access Open OnDemand includes forms to submit batch jobs which include all the relevant parameters in one place. List of Available Features in Open OnDemand ¶ | Basic Functions | GUI Software | Servers | Batch/Slurm | | --- | --- | --- | --- | | File Browser | Abaqus | Jupyter Notebook | Job Composer | | Interactive Desktop | ANSYS | R Studio | Job Viewer | | | Mathematica | Shell | | | | MATLAB | | | | | Stata | | | | | VS Code | | | Command Line Access ¶ Need command line access to a terminal on HPC? No problem! Simply select the Clusters dropdown menu to connect to one of HPC's login nodes. This is also detailed under System Access File Browser ¶ The file browser provides easy access to your /home , /xdisk , and /groups directories and allows you to view, edit, copy, and rename your files. You may also transfer small files (under 64 MB) between HPC and your local workstation using this interface. For larger transfers, see our section on Transferring Data for more efficient methods. Access In the browser at the top of the screen, select the Files dropdown You will be able to select your /home directory, /groups , or /xdisk . If you select /groups or /xdisk , enter your PI's NetID in the Filter field to find your shared group space. Editing Files First, navigate to the file you wish to edit. Then, click the vertical ellipses on the right-hand side and select Edit This will open a file editor in your browser where you may select your color theme, text size, and syntax highlighting. Job Viewer and Composer ¶ Job Viewer The Job Viewer allows you to check the status and time remaining of your running jobs. You can also cancel your jobs using this interface. Note: be careful looking at All Jobs since this will likely timeout trying to organize them all. To use the Job Viewer, navigate to the Jobs dropdown and select Active Jobs This will open a new page listing all your running and pending jobs. You may delete them by clicking the red trash icon under Actions, or view more information about individual jobs using the dropdown on the left next to the ID. Job Composer The Job Composer lets you create and run a Slurm script on any of our three clusters. It should be noted that the Job Composer creates a special string of directories in your /home starting with ondemand/ which is where both your submission scripts and output files will be stored. Make note of the path to your files on the right-hand side of the Job Composer screen under Script location . Interactive Graphical Applications ¶ Open OnDemand provides access to graphical interfaces for some popular software. These can be found under Interactive Apps through the Open OnDemand web browser. The process of starting and accessing these jobs is the same regardless of which application you select. July 31, 2024 changes Beginning on July 31, 2024, OnDemand graphical applications will be limited to four days of runtime to improve general resource availability. Jobs that take longer than four days to run may be converted to batch jobs . If you have questions about using batch jobs, reach out to our consultants . Web Form First, select the desired application from Interactive Apps. This will take you to a form where you will enter your job information. This includes the entries in the following table: | Field | Description | Example | | --- | --- | --- | | Cluster | Select which cluster to submit the job request to. | Puma | | Run Time | The maximum number of hours the job can run. Please note that the maximum possible run time is 10 days (240 hours). | 4 | | Core Count on a single node | The number of CPUs needed. This affects the amount of memory your job is allocated. The maximum that can be requested is dependent on which cluster you choose . | 16 | | Memory per core | The amount of memory needed per core in GB. The amount that can be requested is dependent on which cluster you choose and your desired node type. For more information, see our CPUs and Memory page . | 5 | | GPUs required | The number and type of GPUs needed for your job, if any. | One A100 20GB GPU | | PI Group | Your accounting group. If you do not know your group name, you can either check in the user portal, or can run va on the command line. If the group you entered does not exist, you will receive an error sg: group 'group_name' does not exist | your-group | | Queue | The queue, or partition, to use. Standard is the most common. If your group has buy-in hours, you may use High Priority. | Standard | Once you've entered all your details, click Launch at the bottom of the page. This will take you to a tile with information about your job including job ID and session ID. This information can used for debugging purposes. When you first submit your job, it will show as having a status of \"Queued\". Once your job reaches the front of the queue, it will show a status of \"Starting\". When your session is ready, you can launch the application using Connect at the bottom of the tile. Applications Available ¶ Virtual Desktop Jupyter Notebooks RStudio Matlab Ansys Abaqus Anaconda compatibility issues If you have Anaconda initialized in your account, ensure you have turned off Conda's auto-activation feature . If auto-activation is enabled, your desktop jobs will likely fail with the error Could not connect to session bus One nice feature of Open OnDemand is the ability to interact with HPC using a virtual Desktop environment. This provides a user-friendly way to run applications, perform file management, and navigate through your directories as though you were working with a local computer. Additionally, it eliminates the need to use X11 forwarding when working with GUI applications allowing an easy way to interact with software such as Matlab, VisIt, or Anaconda. Tip To access your own python packages in Jupyter, you can create custom kernels either using a python module or using anaconda . The Jupyter Notebook is an open-source web application that allows you to create and share documents that contain live code, equations, visualizations, and narrative text. When you start a Jupyter notebook, by default your working directory will be your home. If you would like to change this so that your session starts in a different location, you'll need to add a line to the hidden file ~/.bashrc . To do this, open your ~/.bashrc in a text editor and add the following, substituting your desired path in for </path/to/directory> : ``` export NOTEBOOK_ROOT= ``` RStudio is an integrated development environment (IDE) for R. It includes a console, syntax-highlighting editor that supports direct code execution, as well as tools for plotting, history, debugging and workspace management. For an overview of the RStudio IDE, see: https://www.rstudio.com/products/RStudio/ . For information on using R on HPC, see our online documentation on Using R Packages . A GUI for multiple versions of Matlab is available. You can select which version to use in the web form when specifying your resources. Multiple versions of the engineering application Ansys are available. You can specify which version to use in the web form when specifying your resources. To receive Ansys-specific support, see: Community and External Resources A GUI for Abaqus is available. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/open_on_demand/",
      "title": "Open OnDemand/Graphical Jobs - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "df3de7c2-72e9-42b3-9060-4654feda1cd0",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/open_on_demand/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "{\"payload\":{\"allShortcutsEnabled\":false,\"fileTree\":{\"General-Examples/Job-Dependencies\":{\"items\":[{\"name\":\"README.md\",\"path\":\"General-Examples/Job-Dependencies/README.md\",\"contentType\":\"file\"},{\"name\":\"volcano.gif\",\"path\":\"General-Examples/Job-Dependencies/volcano.gif\",\"contentType\":\"file\"},{\"name\":\"volcano.tar.gz\",\"path\":\"General-Examples/Job-Dependencies/volcano.tar.gz\",\"contentType\":\"file\"}],\"totalCount\":3},\"General-Examples\":{\"items\":[{\"name\":\"Cleanup-Tmp-Files\",\"path\":\"General-Examples/Cleanup-Tmp-Files\",\"contentType\":\"directory\"},{\"name\":\"Disable-Core-Dumps\",\"path\":\"General-Examples/Disable-Core-Dumps\",\"contentType\":\"directory\"},{\"name\":\"High-Memory-Node\",\"path\":\"General-Examples/High-Memory-Node\",\"contentType\":\"directory\"},{\"name\":\"Job-Dependencies\",\"path\":\"General-Examples/Job-Dependencies\",\"contentType\":\"directory\"},{\"name\":\"Pipefail\",\"path\":\"General-Examples/Pipefail\",\"contentType\":\"directory\"},{\"name\":\"Serial-Job-Example\",\"path\":\"General-Examples/Serial-Job-Example\",\"contentType\":\"directory\"},{\"name\":\"README.md\",\"path\":\"General-Examples/README.md\",\"contentType\":\"file\"}],\"totalCount\":7},\"\":{\"items\":[{\"name\":\"Apptainer-Examples\",\"path\":\"Apptainer-Examples\",\"contentType\":\"directory\"},{\"name\":\"Array-and-Parallel\",\"path\":\"Array-and-Parallel\",\"contentType\":\"directory\"},{\"name\":\"GPU-Examples\",\"path\":\"GPU-Examples\",\"contentType\":\"directory\"},{\"name\":\"General-Examples\",\"path\":\"General-Examples\",\"contentType\":\"directory\"},{\"name\":\"Images\",\"path\":\"Images\",\"contentType\":\"directory\"},{\"name\":\"Intro-to-Containers\",\"path\":\"Intro-to-Containers\",\"contentType\":\"directory\"},{\"name\":\"Intro-to-HPC\",\"path\":\"Intro-to-HPC\",\"contentType\":\"directory\"},{\"name\":\"Intro-to-Machine-Learning-R\",\"path\":\"Intro-to-Machine-Learning-R\",\"contentType\":\"directory\"},{\"name\":\"Intro-to-Machine-Learning\",\"path\":\"Intro-to-Machine-Learning\",\"contentType\":\"directory\"},{\"name\":\"Life-Sciences\",\"path\":\"Life-Sciences\",\"contentType\":\"directory\"},{\"name\":\"MPI-Examples\",\"path\":\"MPI-Examples\",\"contentType\":\"directory\"},{\"name\":\"Machine-Learning-Examples\",\"path\":\"Machine-Learning-Examples\",\"contentType\":\"directory\"},{\"name\":\"Matlab-Examples\",\"path\":\"Matlab-Examples\",\"contentType\":\"directory\"},{\"name\":\"Python-Examples\",\"path\":\"Python-Examples\",\"contentType\":\"directory\"},{\"name\":\"R-Examples\",\"path\":\"R-Examples\",\"contentType\":\"directory\"},{\"name\":\"Singularity-Examples\",\"path\":\"Singularity-Examples\",\"contentType\":\"directory\"},{\"name\":\"README.md\",\"path\":\"README.md\",\"contentType\":\"file\"},{\"name\":\"_config.yml\",\"path\":\"_config.yml\",\"contentType\":\"file\"}],\"totalCount\":18}},\"fileTreeProcessingTime\":9.879306,\"foldersToFetch\":[],\"reducedMotionEnabled\":null,\"repo\":{\"id\":299408220,\"defaultBranch\":\"master\",\"name\":\"ua-researchcomputing-hpc.github.io\",\"ownerLogin\":\"UA-ResearchComputing-HPC\",\"currentUserCanPush\":false,\"isFork\":false,\"isEmpty\":false,\"createdAt\":\"2020-09-28T19:14:43.000Z\",\"ownerAvatar\":\"https://avatars.githubusercontent.com/u/71899008?v=4\",\"public\":true,\"private\":false,\"isOrgOwned\":true},\"symbolsExpanded\":false,\"treeExpanded\":true,\"refInfo\":{\"name\":\"master\",\"listCacheKey\":\"v0:1694572173.0\",\"canEdit\":false,\"refType\":\"branch\",\"currentOid\":\"4e00844b1d0ad90fa5d6105308d41494d1601e5c\"},\"path\":\"General-Examples/Job-Dependencies/volcano.tar.gz\",\"currentUser\":null,\"blob\":{\"rawLines\":null,\"stylingDirectives\":null,\"csv\":null,\"csvError\":null,\"dependabotInfo\":{\"showConfigurationBanner\":false,\"configFilePath\":null,\"networkDependabotPath\":\"/UA-ResearchComputing-HPC/ua-researchcomputing-hpc.github.io/network/updates\",\"dismissConfigurationNoticePath\":\"/settings/dismiss-notice/dependabot_configuration_notice\",\"configurationNoticeDismissed\":null,\"repoAlertsPath\":\"/UA-ResearchComputing-HPC/ua-researchcomputing-hpc.github.io/security/dependabot\",\"repoSecurityAndAnalysisPath\":\"/UA-ResearchComputing-HPC/ua-researchcomputing-hpc.github.io/settings/security_analysis\",\"repoOwnerIsOrg\":true,\"currentUserCanAdminRepo\":false},\"displayName\":\"volcano.tar.gz\",\"displayUrl\":\"https://github.com/UA-ResearchComputing-HPC/ua-researchcomputing-hpc.github.io/blob/master/General-Examples/Job-Dependencies/volcano.tar.gz?raw=true\",\"headerInfo\":{\"blobSize\":\"5.56 KB\",\"deleteInfo\":{\"deleteTooltip\":\"You must be signed in to make or propose changes\"},\"editInfo\":{\"editTooltip\":\"You must be signed in to make or propose changes\"},\"ghDesktopPath\":\"https://desktop.github.com\",\"gitLfsPath\":null,\"onBranch\":true,\"shortPath\":\"cf0dd4a\",\"siteNavLoginPath\":\"/login?return_to=https%3A%2F%2Fgithub.com%2FUA-ResearchComputing-HPC%2Fua-researchcomputing-hpc.github.io%2Fblob%2Fmaster%2FGeneral-Examples%2FJob-Dependencies%2Fvolcano.tar.gz\",\"isCSV\":false,\"isRichtext\":false,\"toc\":null,\"lineInfo\":{\"truncatedLoc\":null,\"truncatedSloc\":null},\"mode\":\"file\"},\"image\":false,\"isCodeownersFile\":null,\"isPlain\":false,\"isValidLegacyIssueTemplate\":false,\"issueTemplateHelpUrl\":\"https://docs.github.com/articles/about-issue-and-pull-request-templates\",\"issueTemplate\":null,\"discussionTemplate\":null,\"language\":null,\"languageID\":null,\"large\":false,\"loggedIn\":false,\"newDiscussionPath\":\"/UA-ResearchComputing-HPC/ua-researchcomputing-hpc.github.io/discussions/new\",\"newIssuePath\":\"/UA-ResearchComputing-HPC/ua-researchcomputing-hpc.github.io/issues/new\",\"planSupportInfo\":{\"repoIsFork\":null,\"repoOwnedByCurrentUser\":null,\"requestFullPath\":\"/UA-ResearchComputing-HPC/ua-researchcomputing-hpc.github.io/blob/master/General-Examples/Job-Dependencies/volcano.tar.gz\",\"showFreeOrgGatedFeatureMessage\":null,\"showPlanSupportBanner\":null,\"upgradeDataAttributes\":null,\"upgradePath\":null},\"publishBannersInfo\":{\"dismissActionNoticePath\":\"/settings/dismiss-notice/publish_action_from_dockerfile\",\"dismissStackNoticePath\":\"/settings/dismiss-notice/publish_stack_from_file\",\"releasePath\":\"/UA-ResearchComputing-HPC/ua-researchcomputing-hpc.github.io/releases/new?marketplace=true\",\"showPublishActionBanner\":false,\"showPublishStackBanner\":false},\"rawBlobUrl\":\"https://github.com/UA-ResearchComputing-HPC/ua-researchcomputing-hpc.github.io/raw/master/General-Examples/Job-Dependencies/volcano.tar.gz\",\"renderImageOrRaw\":true,\"richText\":null,\"renderedFileInfo\":null,\"shortPath\":null,\"tabSize\":8,\"topBannersInfo\":{\"overridingGlobalFundingFile\":false,\"globalPreferredFundingPath\":null,\"repoOwner\":\"UA-ResearchComputing-HPC\",\"repoName\":\"ua-researchcomputing-hpc.github.io\",\"showInvalidCitationWarning\":false,\"citationHelpUrl\":\"https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/about-citation-files\",\"showDependabotConfigurationBanner\":false,\"actionsOnboardingTip\":null},\"truncated\":false,\"viewable\":false,\"workflowRedirectUrl\":null,\"symbols\":null},\"copilotInfo\":null,\"copilotAccessAllowed\":false,\"csrf_tokens\":{\"/UA-ResearchComputing-HPC/ua-researchcomputing-hpc.github.io/branches\":{\"post\":\"AC8LXBJmGuf0ixvshl1tJ8rlUeiCM6dQS9PxjCNB3ypL2ZwJyogknYJJ0NqvKptm0t5PLrtiY4X-6XmvALAiNw\"},\"/repos/preferences\":{\"post\":\"7VeQfSkmgNm7XIk5yIwxzdyRkswvA0_DthPZxV1OKZexrUMDuBVn1H_7pV5aS1Ym2hcY7vKRq9Z3k4bB23v7pw\"}}},\"title\":\"ua-researchcomputing-hpc.github.io/General-Examples/Job-Dependencies/volcano.tar.gz at master · UA-ResearchComputing-HPC/ua-researchcomputing-hpc.github.io\"}",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/batch_jobs/job_dependencies/files/volcano.tar.gz",
      "scrapeId": "ef301534-e924-40aa-b569-dfef8bbfdc29",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/batch_jobs/job_dependencies/files/volcano.tar.gz",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Jobs with GNU Parallel ¶ GNU Parallel is a powerful tool to submit multiple, independent but similar tasks efficiently within one job. In essence, it is similar to Slurm's array jobs and can be used both as an alternative to array jobs, or together with array jobs. GNU Parallel is available as a module on the HPC, and can be loaded with module load parallel in your batch script. The examples below show how you can use GNU Parallel by itself, and with array jobs. Basic GNU Parallel Job ¶ The Slurm script for a basic GNU Parallel job will look like the following: basic-parallel-job.slurm ``` !/bin/bash SBATCH --ntasks=28 SBATCH --nodes=1 SBATCH --time=00:01:00 SBATCH --partition=standard SBATCH --account=YOUR_GROUP module load parallel seq 1 100 | parallel 'DATE=$( date +\"%T\" ) && sleep 0.{} && echo \"Host: $HOSTNAME ; Date: $DATE; {}\"' ``` In this example, we're make use of a full node with GNU Parallel. The meat of the command lies here: ``` seq 1 100 | parallel 'DATE=$( date +\"%T\" ) && sleep 0.{} && echo \"Host: $HOSTNAME ; Date: $DATE; {}\"' ``` seq 1 100 generates a list between 1 and 100 (inclusive), and we pipe that into a parallel command which will generate one task per element (so 100 tasks). GNU Parallel will find the space on our node as it works through the relevant tasks. Inside the command: DATE=$( date + \"%T\" ) sets DATE so we can visualize tasks and when they're being executed. sleep 0.{} forces each task to sleep for 0.n seconds, where n is the input integer from the seq command. This means, for example, the 2nd task will wait longer than the 10th task, as can be seen in the output file. This is used to demonstrate that these tasks are being executed in parallel. echo \"HOST: $HOSTNAME ; Date: $DATE; {}\" prints out information about the task. {} is piped input which, in this case, is an integer generated by seq between 1 and 100. You can submit the script with sbatch basic-parallel-job.slurm . Unlike an array job, this will produce only one output file: ``` (ocelote) [netid@junonia ~]$ head slurm-74027.out Host: i10n18 ; Date: 16:45:55; 1 Host: i10n18 ; Date: 16:45:55; 10 Host: i10n18 ; Date: 16:45:55; 11 Host: i10n18 ; Date: 16:45:55; 12 Host: i10n18 ; Date: 16:45:55; 13 Host: i10n18 ; Date: 16:45:55; 14 Host: i10n18 ; Date: 16:45:55; 15 Host: i10n18 ; Date: 16:45:55; 16 Host: i10n18 ; Date: 16:45:55; 17 Host: i10n18 ; Date: 16:45:55; 2 ``` Array Jobs and GNU Parallel ¶ Sometimes you need to run a lot of jobs. More than can be reasonably accomplished using arrays since submitting thousands of jobs can be a problem for the system, and GNU Parallel can be challenging to make work in a multi-node environment. In this case, we can combine the forces of GNU Parallel and array jobs to distribute a chunk of tasks across multiple nodes where GNU Parallel will execute them. The Slurm script in this case will look like the following: array-and-parallel.slurm ``` !/bin/bash SBATCH --job-name=Sample_Array_With_GNU_Parallel SBATCH --ntasks=94 SBATCH --nodes=1 SBATCH --time=00:05:00 SBATCH --partition=standard SBATCH --account=YOUR_GROUP SBATCH --array=1-2 module load parallel BLOCK_SIZE=200 seq $(($SLURM_ARRAY_TASK_ID $BLOCK_SIZE-$BLOCK_SIZE+1)) $(($SLURM_ARRAY_TASK_ID $BLOCK_SIZE)) | parallel echo \"JOB ID: $SLURM_JOB_ID HOST NODE: $HOSTNAME EXAMPLE COMMAND: ./executable input_{}\" ``` The main difference with the basic example is setting up a \"block size\". This is the number of tasks GNU Parallel will be executing in each subjob ``` BLOCK_SIZE=200 ``` In this case, we're asking for 200 tasks per subjob and since we're submitting an array job, that totals 400 tasks. The array indices are used to differentiate tasks. seq n m generates a sequence of integers from n to m (inclusive). SLURM_ARRAY_TASK_ID in this case is either 1 or 2, depending on the subjob, so combined with BLOCK_SIZE : Subjob 1 : Generates numbers from 1 to 200 ``` seq $(($SLURM_ARRAY_TASK_ID $BLOCK_SIZE-$BLOCK_SIZE+1)) $(($SLURM_ARRAY_TASK_ID $BLOCK_SIZE)) Doing the math --> seq 1 200-200+1 1 200 --> seq 1 200 ``` Subjob 2 : Generates numbers from 201 to 400 ``` seq $(($SLURM_ARRAY_TASK_ID $BLOCK_SIZE-$BLOCK_SIZE+1)) $(($SLURM_ARRAY_TASK_ID $BLOCK_SIZE)) Doing the math --> seq 2 200-200+1 2 200 --> seq 201 400 ``` You can submit the script with sbatch array-and-parallel.slurm . It will produce two output files, one for each job: ``` (puma) [netid@junonia ~]$ ls *.out slurm-1693973_1.out slurm-1693973_2.out ``` ``` (puma) [netid@junonia ~]$ head *.out ==> slurm-1693973_1.out <== JOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_1 JOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_2 JOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_3 JOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_4 JOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_5 JOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_6 JOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_7 JOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_8 JOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_9 JOB ID: 1693974 HOST NODE: r2u13n2 EXAMPLE COMMAND: ./executable input_10 ==> slurm-1693973_2.out <== JOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_201 JOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_202 JOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_203 JOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_204 JOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_205 JOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_206 JOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_207 JOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_208 JOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_209 JOB ID: 1693973 HOST NODE: r2u07n1 EXAMPLE COMMAND: ./executable input_210 ``` Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/batch_jobs/gnu_parallel_jobs/",
      "title": "Jobs with GNU Parallel - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "e74759a2-e1b0-46b1-a7b2-a6dabb3929f5",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/batch_jobs/gnu_parallel_jobs/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Array Jobs ¶ What Are Job Arrays? ¶ Slurm job arrays are a powerful feature for submitting and managing multiple similar jobs efficiently using a single script. Instead of submitting individual jobs, users define an array with shared characteristics, treating them as a unified entity. Each subjob within the array is assigned a unique environment variable, enabling easy differentiation. Array jobs are submitted in the same manner as regular batch jobs: sbatch myscript.slurm . Slurm then schedules and executes these tasks based on resource availability, streamlining the process. Why Use Job Arrays? ¶ Workflow Efficiency Job arrays streamline the process of managing multiple jobs with similar configurations, reducing manual effort and potential errors in job submission. Resource Utilization By grouping similar tasks into a single job array, you can optimize resource utilization on the cluster. Slurm can efficiently allocate resources to individual tasks within the array based on availability. Scheduler Efficiency Submitting multiple individual jobs in loops can significantly slow down the scheduler for all users. Job arrays help alleviate this issue by reducing the number of job submissions, leading to improved scheduler performance and responsiveness. Scalability Job arrays are particularly useful for parallel and repetitive tasks, such as parameter sweeps, Monte Carlo simulations, or running the same code with different inputs. They provide a scalable approach to handling large numbers of tasks without the need to write and manage hundreds or thousands of related submission scripts. Simplified Management With job arrays, you only need to manage a single submission script for a group of tasks, making it easier to track, monitor, and troubleshoot your jobs. How to Use Job Arrays ¶ Array job resources The resource requirements defined in an array job script are applied to each job in the array. To utilize job arrays, a specific range of task IDs needs to be specified using the --array batch directive in your submission script. This directive is set to a range of integer values and determines the number of individual jobs submitted. It also controls the index associated with each subjob in the array. Since each task within an array can have its own parameters, input files, or commands, a unique identifier is needed to differentiate jobs. These are controlled by the Slurm environment variable $SLURM_ARRAY_TASK_ID which is set to a unique integer value for each subjob. The integers associated with each job match the values specified with your --array directive. For example, the following directive would tell the scheduler to submit three jobs, each with index 1, 2, and 3, respectively. ``` SBATCH --array=1-3 ``` Arrays can begin at any value, so the following option would also submit three jobs, each with index 8, 9, and 10, respectively. ``` SBATCH --array=8-10 ``` Non-sequential array indices can also be used. For example: ``` SBATCH --array=1-3,10,15-20 ``` Monitoring Array Jobs ¶ When you submit a job array, Slurm will assign a single job ID to the array. When you are tracking your jobs , if you run a standard squeue you will only see the parent job ID. If you use the command squeue -r --job <jobid> , that will return a list of all subjobs in the array where each job ID is formatted as <parent_job_id>_<job_array_index> . Output Files ¶ Each subjob in an array produces its own output file. By default, these are formatted as slurm-<parent_job_id>_<job_array_index>.out . Be careful if you set your own custom output filenames . If the output filenames are not distinguished from one another using an array task ID, your output files will overwrite one another. Example Jobs ¶ Basic Array Job Array with Text Filenames In this example, we'll use Python to analyze a series of input files called 1.inp , 2.inp , and 3.inp . This general methodology can be adapted to your own analyses using similar techniques. The most important takeaway is the use of the --array flag and the environment variable $SLURM_ARRAY_TASK_ID which is used to control each job's input. Let's say each of our input files has a series of space-delimited integers that we'd like to sum using a Python script. Rather than using a for loop to process each in a single job, or writing multiple submission scripts to explicitly use different input filenames, we can simply use an array job to define a different input file for each subjob. The structure of our working directory is shown below. Directory structure ``` [netid@junonia ~]$ tree . ├── input │ ├── 1.inp │ ├── 2.inp │ └── 3.inp ├── output ├── sum_inputs.py └── sum_inputs.slurm 2 directories, 5 files ``` Click to view Python script sum_inputs.py ``` !/usr/bin/env python3 import sys, os def sum_file(input_file): with open(input_file, \"r\") as f: data = f.read() numbers = [int(num) for num in data.split()] return sum(numbers) if name == \" main \": array_index = int(sys.argv[1]) input_file = os.path.join(\"input\",f\"{array_index}.inp\") output_file = os.path.join(\"output\",f\"{array_index}.out\") result = sum_file(input_file) with open(output_file, \"w\") as f: f.write(f\"Sum of numbers in {input_file}: {result}\\n\") ``` Below is the Slurm script we can use to execute this workflow. The specific lines that include the relevant batch directive and environment variable that make this an array job are highlighted below. sum_inputs.slurm ``` !/bin/bash SBATCH --job-name=sum_inputs SBATCH --output=slurm_logs/%x_%A_%a.out SBATCH --account=hpcteam SBATCH --partition=standard SBATCH --nodes=1 SBATCH --ntasks=1 SBATCH --time=00:01:00 SBATCH --array=1-3 module load python/3.11 python3 sum_inputs.py ${SLURM_ARRAY_TASK_ID} ``` In this case, we're passing $SLURM_ARRAY_TASK_ID to the Python script which uses its value to find the relevant input file. Submitting this script with sbatch will return a single job ID that we can use to track all three of our jobs. Job submission and tracking ``` [netid@junonia ~]$ sbatch sum_inputs.slurm Submitted batch job 3186754 [netid@junonia ~]$ squeue --job=3186754 -r JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 3186754_1 standard sum_inpu netid PD 0:00 1 (Priority) 3186754_2 standard sum_inpu netid PD 0:00 1 (Priority) 3186754_3 standard sum_inpu netid PD 0:00 1 (Priority) ``` Once our job completes, we can check our output: ``` [netid@junonia ~]$ cat output/* Sum of numbers in input/1.inp: 25 Sum of numbers in input/2.inp: 30 Sum of numbers in input/3.inp: 15 ``` Sometimes, using $SLURM_ARRAY_TASK_ID to directly define input isn't convenient. For example, in the biosciences often times sequence files (fasta, fastq, etc) might have names that are not conducive to sequential numerical ordering. In these cases, one option might be to store input filenames in a file and use the array ID to pull filenames from line numbers. This can be accomplished with the Bash command sed . As an example, let's say we have five input files, each with a protein sequence in fasta format whose structural disorder we want to calculate. Our directory structure might look something like the following: Directory structure ``` [netid@junonia array_filenames_example]$ tree . ├── input │ ├── AJD81427.fa │ ├── AMK26954.fa │ ├── AZU90721.fa │ ├── QDZ58854.fa │ └── QFP98592.fa ├── output └── predict_disorder.slurm 2 directories, 6 files ``` We can write these filenames to a list of input files by using the following: ``` [netid@junonia array_filenames_example]$ ls -1 input/ > InputFiles [netid@junonia array_filenames_example]$ cat InputFiles AJD81427.fa AMK26954.fa AZU90721.fa QDZ58854.fa QFP98592.fa ``` If we use the command sed , we can pull lines from this file using their line numbers. For example, to pull line three: ``` [netid@junonia array_filenames_example]$ sed \"3q;d\" InputFiles AZU90721.fa ``` If we replace the 3 in the above with ${SLURM_ARRAY_TASK_ID} we can submit an array and pull a different line for each subjob. For example: predict_disorder.slurm ``` !/bin/bash SBATCH --job-name=predict_disorder SBATCH --output=slurm_output/%A_%a.out SBATCH --account=hpcteam SBATCH --partition=standard SBATCH --nodes=1 SBATCH --ntasks=1 SBATCH --time=0:05:00 SBATCH --array=1-5 input_filename=\"$( sed \"${SLURM_ARRAY_TASK_ID}q;d\" InputFiles )\" output_filename=\"$( echo $input_filename | awk '{sub(/.fa$/, \".out\"); print}')\" iupred3 input/${input_filename} long > output/${output_filename} ``` In the script above, the parts specific to array jobs are highlighted. We're setting the Bash variable input_filenames to the output of our sed command, which is the filename specific to that subjob. The command awk is being used to replace the file extension .fa with .out so we can save our output data. We then run our example analyses using the protein disorder prediction software iupred3 on the input file and save it to the output file. Submitting with sbatch, we can track our five subjobs: ``` [netid@junonia array_filenames_example]$ sbatch predict_disorder.slurm Submitted batch job 3207840 [netid@junonia array_filenames_example]$ squeue --job=3207840 -r JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 3207840_1 standard predict_ netid PD 0:00 1 (Priority) 3207840_2 standard predict_ netid PD 0:00 1 (Priority) 3207840_3 standard predict_ netid PD 0:00 1 (Priority) 3207840_4 standard predict_ netid PD 0:00 1 (Priority) 3207840_5 standard predict_ netid PD 0:00 1 (Priority) ``` When our analyses are complete, we should see an output file corresponding to each of our input files: ``` (ocelote) [netid@junonia array_filenames_example]$ tree . ├── input │ ├── AJD81427.fa │ ├── AMK26954.fa │ ├── AZU90721.fa │ ├── QDZ58854.fa │ └── QFP98592.fa ├── InputFiles ├── output │ ├── AJD81427.out │ ├── AMK26954.out │ ├── AZU90721.out │ ├── QDZ58854.out │ └── QFP98592.out ├── predict_disorder.slurm └── slurm_output ├── 3207840_1.out ├── 3207840_2.out ├── 3207840_3.out ├── 3207840_4.out └── 3207840_5.out 3 directories, 17 files ``` Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/batch_jobs/array_jobs/",
      "title": "Array Jobs - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "f29ab7d8-4733-4f59-a2a1-20fcb16bfc85",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/batch_jobs/array_jobs/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "ArcGIS Pro Heightmaps This documentation is provided courtesy of Glenn Ingram, Graduate Assistant of Kiri Carini in the Main Library Data Cooperative . His personal information can be found at https://data.library.arizona.edu/geo/about-geospatial-team https://glenningram.github.io/ LiDAR Raster for Blender 3D This tutorial illustrates how to use ArcGIS Pro to prepare a raster suitable for Blender 3D out of .laz Lidar Data. This is also an illustration of the first step for a workflow that will utilize Blender\\ Command Line\\ Rendering . ArcGIS Pro is utilized because all the steps can be completed in a single software - if you do not have access to ArcGIS Pro, a similar process can be achieved using LASTools and QGIS. Step One: Finding Data: The University of Arizona provides a .laz Lidar Dataset for Arizona. To start: Find the corresponding tiles to your area of interest on the Index Map . Select as many tiles as needed, but make sure to use a square or rectangular grid of tiles. Record the tile labels. Using the tiles labels, find the LAZ Files in CyVerse here (Make sure you download the .laz file format) Step Two: Convert LAZ Unlike LAS (.las) files, LAZ (.laz) files cannot be opened or added directly to ArcGIS Pro to display point cloud data on a map. However, it is possible to convert the LAZ files to LAS datasets to show on the map. Open ArcGIS Pro. Under geoprocessing, use the Convert LAS tool to convert each .laz file to a .las. Fill in the tool parameters as required and complete this step for each .laz file. For additional information or help, refer to Convert LAS . Step Three: Create LAS Dataset Once files are converted to .las, we can build a dataset. Use the Create LAS Dataset tool found in geoprocessing. Refer to Create LAS Dataset for more information. Step Four: Dataset to Raster The dataset can be converted to a raster once all .las files are collected in the dataset. Use the LAS Dataset to Raster tool. More information here . Step Five: Project Raster Once the raster goes into Blender, it will lose any coordinate data and any projection occurring in ArcGIS. Reprojecting the raster will ensure that the relief produced in Blender aligns with any other vectors or imagery being used in the project. Determine which projection is best for your project and reproject the new raster. The rest of the project will have to remain in this projection to stay aligned. Step Six: Raster Calculator Your raster likely included decimal values, which Blender will not process well. Use the Raster Calculator and the “Int” function to change raster values to whole numbers. While using the Raster Calculator , we need to rescale the raster for use in Blender. A raster pixel can hold a value between 0 and 65,535; we will have to use the maximum value to create the highest quality render in Blender. The Raster Calculator can be used to rescale the raster. The equation we will be using is (Pixel Value – Lowest Value) ÷ (Highest Value – Lowest Value) * 65,535 In the Raster Calculator , this will look like (“raster” – Lowest Value) / (Highest Value – Lowest Value) * 65535 Step Six: Copy Raster With our raster rescaled, it is time to export. We will use the Copy Raster tool. Set the Pixel Type to “16 bit unsigned” and Format to “Tiff.” and output to a location on your computer. Bonus Step: Open in Photoshop Opening the raster and resaving as a tiff is an easy step that “cleans” the file and reduces the file size without losing quality if you have access to Photoshop. You can also use photoshop to view your rasters pixel size - a needed input for Blender. Final Step: Blender Cartographer Daniel Huffman has an extensive and detailed tutorial walk-through for setting up Blender for creating shaded reliefs. Use these steps for creating your Blender file. When the Blender settings are complete, save the file as a package, and you are ready to use the HPC! Remember, your raster has already been prepared for Blender in ArcGIS Pro, so some early steps in the tutorial will be redundant. Good luck! {\"serverDuration\": 20, \"requestCorrelationId\": \"531c1ee744e546d58d1dc765a3d589c4\"} Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/arcgis_pro_heightmaps/",
      "title": "ArcGIS Pro Heightmaps - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "f7bcacbc-587f-469b-acaf-67af0a197d50",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/arcgis_pro_heightmaps/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Getting Started with Paraview GUI ParaView is a powerful data visualization software that many researchers can find useful for getting a visual understanding of their data. This guide will be a brief introduction to ParaView including how to install it and use its GUI interface. For more information on how to use the ParaView terminal (PvPython), see Getting Started With ParaView Terminal . Installing ParaView ¶ You can setup ParaView on our HPC cluster or on your workstation. Follow the instructions below based on which system you want to install ParaView on. Choose your system HPC Workstation These instructions will get you setup with the Paraview GUI on our HPC systems. Feel free to copy and paste this code into an OOD Remote Desktop Terminal, and consult the lower explanations for details about each line. ``` apptainer pull docker://ghcr.io/devinbayly/vtk:latest wget \"https://www.paraview.org/paraview-downloads/download.php?submit=Download&version=v5.11&type=binary&os=Linux&downloadFile=ParaView-5.11.0-MPI-Linux-Python3.9-x86_64.tar.gz\" -O paraview.tar.gz tar xf paraview.tar.gz apptainer exec vtk_latest.sif ./ParaView-5.11.0-MPI-Linux-Python3.9-x86_64/bin/paraview ``` ParaView is available for Windows, Mac, and Linux and can be downloaded from https://www.paraview.org/download/ . You can choose which version to download from a drop-down menu on the page. This tutorial uses version 5.11.0, but you can use other versions if those work for you better. Run the downloaded executable (on Linux you will have to extract the files from a tarball) and follow the instructions to install it. Running ParaView GUI ¶ To run the ParaView GUI, find the ParaView executable and run it on your computer. Once open, close any popups and you will see the default layout. If you plan on working with Python, from the top drop-down you can click View → Python Shell . This Python shell is equivalent to PvPython in the ParaView Terminal option. To get started with the terminal, first, add a shape. Try adding a sphere to the scene from the drop-down by clicking Sources → Geometric Shapes → Sphere . While nothing is added to the view here, you can see that in the pipeline tab on the left, a sphere has been added. To show this in the view, you can click the eye icon to the left of the new object. To maneuver in the view you can use your left mouse button to rotate, your right mouse button to zoom, and your middle mouse button to pan. Alternatively, Shift + right mouse button can be used to pan and the scroll wheel can be used to zoom. The view can be reset at any time with the Reset button shown below: Loading Data ¶ Let’s load some example data into ParaView. Click the Open button from the toolbar (shown below), or go to File → Open , or press Crtl + O . On the left side under Favorites, there should be a directory called Examples . From that, click can.ex2 and hit OK . No data should appear yet but information about the data should appear in the properties tab. In this, we can select what information from our data set we want to load. For this example, we can leave all the default settings. Click Apply to confirm the settings and you should see the example data appear in the view. Changing The Visualization ¶ This visualization can be played with the green play button at the top of the screen. Currently, the colors just show the two different shapes. Let’s say we want to visualize the acceleration of all the points of the shape. In the toolbar, find the drop-down that currently says vtkBlockColors and switch it to VEL . Play the animation again to visualize the result. While this does provide some useful information, it may not be the best way of visualizing velocity. Instead, let's add some vectors to see how each point is moving. Start by adding a Glyph to your data. With your data selected click the Glyph button from the toolbar. Next, set both the Orientation Array and Scale Array to VEL and hit Apply . You should notice that the arrows do get added but they are way too large. Change this by editing your scale factor until it looks right (0.0005 worked for me). And there you have it! You can play the animation and watch as the arrows change with time. Later tutorials will work with specific data types as well as other features included in ParaView but after this, you should have a base understanding of how to navigate the scene and load some basic data. ParaView on HPC video walkthrough ¶ Visualization on HPC - YouTube University of Arizona UITS Research Technologies 98 subscribers Visualization on HPC University of Arizona UITS Research Technologies Search Watch later Share Copy link Info Shopping Tap to unmute If playback doesn't begin shortly, try restarting your device. More videos More videos You're signed out Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer. CancelConfirm Share Include playlist An error occurred while retrieving sharing information. Please try again later. Watch on 0:00 0:00 / 3:50•Live • Watch on YouTube Was this page informative?",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/getting_started_with_paraview_gui/",
      "title": "Getting Started with Paraview GUI - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "f3118c03-c7af-491c-b627-0398923221c6",
      "viewport": [
        "width=device-width,initial-scale=1",
        "width=device-width, initial-scale=1"
      ],
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/getting_started_with_paraview_gui/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Graphs and Exporting Data While visualizations in the terminal are ParaView's strength, it also includes tools to plot data for different types of visualizations. Creating Graphs In Paraview ¶ For this, I’m going to use the same example as in Getting Started With ParaView GUI . The first thing you want to do is select the source you want to work with. Next, from the toolbar select Filters → Data Analysis then select whichever plot works best for you. For this example, I’m going to select Plot Data Over Time. For some options, a warning will pop up stating that it could take a long time to plot the data. This is certainly the case for large data sets but if you're using the example, it should take only a second. Click apply in the properties tab and an indicator in the bottom right should appear as your data gets processed. Once your data is fully processed, a graph should appear, splitting the viewport in two. Keep in mind that what is visible is view specific so selecting the 3D viewport will cause an eye to appear next to the base source in the pipeline (indicating that it is what's being shown in this view), and likewise selecting the graph view will remove the eye on the source and put it on the new graph filter. To demonstrate, compare the two images above. In the first, I clicked on the viewport giving it a blue border to indicate it's selected and also placing the eye icon on can.ex2 in the pipeline (because it's what's being shown in the viewport). Then in the second image, once I click on the graph, the blue border changes and the eye icon moves to PlotDataOverTime1. From this point, you can use the properties menu to modify the values of the graph as you see fit, making sure to have the graph selected in the Pipeline Browser. After changes are made, many will require you to press Apply at the top of the properties menu for them to take effect. Exporting Data To A Spreadsheet ¶ While ParaView does have many data management tools and graphs, it's certainly not as many as other data analysis software. To use the data in other programs, you'll need to export it to a format of your choice, probably the most simple of which is a CSV (a standard format for spreadsheets). Select your source whose data you'd like to export, then, from the toolbar, go to Extractors → Data → CSV . Make the desired changes in the properties tab, press Apply , then go to File → Save Extracts . At this point select where you would like to save the data and it should generate CSVs in the specified folder. Common Issues ¶ If your data has outliers or null values, many plotting functions have difficulties correctly reading the data. If this is the case, apply a Threshold filter ( Filters → Common → Threshold ), scale the data to whatever range you want, then apply the graph to the new threshold filter. Note that you'll have to remove the old graph, add the threshold, then re-add the graph to the threshold. For example (before and after adding Threshold): Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/graphs_and_exporting_data/",
      "title": "Graphs and Exporting Data - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "fd1c6dcc-57d9-4813-9757-c9811d8e6c7f",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/running_jobs/visualization/paraview/graphs_and_exporting_data/",
      "statusCode": 200
    }
  }
]