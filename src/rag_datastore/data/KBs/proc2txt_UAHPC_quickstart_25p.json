[
  {
    "processed_text": "Skip to content Welcome to the UArizona HPC Documentation Site ¶ Introduction ¶ The University of Arizona offers High Performance Computing (HPC) resources in the Research Data Center (RDC), a state-of-the-art facility that hosts our large computer clusters. HPC services are available at no cost to researchers. Each faculty member is eligible for a free standard allocation of CPU time and storage space . This documentation site provides technical details relevant to using our HPC system. Whether you are just starting your journey into computational sciences or are a seasoned programmer, we hope you will find something useful in these pages. This site is managed by the HPC Consult team. Please contact us if you have questions or comments about the content of this site. Featured Links ¶ Account Creation If you are an active UArizona affiliate (e.g. student, post-doc, faculty), you can register an account. If you are not affiliated with UArizona but are working with collaborators here, you can register as a Designated Campus Colleague (DCC). HPC Quick Start If you are new to the UArizona HPC, or to HPC in general, our self-guided quick start tutorial will provide a solid foundation for using our system. Getting Help \\| FAQs Find out how to contact the HPC Consult team, or view our FAQs, glossary, cheat sheets, and more! Events Calendar \\| Workshop Materials Every semester we host training sessions on topics including intro to HPC, machine learning, parallel computing, and beyond. Click the link above to see our workshop schedule, our old training materials, and find more workshops from around campus. System Highlights 2024 ¶ Download Full Report Over the past four years, Puma has been a vital resource for researchers at the University of Arizona. In fiscal year 2024, our systems supported over 300 million compute hours enabling diverse research spanning climate modeling, genomics, neuroimaging, and more. With more than 80 active departments, 500 principal investigators, and 1,500 users, Puma continues to advance discovery and innovation. Researchers rely on our HPC resources not only for computational power, but also for expert support. This past year, we assisted users with nearly 2000 support requests. Explore highlights from this year, including testimonials from researchers and key statistics by downloading the full annual report. Highlighted Research ¶ Ten Millionth Job Planetary History Cloud Research Hypersonic Flow Puma has been a tremendous resource for our research community. Just recently it processed the 10 millionth job since we provisioned it in 2020. Just to get a perspective on that, if you took one step for each job you could walk to Niagara Falls. And back. David Castellano, a member of Dr. Ryan Gutenkunst's team, was the researcher who achieved this milestone. They study the evolutionary processes that generated the complex networks that comprise life. Dr. Gutenkunst told us that David’s been maximizing his chances to hit this milestone with all the jobs he’s been running. David says about his work: “Understanding the relationship between DNA mutation rates and fitness effects is central to evolutionary biology. My work is investigating this relationship in three species: Homo sapiens, Mus musculus, and Arabidopsis thaliana. The inference of fitness effects from population genomics data requires intensive computation which could not be possible without a High Performance Computing service.” The software used in their research is called ‘dadi’: Diffusion Approximations for Demographic Inference. This work on three species with 96 mutation types and 1000 bootstrap replicates equates to 288,000 compute jobs. Reconstructing the History of the Solar System Using HPC Erik Asphaug’s Planetary Formation Lab in the Lunar and Planetary Laboratory uses smoothed-particle hydrodynamics (SPH) simulations to explore how collisions between bodies in the Solar System shape its evolution through time. These three-dimensional simulations, which approximate planetary bodies as collections of particles, incorporate realistic geologic properties to track their structural and thermal changes during and after giant impacts. From Eric: “The access to increased time allocations as well as large volumes of temporary storage on xdisk provided by the HPC has revolutionized our ability to run our most complex simulations at high resolution, with enough space and time to explore the full parameter space necessary to make key discoveries that inform our understanding of Solar System evolution.” One of their major projects has occupied a large fraction of their HPC hours and storage: the capture of Pluto’s moon, Charon, from a giant impact early in the Solar System’s history. High resolution is also critical to track detailed interactions between Pluto and Charon, including any material transferred between them. Without the HPC and the allocation of computation time and storage space, they would not have been able to run the hundreds of models necessary to successfully reproduce systems that look similar to Pluto and Charon today. The models have revealed new insights about how bodies like Pluto capture satellites: the dwarf planet and its proto-satellite collide, briefly merge, and then re-separate as Charon slow begins to move outward. They call this new process, which significantly redefines our understanding of giant collisions, “kiss and capture.” An example kiss-and-capture is shown in the image above. The simulation shown covers 60 hours of model time, which takes ~1.5 months on the HPC. The ability to run such long simulations in parallel was crucial to completing this work. Read more about the full story here! Sylvia Sullivan is an Assistant Professor in Chemical and Environmental Engineering who performs atmospherically related research and has a joint appointment to the Department of Hydrology and Atmospheric Sciences. Her academic background is in chemical engineering, but she has picked up atmospheric science and computing skills along the way to model and understand cloud and storm systems. “I really liked environmental work because I felt it was very impactful,” she says. Her research includes investigating cloud ice formation. From a chemical engineering perspective, you can think about clouds as a control volume, flows in and out and phase changes occurring inside. Along with this more technical view, Sylvia says she “fell in love with clouds because they are very beautiful and poetic”. This blend of fields brought her to the University of Arizona as it is one of the only Universities where Chemical and Environmental Engineering are in the same department. And besides, “Tucson is a wonderful location”. She is building a research group to study the impact of ice clouds, particularly their energetic and precipitation effects. Sylvia’s group runs very high-resolution simulations called storm resolving simulations, where the meshes are fine enough to represent individual storms. In global climate models, the mesh has a resolution on the order of 100 km, in which several storm cells can form simultaneously. These storm-resolving computations are very expensive and produce terabytes of data, which then need to be post-processed and visualized. Currently, Sylvia and her group are very focused on working with other visualization experts on campus to illustrate the structures and evolution of clouds and storm systems. Faster Speeds Need Faster Computation Professors Christoph Hader, Hermann Fasel, and their team are exploring the use of our GPUs to optimize Navier-Stokes codes for simulating the flow field around hypersonic vehicles traveling at size times the speed of sound (Mach 6) or more. In the image to the right, instantaneous flow structures obtained from a DNS for a flared cone at Mach 6 are visualized using the Q-isocontours colored with instantaneous temperature disturbance values. The small scales towards the end of the computational domain indicate the regions where the boundary layer is turbulent. Available Resources ¶ Our Clusters (click to expand) Puma Ocelote El Gato Implemented in the middle of 2020, Puma is the biggest cat yet. Similar to Ocelote, it has standard CPU nodes (with 94 cores and 512 GB of memory per node), GPU nodes (with Nvidia V100) and two high-memory nodes (3 TB). Local scratch storage increased to ~1.4 TB. Puma runs on Rocky Linux 9. As is the case for our other supercomputers, we use the RFP process to get the best value for our financial resources, that meet our technical requirements. This time Penguin Computing one with AMD processors. This is tremendously valuable as each node comes with: Two AMD Zen2 48 core processors 512GB RAM 25Gb path to storage 25Gb path to other nodes for MPI 2TB internal NVME disk (largely available as /tmp) Qumulo all flash storage array for shared filesystems Two large memory nodes with 3TB memory and the same processors and memory as the other nodes Six nodes with four Nvidia V100S GPU's each Ocelote arrived in 2016. Lenovo's Nextscale M5 technology was the winner of the RFP mainly on price, performance and meeting our specific requirements. Ocelote has one large memory node with 2TB of memory and 46 nodes with Nvidia P100 GPUs for GPU-accelerated workflows. This cluster is actually the next generation of the IBM cluster we call El Gato. Lenovo purchased IBM's Intel server line in 2015. In 2021, Ocelote's operating system was upgraded from CentOS6 to CentOS7 and was configured to use SLURM, like Puma. It will continue until it is either too expensive to maintain or it is replaced by something else. - Intel Haswell V3 28 core processors - 192GB RAM per node - FDR infiniband for fast MPI interconnect - Qumulo all flash storage array (all HPC storage is integrated into one array) - One large memory node with 2TB RAM, Intel Ivy Bridge V2 48 cores - 46 nodes with Nvidia P100 GPU's Implemented at the start of 2014, El Gato has been reprovisioned with CentOS 7 and new compilers and libraries. From July 2021 it has been using Slurm for job submission. El Gato is our smallest cluster with 130 standard nodes each with 16 CPUs. Purchased by an NSF MRI grant by researchers in Astronomy and SISTA. Compute UArizona HPC systems are available to all university faculty, staff, undergraduate and graduate students, postdocs, and designated campus colleagues (DCCs) at no cost. Researchers have access to compute resources on our three clusters Puma, Ocelote, and El Gato located in our data center. Presently each research group is provided with a free standard monthly allocation on each: 100,000 CPU-hours on Puma, 70,000 CPU-hours on Ocelote, and 7,000 CPU-hours on El Gato. Funding Sources UArizona HPC systems are funded through the UArizona Research Office (RII) and CIO/UITS (Chief Information Officer, and University Information Technology Services). Staff is funded to administer the systems and provide consulting services (no charge) for all researchers. Regulated Research These resources specifically do not support Regulated Research, which might be ITAR, HIPAA or CUI (Controlled Unclassified Information). For more information on services that can support regulated research, see: HIPAA support services and CUI support services . News ¶ Puma OS Update As of January 29th, 2025, Puma’s operating system has been updated from CentOS 7 to Rocky Linux 9. Need help transitioning to the new operating system? Visit our migration documentation for detailed instructions. Fall Semester Workshops This Fall semester we are conducting the workshops in a different manner. Rather than compress them into a week, there will be one each Friday at 11am. We plan to use a hybrid modality – you can attend in person which provides greater opportunity to engage; or attend virtually by Zoom. In person sessions will be held in Weaver Science and Engineering Library Rm 212. There will be a recorded version made available on YouTube. Registration Form Calendar July 2024 Maintenance User portal interface change for mobile compatibility. Open OnDemand graphical jobs limited to four days, reduced from 10 days. For workflows that need longer than four days, batch jobs can be used. Contact our consultants for help if you're unsure how to do this. New partitions have been introduced for GPU jobs. This will prevent non-GPU jobs from running on GPU nodes, improving availability. See batch directives for more information on how to request GPU nodes. New Ocelote GPUs We recently added 22 new P100 GPUs to Ocelote. Need to request multiple GPUs on a node and you're finding Puma queue times too slow? You can now request two GPUs per node on Ocelote using --gres=gpu:2 . Acknowledgements ¶ Published research that utilized UArizona HPC resources should follow our guidelines on how to acknowledge us. If you wish for your research to be featured in our Results page, please contact HPC consult with news of the publication! We respectfully acknowledge the University of Arizona is on the land and territories of Indigenous peoples. Today, Arizona is home to 22 federally recognized tribes, with Tucson being home to the O’odham and the Yaqui. Committed to diversity and inclusion, the University strives to build sustainable relationships with sovereign Native Nations and Indigenous communities through education offerings, partnerships, and community service. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/",
      "title": "UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "b3b19777-59ad-4135-af4e-da40d7aec616",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Updates ¶ The Puma cluster is currently being updated from the outdated CentOS 7 operating system to Rocky Linux 9. This will involve updating both the operating system and much of the installed software. A small number of nodes were previously made available in a testing environment. At the October 30th maintanance, an initial subset of compute nodes will be available for full production runs. During November and December 2024 more nodes will be updated. In January all nodes will be updated, by which point all users must transition to continue using Puma resources. See the below for information on using the new system. This update will have a number of impacts, and we are taking steps to minimize disruptions to HPC users. Things to not worry about All user files will remain unchanged. The Slurm scheduler and procedure for submitting jobs will remain unchanged. HPC systems will continue to be available for use during the OS update. Things to note Compute hour allocations on the test system will be independent of allocations on the existing Puma system. Significant changes to system software have occurred, most importantly: updating the default compiler from GCC 8 (module gnu8 ) to GCC 13 (module gnu13 ). updating the default MPI distribution from OpenMPI 3 (module openmpi3 ) to OpenMPI 5 (module openmpi5 ). Some previously compiled software will not run on the new system, and will need to be recompiled. HPC staff have recompiled public software modules when necessary. The majority of software modules previously available will remain available, sometimes with version changes. Submitting jobs to the updated cluster ¶ Subsets of nodes will be converted to the new OS, effectively creating a new cluster. The updated cluster can be selected by entering puma9 , just as the target cluster is currently specified by entering puma , ocelote or elgato . The resources available on the Puma9 cluster will remain functionally identical to those on the existing Puma, including the number of CPUs per node and memory per CPU. Thus, Slurm batch scripts that work properly on Puma should work on the new Puma without modifications. Adapting existing analyses. ¶ Support Please open a support ticket if you have difficulty using the new updated Puma9 system. Researchers may currently use cluster resources in a number of ways. Here are general guidelines for adapting to the new environment: Users of Open OnDemand Open OnDemand users should generally be able to continue usage unchanged. However, users of RStudio should see the R section below. Users of software modules Users who load analysis software with the module load command will generally be able to continue their analyses unchanged. All commonly used software modules have been transferred to Puma9, sometimes with version updates or changes. Users of Conda environments We recommend switching to Mamba going forward. However, many Conda users should be able to continue using their existing environments, although in some cases may need to recreate them. Users of Python Users who run pure Python code, with or without the use of virtual environments will likely be able to continue their analyses unchanged. The same versions of Python are available on Puma9. Users of R The primary challenge for R users will be to maintain separate R package libraries for the old and new operating systems. R packages installed under the old CentOS 7 operating system may not function under the new Rocky 9 systems, and vice versa. R users should maintain separate R libraries and switch between them as necessary. See the Creating a Custom Library and Switching Between Custom Libraries sections on our R documentation page for details. We suggest that your Puma9 R library be named something like ~/R/library_4.4-puma9 . Users who compile code themselves In many cases user-compiled software will need to be recompiled to run on Puma9. Important software changes ¶ The procedure for using the Conda package manager is expected to change, and existing environments will likely need to be rebuilt. More news on this will be forthcoming. In addition to the currently available Cuda 11.8, Cuda 12.4 and 12.5 will also be available. Intel compiled software ¶ On the previous system, a number of software modules were provided for use in compiling software using the Intel compiler, e.g., modules hdf5-intel , netcdf-intel , etc. On the new system, you should first load the Intel software module with module swap gnu13 intel . After loading the Intel module, you can load the Intel specific modules without the -intel specifier, i.e., as hdf5 , netcdf , etc. | old module name | use now | | --- | --- | | hdf5-intel | hdf5 | | netcdf-intel | netcdf | | netcdf-cxx-intel | netcdf-cxx | | netcdf-fortran-intel | netcdf-fortran | | petsc-complex/intel | petsc | | petsc-real/intel | petsc | | gsl-intel | gsl | | phdf5-intel | phdf5 | Anaconda / Conda / Mamba ¶ Mamba Guide See Mamba for our guide on using Mamba in place of Anaconda. While not part of the OS updates itself, we want to take this opportunity to bring to your attention upcoming changes to our use of Anaconda. Anaconda.org has started enforcing license restrictions which has necessitated these changes. You can read more about the license restrictions here . In short: The license restrictions only come into effect when a user installs packages from The Anaconda Repository . The license restrictions do not apply if a user uses the conda tool to install packages from community repositories like conda-forge . The existing Anaconda modules on the HPC automatically load The Anaconda Repository, and thus come under the purview of the license restrictions. If you use a local installation of conda ( not the Anaconda modules), and you only install packages from community repositories, you need not read any further. However, you might still find the following useful. We are considering the following changes to help users transition to a new setup that will not be affected by the license restrictions: Deprecating the existing Anaconda modules. We will not install any new version of Anaconda. The existing modules will stay for a while, likely not beyond the end of the year, to give users time to transition to the new setup. Creating a new module based on the Miniforge distribution. Miniforge provides access to two tools — conda and mamba — and the conda-forge repository. With the Miniforge module you will be able to do the following: Install software from conda-forge , which is an extensive community repository. Whatever software you want, you will likely find it there. Of course, if required you will be able to use other repositories, except The Anaconda Repository. You can use mambda instead of conda . While access to conda will not go away, mamba is a much, much faster drop-in replacement for conda . Almost anything conda can do, mamba can do faster. Sometimes even succeeding where conda fails. We think you will have a much pleasant experience with mamba than with conda . If you are on the fence about this change, we highly recommend that you use the transition period to recreate and test your workflows and scripts with the mamba + conda-forge setup. Please do not wait till the last moment to make the necessary changes. While mamba is a drop-in replacement for conda , it is not a guarantee that your existing scripts will not break. Going forward, mamba + conda-forge will be the main setup that we support. We will share more details about how to use mamba shortly. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/resources/updates/",
      "title": "Updates - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "46831a66-e3c9-40ac-95d4-c66dfbaacd56",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/resources/updates/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Summary ¶ That's it! You now should have a sense of how UArizona's HPC systems are laid out, how to log in, and how to access compute nodes and software. To continue learning about HPC, our online documentation has a lot more information that can help get you started. For example FAQs , information on HPC storage , and file transfers . You may also want to check out our workshops as well as other community events which offer great training materials on a variety of topics. Other great resources include: virtual office hours every Wednesday from 2:00-4:00pm, consultation services offered through ServiceNow, and a YouTube channel with training videos. External Resources ¶ While this documentation covers some concepts and techniques that are widely applicable, it's beyond our scope to cover everything you may need or want to know while using the HPC. We highly encourage users to supplement their learning with resources from around the web! Below are some recommended resources to get started. | Bash & Linux reference | External Documentation | Misc HPC Tutorials | | --- | --- | --- | | Bash Manual | Slurm | Intro to HPC | | Bash scripting beginner tutorial | HPC Wiki | Lawrence Livermore HPC Tutorials | | Intro to Bash scripting e-book | | Software Carpentry | | General purpose linux tutorials | | Cornell HPC Roadmaps | Please note that information related to other HPC clusters may not necessarily apply to the UArizona HPC cluster. Each system is configured differently, and what may be supported on another system may not be supported here, or vice versa. Please refer to this documentation site for information specific to the UArizona HPC, or ask HPC Consult if you have any questions. Great Job Reaction GIF - Find & Share on GIPHY \\ '\\ \\ ' Related Gifs Continue Watching on GIPHY Embed Link Was this page informative?",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/quick_start/summary/",
      "ogUrl": "https://media4.giphy.com/media/XreQmk7ETCak0/giphy.gif?cid=dda24d50d0fmr2xyd6yzkplrm2hetp50x0b293alyyk6lsew&ep=v1_internal_gif_by_id&rid=giphy.gif&ct=g",
      "title": "Summary - UArizona HPC Documentation",
      "author": "GIPHY",
      "og:url": "https://media4.giphy.com/media/XreQmk7ETCak0/giphy.gif?cid=dda24d50d0fmr2xyd6yzkplrm2hetp50x0b293alyyk6lsew&ep=v1_internal_gif_by_id&rid=giphy.gif&ct=g",
      "robots": "noindex, noimageindex, noai, noimageai",
      "favicon": {},
      "og:type": "video.other",
      "ogImage": "https://media4.giphy.com/media/XreQmk7ETCak0/200.gif?cid=dda24d50d0fmr2xyd6yzkplrm2hetp50x0b293alyyk6lsew&ep=v1_internal_gif_by_id&rid=200.gif&ct=g",
      "ogTitle": "Great Job Reaction GIF - Find & Share on GIPHY",
      "keywords": "Animated GIFs, GIFs, Giphy",
      "language": "en",
      "og:image": "https://media4.giphy.com/media/XreQmk7ETCak0/200.gif?cid=dda24d50d0fmr2xyd6yzkplrm2hetp50x0b293alyyk6lsew&ep=v1_internal_gif_by_id&rid=200.gif&ct=g",
      "og:title": "Great Job Reaction GIF - Find & Share on GIPHY",
      "scrapeId": "0c78a28a-8401-4cfe-b8fe-2025c44e7415",
      "viewport": [
        "width=device-width,initial-scale=1",
        "width=device-width, initial-scale=1"
      ],
      "fb:app_id": "406655189415060",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/quick_start/summary/",
      "ogSiteName": "GIPHY",
      "statusCode": 200,
      "description": "Discover & share this Animated GIF with everyone you know. GIPHY is how you search, share, discover, and create GIFs.",
      "og:site_name": "GIPHY",
      "alexaVerifyID": "HMyPJIK-pLEheM5ACWFf6xvnA2U",
      "ogDescription": "Discover & share this Animated GIF with everyone you know. GIPHY is how you search, share, discover, and create GIFs.",
      "og:description": "Discover & share this Animated GIF with everyone you know. GIPHY is how you search, share, discover, and create GIFs."
    }
  },
  {
    "processed_text": "Skip to content Account Deletion ¶ Manual Deletion ¶ If you wish to delete your HPC account, you may do so through the User Portal . Navigate to the Support tab and click the Close Your HPC Account link. In a new window, you will be prompted to manually confirm by entering confirm at the prompt. Click Close Account to complete the process. Loss of University Affiliation ¶ Losing affiliation with the university will result in the denial of access to HPC resources. This will happen automatically on the day of termination according to the University of Arizona Records Database. Data may be retrievable if a student or employee is reinstated, or by the PI. More details are found under Policies . Please contact us for support in this case. If you are losing affiliation and require continued access to HPC services, you may register as a Designated Campus Colleague (DCC) through Human Resources. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/registration_and_access/account_deletion/",
      "title": "Account Deletion - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "1335ea05-df2b-4818-b708-90604e7dd234",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/registration_and_access/account_deletion/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Maintenance ¶ Planned Maintenance ¶ Most maintenance is performed during regular hours with no interruption to service. System wide maintenance is usually planned ahead of time and is scheduled for Wednesdays from 8:00 AM to 5:00 PM with at least 10 days notice. These will be planned to occur four times per year. Maintenance windows represent periods when UITS may choose to drain the queues of running jobs and suspend access to the cluster operation for HPC maintenance purposes. The notification will describe the nature and extent (partial or full) of the interruptions of HPC services. System-wide Maintenance ¶ Impacts to job queues During system-wide maintenance cycles, jobs queues are impacted before and during maintenance. Jobs submitted whose runtimes would overlap with maintenance are held until maintenance is concluded. Some maintenance cycles require the entire system to be taken offline. In preparation, batch queues will be modified prior to scheduled downtimes to hold jobs which request more wallclock time than remains before the shutdown. Held jobs will be released to run once maintenance concludes. Rolling maintenance ¶ Impacts to job queues During rolling maintenance cycles, job queues are impacted during and after maintenance. All nodes are drained, meaning they cannot accept new jobs and must allow running jobs to complete before they can be updated, rebooted, and put back online. The system may be slower to accept new jobs for 10 days following these maintenance cycles. Rolling maintenance cycles are implemented to facilitate updates or maintenance tasks without necessitating a complete system shutdown. Throughout rolling maintenance, nodes will stop accepting new jobs, allowing currently running tasks to finish uninterrupted. As nodes gradually become vacant, they are taken offline, updated, rebooted, and then restored to service. This iterative process ensures minimal disruption to ongoing computational tasks while maintenance is underway. It's important to note that during rolling maintenance cycles, job queues may experience a temporary slowdown as nodes await reboot. Emergency Maintenance ¶ Unavoidable (emergency) downtime may occur as a result of any of the above reasons at almost any time. Such events are rare and great effort is made to avoid these situations. However, when emergency maintenance is needed, the UITS unit responsible for the item affected will provide as much notice to users as possible and work to resolve the fault as quickly as possible. Any emergency outages will be announced via email through the hpc-announce@list.arizona.edu mailing list. Maintenance History ¶ January 29, 2025 Type : Rolling Maintenance Routine patching of all nodes. Completion of OS migration. All remaining CentOS 7 Puma nodes migrated to Rocky Linux 9. Puma9 made default cluster. October 30, 2024 Type : Rolling Maintenance Routine patching of all nodes and storage array. OS upgrades continue. A block of nodes were migrated from CentOS 7 to Rocky 9, accessible from the login nodes using puma9 . More details can be found on our OS Updates page . July 31, 2024 Type : Rolling Maintenance OnDemand graphical jobs limited to four days to support general resource availability. User portal upgraded to support mobile clients. New GPU partitions introduced to improve GPU resource availability. April 24, 2024 Type : Rolling Maintenance OnDemand Upgrade. Gatekeeper moved to EL8 operating system. Enabled job script storage in slurm accounting. January 31, 2024 Type : Rolling Maintenance General operating system patches. Qumulo storage array update. Slurm configuration improvements. Metrics (XDMOD) OS upgrade. RStudio Server support for R version 4.3.2. OnDemand OS upgrades. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/policies/maintenance/",
      "title": "Maintenance - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "1a891e09-1dc2-4e53-a282-bd233969447d",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/policies/maintenance/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Group Management ¶ Overview ¶ HPC groups allow faculty members to manage file permissions, job allocations, and group members. When a PI creates a new HPC account, a group is created with an allocation of space and time. Additional storage is available for free upon request, called xdisk There are two types of Groups: Research Groups Research groups include any faculty, postdocs, graduate students, DCCs, staff, or student workers actively affiliated with your group's research. Class Groups These are for educational purposes only and will include students enrolled in a semester-long course. Research Groups ¶ If you are a faculty member who has registered for an HPC account , a research group named after your UArizona NetID has been automatically created for you. This group has an allocation of CPU hours associated with it as well as communal storage for your data . Membership and Allocation Members of research groups have full access to the PI's allocation. PIs are able to create multiple research groups that pull from the same allocation. Users are able to be members of multiple research groups. Creating multiple research groups does not change the total CPU-time allocation. Permissions Research groups can also be used to manage access permissions to files and folders on HPC. The PI can use the chgrp command to change which group has access rights corresponding to the 'group' setting on a particular item. See our Linux File Permissions cheat sheet for more information. Adding Members ¶ To add members to your research group, go to https://portal.hpc.arizona.edu/ and click the Groups tab at the top of the screen. Click your group's dropdown tab and navigate to where it says Add a new group member . In the field below, enter the new group member's NetID and click \" ADD MEMBER\". To add members in bulk, you may also select Upload Member List and upload a CSV file of UArizona NetIDs. The process of adding new members may take a few seconds to complete. Once the changes have taken place, you will see the user's NetID in your group. Creating a New Group ¶ A new group can be created at any time through the user portal. New groups will share their time and storage allocations with your primary group. Alternate research groups can be a good solution for managing file permissions. For example, if a particular directory and its contents needs restricted access, you could do this by creating a new research group, adding the group members who need access to those files, and then changing the group ownership of the files/directories. To create a new group, log into the user portal, navigate to the Groups tab and select the Add New Group dropdown menu. In the Group Name field, enter the name of the group you'd like to create. Under Group Type , select researchGroup , then select the to confirm. Once your group has been created, you will see it when running va (short for View Allocation) in the same block as your primary group: ``` (puma) [faculty-netid@junonia ~]$ va PI: parent_1206 Total time: 7000:00:00 Group: faculty-netid Time used: 0:00:00 Time encumbered: 0:00:00 Group: your-new-group Time used: 0:00:00 Time encumbered: 0:00:00 Total used: 0:00:00 Total encumbered: 0:00:00 Total remaining: 7000:00:00 ``` Class Groups ¶ Tip If you are interested in having an HPC staff member come to your class to do an Intro to HPC presentation, reach out to our consultants. If you are a faculty member and are teaching a course that makes use of HPC resources, you can create a class group that will grant your students system access. Class groups are designed to be created and used for one semester only. Class Group Restrictions ¶ Due to Arizona sales tax restrictions class groups are restricted to the Ocelote cluster and cannot use Puma or El Gato. To submit standard jobs on Ocelote, students will use the class group's name for the --account Slurm directive. For example: ``` SBATCH --account=hpc101 SBATCH --partition=standard ``` Class group members may also use the Windfall partition on Ocelote. Creating a Class Group ¶ Log into your user portal , navigate to the Groups tab, and select the Add New Group dropdown option at the top of the page.There will be an option to specify your Group Type . Choose classGroup from the dropdown menu. Then under Group Name enter the name of your group. Finally, click to complete the process. Once this process is complete, you can find your group's dropdown under the Groups tab. There you can add students either individually or in batch by uploading a CSV file with your student's NetIDs. You may also remove students from the group by clicking the \"REMOVE \" button. You can also delete the group itself by selecting \" REMOVE GROUP\". File Permissions and Storage ¶ Students in your class group will only be able to access files and directories owned by the class group. This means they will not be able to access files and directories owned by your standard research group. Running Jobs and Allocations ¶ Due to Arizona sales tax restrictions class groups may only use the Ocelote cluster. To submit standard jobs on Ocelote, students will use the class group's name for the --account Slurm directive. For example: ``` SBATCH --account=hpc101 SBATCH --partition=standard ``` Standard hours used on Ocelote are pulled from the same pool as your research group so make sure to plan accordingly. If a student runs the command va , they will see the class group as being nested under the total time allocated to your primary research group as well as any others you may have created. Students will not see the names of your other research groups if they run va unless they are members. ``` (ocelote) [faculty_netid@wentletrap ~]$ va Windfall: Unlimited PI: parent_000 Total time: 100000:00:00 Group: hpc101 Time used: 0:00:00 Time encumbered: 0:00:00 Group: faculty_netid Time used: 0:00:00 Time encumbered: 0:00:00 Total used: 0:00:00 Total encumbered: 0:00:00 Total remaining: 100000:00:00 ``` Delegating Group Management Rights ¶ Adding a Delegate ¶ PI's can delegate management rights to trusted group members. Delegates may create research and class groups, sponsor users, remove users, and request and manage storage offerings on behalf of their faculty sponsor. To add a group member as a delegate, the PI can click the Manage Delegates link on the home page of the user portal . In the Manage Delegates window that appears, select Add a delegate , enter your group member's NetID, and click . Instructions for Delegates ¶ Once a group member has been added as a delegate, they can log into the user portal, then select Switch User from the Home tab. In the next window, they can enter their PI's NetID under Switch user form --> UA NetID , and click . They should see Current effective user change from their own NetID to their PI's NetID. This will allow them to perform functions on their PI's behalf. They may switch back to their own account at any time by following the same process, entering their own NetID in the form instead of their PI's. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/registration_and_access/group_management/",
      "title": "Group Management - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "2463a67f-1999-48a1-ade9-e66123b69fd7",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/registration_and_access/group_management/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Software ¶ As much fun as echoing \"Hello World\" is, it's hard to perform analyses without software. We've seen that software packages are not available on the login nodes, but now that we're interactively connected to a compute node, we can view and use what's available. Since the HPC is a shared system, we need to organize our installed packages in a way that's helpful for everyone. There are three main categories of software on HPC: System Software ¶ System software represents executables pre-installed on the system, and are available everywhere. This includes basic bash commands, as well as tools developed by our team, like va and nodes-busy . Modules ¶ Modules are software packages that are only accessible on compute nodes. Modules make it easy to load and unload software from your environment and allow hundreds of packages to be available on the same system without dependency or versioning conflicts. Specifying module versions It's always good practice to specify which software version you need when loading a module to ensure a stable environment. You can view and load software modules using the commands module avail and module load , respectively. For example: ``` [netid@gpu66 ~]$ module avail python ------------------------------------------------- /opt/ohpc/pub/modulefiles -------------------------------------------------- python/3.6/3.6.5 python/3.8/3.8.2 (D) python/3.8/3.8.12 python/3.9/3.9.10 [netid@gpu66 ~]$ module load python/3.9 [netid@gpu66 ~]$ python3 --version Python 3.9.10 ``` Try running module avail without specifying any arguments. You'll notice we have a lot available. These software packages can be used both interactively for shorter jobs and testing, or in your batch scripts. If you have software that you would like installed as a module, please refer to our software policies to see if it's a good candidate. Personal Software ¶ Installing software packages locally into your own directories is encouraged. This means you can download, install, manage, and customize software from the internet (e.g. Pip, CRAN, GitHub) without waiting for someone else to install it on HPC for you. Software should be compiled on a compute node so that the load doesn't strain the login nodes and to ensure you have access to the newest system software and compilers. If you would like to install your own software, please refer to our guidelines in User Installations which also has some handy tips and tricks if you've never installed software locally (i.e., not in a root-owned location) before. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/quick_start/software/",
      "title": "Software - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "2a02233f-aa6e-4cb8-8912-98e5abf28447",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/quick_start/software/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Storage and Transfers ¶ When you first access a login node, you're located in your home directory. This is a space with a 50 GB limit and is accessible to only you. The files you store here are housed on a large storage array and are accessible anywhere you are on the system except the bastion host. Do not overfill your home directory Once your data reaches the 50 GB limit you will experience problems, like the inability to log into Open OnDemand because the session file cannot be created. And your jobs may fail with unexpected errors depending on how your software reacts to no more space for writing output. To store your files in your home, you will need to transfer them to the system. Small files can most easily be transferred to/from HPC using our web interface Open OnDemand . In the upper-left you'll see a dropdown called Files where you can select Home Directory. On the following page, select \"Upload\" to open a window where you can drag/drop files. For larger files, we have a designated Data Transfer Node (DTN). Comprehensive instructions for alternative methods for file transfers can be found on our data transfer page . With larger files comes the need for more storage. If you find your home is insufficient to store your data, group allocations are available. See our storage documentation for details on options that are available. Now that we're on the login nodes and know where our files are, it's time to access a compute node. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/quick_start/storage_and_transfers/",
      "title": "Storage and Transfers - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "293dd693-40ec-45b2-afc4-fc7a2c6c303f",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/quick_start/storage_and_transfers/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Logging In ¶ System Access ¶ Account creation is necessary to log in If you have not yet done so, you will need to register for an account to log in. See our registration documentation for steps. Login issues If you experience any issues during the login process, see our FAQs for common problems . Do not run computations on the login nodes. See Running Jobs for detailed instructions on the proper way to run computationally intensive tasks. Once you've successfully registered for an HPC account, you're ready to log in. There are two main methods to access the HPC system Open OnDemand This is a browser-based application that provides users with a graphical interface to access the HPC filesystem, run software that requires a graphical component, or access an interactive desktop environment. The login portal for Open OnDemand uses the familiar UArizona WebAuth login screen. HPC accounts are tied to university accounts, so use your standard NetID and password (i.e. the one used for your email). Terminal The terminal is a text-based command interpreter provided by the operating system on your local machine. Mac and Linux users can access the \"Secure SHell\" (SSH) command by default, and Windows users will either have to use the Linux subsystem for Windows , or a program called PuTTY . Using these tools, users can access a command-line environment on the HPC, which can be used to manage files, write code, install software, and submit jobs. See our Bash Cheat Sheet for an overview of common commands. System Layout ¶ The inner workings of HPC systems may be somewhat obscured to new users. In this section, we'll give you an idea of how the system is laid out so you understand exactly where you are at each stage of the login process and what activities are performed where. The bastion host ¶ In another browser window, open our instructions on logging in from the command line . Start by following the first step shown that's specific to your operating system. Stop when your terminal displays ``` Success. Logging you in... Last login: This is a bastion host used to access the rest of the RT/HPC environment. Type \"shell\" to access the job submission hosts for all environments ``` If all has gone well, you are now connected to what is known as the bastion host. The bastion host is the first computer you land on when you log in using the hostname hpc.arizona.edu . This machine is only used to validate your credentials and provide a gateway to the rest of the HPC environment. It is not used for storing files and has no software installed so no computational work is done at this stage. As a test, try running the command hostname : ``` [user@gatekeeper 14:50:49 ~]$ hostname gatekeeper.hpc.arizona.edu ``` The output shows gatekeeper which is the name of this node and is how you can tell you're connected to the bastion. Next, to advance from the bastion host, type the command shell . The login nodes ¶ After you type shell on the bastion host, you're connected to a computer called a login node. We have two of these available and you will be assigned one at random. If you run the hostname command as you did on the bastion host, you should see either wentletrap or junonia . A login node is a shared workspace with minimal computational capabilities and very little software installed. This is not the place where computational work is done so users should not run their analyses, compile their software, or perform computationally intensive work in this location. Instead, the login nodes are meant for activities such as managing files, writing scripts, submitting and monitoring jobs, and viewing system resources. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/quick_start/logging_in/",
      "title": "Logging In - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "556ed368-c9e3-4d5c-9cf5-bbe7a5ab88f4",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/quick_start/logging_in/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Account Creation ¶ Overview ¶ All UArizona Faculty, Staff, Students, and Affiliates are eligible for HPC accounts free of cost. If you are considered to be a PI in the University's employee system (EDS) you may establish a PI account. PI Accounts are self-sponsored, manage their own HPC group(s), and receive their own time and storage allocations to be shared among group members. Staff, students and affiliates must be sponsored by a PI at the PI's discretion to obtain HPC access. Users may be a member of more than one HPC group. How to Register ¶ The process of registering for an HPC account varies depending on your affiliation with the university. Take a look at the list below to determine how you should register: I'm a faculty member/principal investigator (PI) If you are a research faculty member or principal investigator, you can sponsor yourself for access through our user portal. Step 1 : Visit https://portal.hpc.arizona.edu/ . This will automatically create an HPC account for you. Step 2 : Go to https://portal.hpc.arizona.edu/portal/sendlink.php and click the link on the left-hand side as shown below This will automatically redirect you back to the user portal, create a research group for you, and add you as a member. You can check your group and membership in the portal by going to the Groups tab and clicking your group's dropdown menu to view its members. For more information on how you can manage your HPC group, see our Group Management documentation. I'm a student, postdoc, staff member, or Designated Campus Colleague If you are affiliated with the University of Arizona but are not faculty, you will need to request sponsorship from a faculty member. This can be done through our web portal. Step 1 : Create an HPC account by navigating to https://portal.hpc.arizona.edu/ . Step 2 : Request sponsorship from a UArizona faculty member. Note: Your faculty sponsor will need their own HPC account before they are able to sponsor others. To request sponsorship, navigate to https://portal.hpc.arizona.edu/portal/sendlink.php . On the right-hand side, enter your sponsor's email address and click send. Your sponsor will then receive an email with a link used to authorize your account. Once they confirm your request, you will receive an email with instructions for accessing the HPC systems. Note: it may take up to 15 minutes after approval to receive a confirmation email and for your account to officially be activated. If you do not receive an email verification, you should contact your sponsor and confirm receipt and approval of the HPC account request. If your account has been approved but you have not received verification, you should contact HPC consulting and provide your NetID, your name, and the email address of your sponsor. I'm not affiliated with the university HPC systems are restricted to those with valid university credentials and are not available for general public use. However, if you are not officially affiliated with the university but are actively collaborating with university members, you may register for Designated Campus Colleague (DCC) status . This is done through human resources and provides collaborators with active UArizona credentials. Once your DCC status is approved, you may request sponsorship from a university faculty member (see the section above). Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/registration_and_access/account_creation/",
      "title": "Account Creation - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "59e65453-4293-4e51-b87a-9194b936da7c",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/registration_and_access/account_creation/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Special Projects ¶ Overview ¶ The Research Computing Governance Committee (RCGC) has approved support for \"Special Projects\" that use more than the standard allocation of hours. When a special project request is granted, an additional allocation of standard hours is provided for a limited period of time. There is not a specific definition of what comprises a project, but it is often support for publication or grant deadlines, or graduation dates. Each request is considered on a case by case basis. Authorized requestors ¶ Project requests must be submitted by a Principal Investigator (PI); partly because the additional time granted will go to the allocation of the PI. Guidelines ¶ All special project allocations are temporary. Special project allocations cannot be granted if they will impact the community of users with a standard allocation. PIs can only be granted a special project once within a 12 month period (starting from the ending period of the last special project). PI groups that routinely need more standard computing hours should supplement their needs in other ways (e.g. buy-in, accessing national/international computing resources, etc). Our research facilitators can help UArizona PIs access national-scale computing centers (e.g. ACCESS, TACC, Open Science Grid, etc). Categories ¶ | Size | Requirements | | --- | --- | | 10,000 hours per month or less | The majority of requests fall into this category. These requests provide a general statement of the need and are one month or less in duration. These requests are occasional and they can be granted automatically by UITS staff with minimal impact to standard queue usage. This will be done at the discretion of the Research Technologies staff. | | 10,000 to 100,000 hours per month | These requests must provide a defined number of hours and not to exceed 3 months. A defined statement of need (e.g. publication deadline) will be provided. These requests are occasional, and can be granted by Research Technologies staff after considering the researcher’s need, alternative ways to solve those needs, and assessing that there is no overall impact to the system. | | Greater than 100,000 hours per month | These requests must provide a defined number of hours, defined duration not to exceed 6 months, and a defined statement of need (e.g. publication deadline). These requests required PIs to report back the results of their calculations (publications, conference presentations, etc) for potential inclusion in our online documentation. Benchmarking, profiling, or assessment of analyses run should also be provided to help the UArizona HPC understand how the additional computing time was used and ways that need can be met in the future without a special project. These requests will be forwarded to the HPC policies subcommittee of RCGC and require committee approval before being granted. | Submitting a Request ¶ Requests are submitted via a web form in the HPC user portal under Support Requests . In the form that opens, Please include: Number of additional standard hours needed for the special project. Duration of the Special Project in months. Reason for the temporary increase. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/policies/special_projects/",
      "title": "Special Projects - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "6ba69937-4334-4ee0-8b63-f0c545d3313a",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/policies/special_projects/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content What is HPC? ¶ Keywords in bold Important concepts are emphasized in bold in the text below. We will explore these keywords in depth in the upcoming HPC Quick Start sections. If you've never used an HPC system before, you may be wondering what one is and why you'd want to use it. This section is designed to give you a big picture understanding of what these systems offer and how they may benefit your research. Introduction to HPC ¶ What is HPC? ¶ HPC stands for High Performance Computing and is synonymous with the more colloquial term Supercomputer . A supercomputer is a collection, or cluster, of a large number of regular computers (referred to as compute nodes ) connected over a network. Each of the computers is like a local workstation though typically much more capable. For example, a standard laptop might have 4 CPUs and 8 GB of RAM. Compare this with a standard compute node on one of our clusters, Puma, which has a whopping 94 CPUs and 470 GB of RAM. Shared Resource Model ¶ Another main difference between a supercomputer and a personal workstation is that the supercomputer is a shared resource . This means there may be hundreds or even thousands of simultaneous users. Each of these users connects to HPC from their own local workstation and can run work on one or more of HPC's compute nodes. You can imagine with this shared resource model, however, that without some sort of coordination, managing which users get what resources turns into a major logistical challenge. That's why supercomputers use job schedulers . Role of Job Schedulers ¶ Batch Jobs Documentation More comprehensive information on using job schedulers and running batch jobs can be found in the Running Jobs section. A job scheduler is software used to coordinate user jobs. In our case, we use a scheduler called Slurm . You can use it by writing a batch script that requests compute resources (e.g., CPUs, RAM, GPUs) and includes instructions for running your code. You submit this script to the job scheduler which then goes and finds available resources on the supercomputer for your job. When the resources become available, it initiates the commands included in your batch script, and outputs the results to a text file. Benefits of HPC ¶ Scaling Up and Scaling Out ¶ Supercomputers provide opportunities for data storage and parallel processing that far surpass what is capable in a standard workstation. These systems provide researchers with the ability to scale up or scale out their work. Increasing the data throughput of a single job is known as scaling up . This may mean moving from a 500 GB database on a workstation to a 5 TB database on the HPC, or raising the resolution of your simulation by a factor of 10 or 100. Other types of analyses may benefit from an increased number of jobs, such performing parameter sweeps, running Monte Carlo simulations, or performing molecular dynamics simulations. Local machines are limited by the number of cores accessible to them, decreasing the number of simultaneous computations as compared to an HPC. An increase in the number of CPUs used during analysis is known as scaling out your work. Workflow Automation ¶ Automation is another feature of HPC systems that allows users to schedule jobs ahead of time, and for those jobs to be run without supervision. Managing a workstation or keeping an SSH terminal active while scripts are running can lead to major complications when running extended analyses. Batch scripts allow a prewritten set of instructions to be executed when the scheduler determines that sufficient resources will be available. This allows for jobs with extended completion times to be run for up to 10 days (a limit imposed by the scheduler). Real-time output is saved to a text file, allowing you to check the progress of the job. Checkpointing is recommended for jobs that require longer than 10 days. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/quick_start/what_is_hpc/",
      "title": "What Is HPC? - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "707230aa-4a73-4e31-a7cb-4ec63c8d1179",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/quick_start/what_is_hpc/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Acceptable Use ¶ High Performance Computing (HPC) facility users are responsible for complying with all University policies . The supercomputers represent a unique resource for the campus community. These computers have special characteristics that are not found, or are of limited availability, on other central computers, including parallel processing, large memory, and a Linux operating system. The UArizona HPC resources require close supervision by those charged with management of these resources. Controlled Data ¶ UArizona HPC does not provide support for any type of controlled data. No controlled data (HIPAA, EAR, FERPA, PII, CUI, ITAR, etc.) can be analyzed or stored on any HPC storage. For HIPAA data we maintain a separate cluster called Soteria. See Secure Services . Federal Regulations ¶ Equipment purchased exclusively for research purposes is exempt from Arizona State Sales Tax. See UA FSO statement of Research Equipment Tax Exemption . However, there are exceptions to this exemption that impact central, shared use research facilities. These exceptions include \"research in social sciences or psychology\"—see AZ ARS 42-5061 subsection B.15. Machinery or equipment used in research and development . For the purposes of this paragraph, \"research and development\" means basic and applied research in the sciences and engineering , and designing, developing or testing prototypes, processes or new products, including research and development of computer software that is embedded in or an integral part of the prototype or new product or that is required for machinery or equipment otherwise exempt under this section to function effectively. Research and development do not include manufacturing quality control, routine consumer product testing, market research, sales promotion, sales service, research in social sciences or psychology , computer software research that is not included in the definition of research and development, or other nontechnological activities or technical services. (emphasis added) In order to provide research computing resources to researchers from these exception areas, some of the research computing resources have been purchased with Arizona taxes included. The result is that there are resources available to all campus researchers, with the caveat that researchers in the social sciences, psychology and instructional projects areas are restricted to using resources that are purchased with taxes paid. Please contact our HPC consultants to learn about the resources that are available for social sciences, psychology and instructional purposes. Access for Research and Limited Access for Instruction ¶ As described in the 'Sales Tax Exemption' section above, most of the HPC systems are limited to research applications as defined in Section B-14 of ARS Statute 42-5061 by the Arizona Legislature. All users are expected to use these resources accordingly and to use other computing systems for non-research purposes. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/policies/acceptable_use/",
      "title": "Acceptable Use - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "7e3f6663-22d6-448d-bca8-9b302191ab92",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/policies/acceptable_use/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Loss of University Affiliation ¶ Affiliation Loss Policy ¶ When you lose affiliation with the university, such as through graduation or leaving a work position, access to HPC resources will be automatically terminated on the effective date listed in the University of Arizona Records Database. Unfortunately, we are unable to extend HPC access for those losing affiliation. However, you may regain access by registering as a Designated Campus Colleague (DCC) through Human Resources. Once approved, DCCs can request sponsorship from a university faculty member. Retirement and Emeritus Faculty ¶ HPC users who retire retain their university affiliation and continue to have access to HPC resources. However, emeritus faculty are not eligible for Principal Investigator (PI) status. To maintain access to compute and storage allocations, emeritus faculty must be sponsored by an active university PI. Data After Affiliation Loss ¶ Non-PI Users ¶ Home directory data are retained for 90 days following loss of affiliation. If university affiliation is reestablished and HPC access is restored within those 90 days, files will be accessible upon login. Data stored in shared locations such as /groups or /xdisk persist for as long as the storage location is available ( see below ) and are not deleted after affiliation loss. However, it should be noted that without access to HPC, data will not be directly downloadable by the user. Users will want to plan ahead of time and retrieve any data they need off the system before they lose access. Otherwise, users will need to coordinate with active group members who may retrieve their data on their behalf. If file permission changes are needed, contact our consultants and they can help. PIs ¶ Home directory data are retained for 90 days following loss of affiliation. If university affiliation is reestablished and HPC access is restored within those 90 days, files will be accessible upon login. A PI's communal /groups directory will be removed on the first day of denial of access. PI's should coordinate with their group members to ensure data are moved off the system prior to this to prevent data loss. The exception is if the faculty member is retiring with emeritus status or transitioning to DCC status, provided there is no gap in affiliation between their faculty status and the new role. In this case, they may retain their /groups directory. Similar to above, a PI's communal directory /xdisk will be removed on the first day of denial of access. The exception is if the faculty member is retiring with emeritus status or transitioning to DCC status, provided there is no gap in affiliation between their faculty status and the new role, then the xdisk will remain until its expiration date. It should be noted that because emeritus faculty and most DCC affiliates are not classified as PIs, once the xdisk expires, a new one cannot be requested. If the a communal directory has been deleted and data retrieval is needed, contact us immediately and we may be able to restore your files. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/policies/loss_of_university_affiliation/",
      "title": "Loss of University Affiliation - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "85d1f33f-aae6-4134-bf00-eed33138d111",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/policies/loss_of_university_affiliation/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/resources/compute_resources/",
      "title": "Compute Resources - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "5f3e5b5b-eb9d-497b-b25b-acc16f2a707f",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/resources/compute_resources/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content HPC Quick Start ¶ Overview ¶ Welcome to the University of Arizona's (UArizona) High Performance Computing (HPC)! This Quick Start Tutorial is the place to be if you are new to HPC, or simply wish to take a refresher of the basics. We will cover some concepts that are general to HPC, as well as those that are specific to the system here at UArizona. Getting Started Checklist ¶ Before beginning this tutorial, you'll want to ensure that you're familiarized with our system policies and have registered for an account. If you haven't done so yet, please take a moment to review the following pages before proceeding with the rest of the tutorial. Policies : Read up on our HPC guidelines. In particular: Acceptable use Acknowledgements What happens if you lose university affiliation Register for an account : To log into HPC, you'll need to have registered for an account. If you have not yet done so, see our registration documentation for steps. Note that the process varies based on your university affiliation. Learning Objectives ¶ By the end of this quick start, you should know: What HPC is How the UArizona HPC is structured How to log in What storage you have access to What time allocations are How to access computational resources How to access software Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/quick_start/overview/",
      "title": "Overview - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "9ef4549c-7025-4707-bc06-5e3f8cec6130",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/quick_start/overview/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Accessing Compute Nodes ¶ The Compute Nodes ¶ Unlike the bastion host and login nodes, there are many compute nodes and each has, as the name suggests, a large amount of computational resources available to run your work. For example, Puma standard nodes have 94 available CPUs and a whopping 470 GB of RAM. To get a sense of what the cluster looks like, try running the command nodes-busy . The output should look something like this: ``` ✚ Buy-in nodes. Only accept high_priority and windfall jobs (puma) [netid@wentletrap ~]$ nodes-busy ============================================================== ▒ System Status ▒ Wed Feb 14, 03:42:09 PM (MST) 2024 Standard Nodes r1u16n2 :[▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒] 100.0% r1u17n2 :[▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒] 100.0% r1u18n1 :[▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒] 100.0% r1u25n1 :[▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒] 100.0% r1u26n1 :[▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒] 100.0% r1u26n2 :[▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒] 100.0% r1u27n1 :[▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒▒] 100.0% ``` Each line shows one compute node on the cluster you're connected to and how busy it is running jobs. By default, when you first log in you're connected to the Puma cluster. This is the largest, newest, and generally provides the most in terms of computational resources. However, we have two other clusters available: Ocelote and El Gato, each with a good number of computational resources available and shorter wait times to access them. When you first connected to a login node in the previous section, your terminal should have displayed: ``` The default cluster for job submission is Puma Shortcut commands change the target cluster Puma: $ puma (puma) $ Ocelote: $ ocelote (ocelote) $ ElGato: $ elgato (elgato) $ ``` This shows you the various shortcuts you can use to connect to the different clusters. Try running the command elgato now. You should see a change in your terminal prompt to indicate that your cluster has changed. ``` (puma) [netid@wentletrap ~]$ elgato (elgato) [netid@wentletrap ~]$ ``` Not every cluster is the same Note that we are using El Gato in this tutorial. This is our smallest cluster and runs an older operating system than our newest and largest, Puma. This means workflows developed on one may not be compatible with the other. For information on the operating systems used by our clusters, see: Compute Resources . Job Charging ¶ Before we connect to a compute node, let's quickly cover how access is charged. Every HPC group gets a free allocation of CPU hours that they can spend every month to access compute resources. You can think of a CPU hour as a token to buy one CPU for one hour, so if you want to reserve 5 CPUs for 10 hours, this will charge 50 CPU hours to your account. You can see more detailed information on job queues, allocations, and job charging on our Time Allocations page which has a comprehensive breakdown. For this tutorial, we'll focus on the standard partition. This is a job queue and is one that consumes your standard allocation. To use this job queue, you'll need to know your account name. To check, use the command va which stands for \"view allocation\". The output will look something like: ``` (elgato) [netid@wentletrap ~]$ va Windfall: Unlimited PI: parent_974 Total time: 7000:00:00 Total used*: 1306:39:00 Total encumbered: 92:49:00 Total remaining: 5600:32:00 Group: group_name Time used: 862:08:00 Time encumbered: 92:49:00 *Usage includes all subgroups, some of which may not be displayed here ``` You should see a name next to the Group field (in the example above, this is group_name ). If you see multiple groups, then you are sponsored in multiple groups and can choose any one of your group names. Note the name of your account and hang onto it for the upcoming sections. Interactive Jobs ¶ Now, let's actually access a compute node. When you're connected to a login node, you can initiate a Slurm job to interactively connect to a compute node by using the command interactive . By default, this will give you one CPU for one hour. You can adjust this using the different flags available which are documented on our Interactive Jobs page. For now, we'll stick with the default resources. To access a session, run the following, substituting your own group name (that you found with va in the section above) for <group_name> : ``` interactive -a ``` For example, in my case: ``` (elgato) [netid@wentletrap ~]$ interactive -a hpcteam Run \"interactive -h for help customizing interactive use\" Submitting with /usr/local/bin/salloc --job-name=interactive --mem-per-cpu=4GB --nodes=1 --ntasks=1 --time=01:00:00 --account=hpcteam --partition=standard salloc: Granted job allocation 1800857 salloc: Nodes cpu39 are ready for job [netid@cpu39 ~]$ hostname cpu39.elgato.hpc.arizona.edu ``` You'll notice once your job starts that your command line prompt changes to display the name of the compute node. If you run hostname , this should match your command line prompt and show you the name of the compute node you're connected to. In my case, I'm connected to the El Gato compute node cpu39 . You'll also notice that your session has been assigned a job number (in the above, you can see this as Granted job allocation 1800857 ). A job number is assigned to every job on the system and is used to keep track of job statistics and metrics. Since you're in an interactive session, you now have exclusive access to the resources you've reserved which means you can do things like use software to develop, test, run, and debug your code. Interactive sessions are optimal for these sorts of actions. However, you might run into problems executing your analyses if: Your session times out due to inactivity Your internet connection gets disrupted Your computer gets closed or turned off You want to run more than one job at a time That's where batch jobs come in. Batch Jobs ¶ Detailed Intro to Batch Below is a brief summary of batch scripts and how to run them. For a more detailed walkthrough, see our Batch Jobs documentation. Batch jobs are the real workhorses of HPC. In contrast to interactive jobs, batch jobs are a way of submitting work to run on a compute node without the need for an active connection. This allows you to execute large jobs that may need a long time to run, or many (hundreds or even thousands) of jobs without the need to be present. Batch jobs are run using batch scripts. Batch scripts are just text files that act as blueprints that the scheduler uses to allocate resources and execute the terminal commands needed to run your analysis. Batch scripts have three sections: The \" shebang \" will always be the line #!/bin/bash . This tells the system to interpret your file as a bash script. Our HPC systems use bash for all our environments, so it should be used in your scripts to get the most consistent, predictable results. The directives section will have multiple lines, all of which start with #SBATCH . These lines are interpreted as Slurm directives by the scheduler and are how you request resources on the compute nodes, set your output filenames, set your job name, request emails, etc. The code section in your script is a set of bash commands that tells the system how to run your analyses. An example batch script might look like the following: ``` !/bin/bash -------------------- Directives Section -------------------- SBATCH --job-name=hello_world SBATCH --account= SBATCH --partition=standard SBATCH --nodes=1 SBATCH --ntasks=1 SBATCH --time=00:01:00 -------------------- Code Section -------------------- echo \"Hello world, I am running on compute node $HOSTNAME\" sleep only used for demonstration purposes sleep 30 ``` In the script above, we're requesting one minute of runtime ( --time=00:01:00 ), one CPU ( --ntasks=1 ), one physical computer ( --nodes=1 ), and are using the standard partition. More detailed information on what each of these directives mean plus many others can be found in our Batch Directives documentation. Try creating a file on HPC called hello_world.slurm and add the contents above, replacing <your_group> with your own group's name. Not sure how to create a text file from the command line? Try nano hello_world.slurm , then simply enter your text. To save and exit, use Ctrl + X , select Y to save, and Enter to complete the process. Once you have your text file, you can submit your job by using the command sbatch followed by your script's name. This will return a job ID (just like your interactive job). For example: ``` [netid@cpu39 ~]$ sbatch hello_world.slurm Submitted batch job 1940917 ``` This sends your script to the scheduler, which puts your job in queue. Once the resources you have requested become available, your job automatically begins running. The time your job spends in queue depends on many factors, including the scale of your resource request and the overall system usage. To check on jobs you have submitted, use the command squeue --job=<your_jobid> . For example: ``` [netid@cpu39 ~]$ squeue --job 1940917 JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 1940917 standard hello_wo netid R 0:08 1 cpu37 ``` After the scheduler has received your request, the rest happens automatically. This means you can close your SSH connection, or even turn off your personal computer and walk away without interrupting your jobs. Submitting from login and compute nodes You can submit your jobs either from the login nodes or from compute nodes. If you submit from an interactive session, you can terminate your interactive job and it will not affect the batch job you've submitted. Once your job starts, you should see an output text file with the name slurm-<your_job_id>.out . This will contain any text that would have been printed to the terminal if you had run your work interactively. This text file is updated in real time allowing you to monitor your job's progress as it runs. Checking the output of the script we just ran, you should see something that looks like the following: ``` [netid@cpu39 ~]$ cat slurm-1940917.out Hello world, I am running on compute node cpu37 ``` That's it! You just ran your first batch job. As a next step, we'll cover how you can access software to run your analyses, allowing you to do more in your jobs than just run bash commands. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/quick_start/accessing_compute/",
      "title": "Accessing Compute Nodes - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "ae1bf35b-85f6-45f6-9b0b-5ab078694246",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/quick_start/accessing_compute/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Time Allocations ¶ Do not run computations on the login nodes. CPU time allocations do not apply to login nodes. See Running Jobs for detailed instructions on the proper way to run computationally intensive tasks. Group Allocations ¶ All University of Arizona Principal Investigators (PIs; typically faculty) that register for access to UArizona High Performance Computing (HPC) services receive free standard allocations on the HPC clusters which are shared among all members of their team and refreshed on a monthly basis. All PIs receive a standard allocation in addition to the windfall partition. A breakdown of the allocations available on the system and their usage is shown below. High Priority Please note that the High Priority partition is only available to PI groups who participated in the buy-in process for Puma. PIs will be notified when another buy-in session is available Qualified Hours Qualified Hours are only available to groups which have been awarded a special project. See Policies for information on how to apply. Standard Windfall High Priority Qualified Every group receives a free allocation of standard hours that refreshes on the first day of each month. | | Puma | Ocelote | El Gato | | --- | --- | --- | --- | | Standard CPU Hours | 150,000 | 100,000 | 7,000 | In batch jobs, standard hours can be used to request resources on CPU-only nodes with the directives ``` SBATCH --account= SBATCH --partition=standard ``` To request GPU resources using standard hours: ``` SBATCH --account= SBATCH --partition=gpu_standard SBATCH --gres=gpu: ``` Windfall is a partition available to jobs that enables them to run without consuming your allocation , but it also reduces their priority . This means windfall jobs are slower to start than other partitions. In addition to lower priority, windfall jobs are preemptible, meaning standard and high-priority jobs can interrupt a running windfall job, effectively placing it back in the queue. The purpose of windfall is to ensure that the clusters are busy at all times, and to allow researchers additional compute while increasing the efficiency of the system. The --account flag should be omitted when using the Windfall partition. In batch jobs, the windfall partition can be used to request resources on CPU-only nodes with the directive: ``` SBATCH --partition=windfall ``` To request GPU resources: ``` SBATCH --partition=gpu_windfall SBATCH --gres=gpu: ``` High priority allocations provide access to an additional pool of purchased compute nodes and increase the priority of jobs such that they start faster than standard jobs. Please check with your PI to ensure that your group has access before including these directives in your jobs. In batch jobs, high priority hours can be used to request resources on CPU-only nodes with the directives: ``` SBATCH --account= SBATCH --partition=high_priority SBATCH --qos=user_qos_ ``` To request GPU resources with high priority hours: ``` SBATCH --account= SBATCH --partition=gpu_high_priority SBATCH --qos=user_qos_ SBATCH --gres=gpu: ``` Groups with an upcoming deadline (e.g., conference, paper submission, graduation) are eligible to apply for a Special Project allocation once per year. Special projects provide an additional pool of standard hours, known as \"qualified hours\" to the group for a limited amount of time. In batch jobs, qualified hours can be used to request resources on CPU-only nodes with the directive: ``` SBATCH --account= SBATCH --partition=standard SBATCH --qos=qual_qos_ ``` To request GPU resources with qualified hours: ``` SBATCH --account= SBATCH --partition=gpu_standard SBATCH --qos=qual_qos_ SBATCH --gres=gpu: ``` For more information on batch directives and --gres options, see our batch directives documentation. See: interactive jobs , batch jobs , or Open OnDemand for more information on the specific syntax for using hours in different jobs. How Allocations are Charged ¶ The number of CPU hours a job consumes is determined by the number of CPUs it is allocated multiplied by its requested walltime . When a job is submitted, the CPU hours it requires are automatically deducted from the account. If the job ends early, the unused hours are automatically refunded. For example, a job requesting 50 CPUs for 10 hours will be charged 500 CPU hours. When the job is submitted, all 500 CPU hours are deducted from the user's account. However, if the job only runs for 8 hours before completing, the unused 100 CPU hours would be refunded. This accounting is the same regardless of which type of node you request. Standard, GPU, and high memory nodes are all charged using the same model and use the same allocation pool. Charging discrepancies If you find you are being charged for more CPUs that you are specifying in your submission script, it may be an issue with your job's memory request . Allocations are refreshed on the first day of each month. Unused hours from the previous month do not roll over. How to Find Your Remaining Allocation ¶ To view your allocation's used, unused, and encumbered hours , use the command va (short for \"view allocation\") in a terminal. For example: ``` (elgato) [user@gpu5 ~]$ va Windfall: Unlimited PI: parent_974 Total time: 7000:00:00 Total used*: 1306:39:00 Total encumbered: 92:49:00 Total remaining: 5600:32:00 Group: group1 Time used: 862:08:00 Time encumbered: 92:49:00 Group: group2 Time used: 0:00:00 Time encumbered: 0:00:00 *Usage includes all subgroups, some of which may not be displayed here ``` Note that if your PI has created multiple groups , each of these groups consumes CPU hours from the same allocation . You can see the total allocation pool and the usage for each group you are a member of in the output of va . Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/resources/allocations/",
      "title": "Allocations - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "b547f845-b7e6-4a9e-bf19-d91a5b836fc5",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/resources/allocations/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Access ¶ Eligibility ¶ The University of Arizona offers high performance computing resources to all active faculty, staff, students, and affiliates free of cost . Users are required to have active university credentials to gain access to all HPC services. If you are collaborating with university members but are not actively affiliated with the university, you may apply for Designated Campus Colleague (DCC) status through HR. DCC status grants non-affiliated individuals with active university credentials required for accessing university services. Email List Registration ¶ All HPC account owners and sponsors are required to be members of the HPC-Announce email list and will automatically be added as members during the account creation process. This email list is used to send HPC system-related updates and notifications. Deleting your HPC account will automatically remove you from the listserv. Registration Process ¶ Principal Investigators (PIs) PIs (typically faculty members) can sponsor themselves for access by following the instructions on our Account Creation page. Once a PI has successfully set up their group, they may sponsor other university members for access. Non-PIs Non-PI university members may request sponsorship from PIs with active HPC accounts to gain access by following the instructions on our Account Creation page. Loss of University Affiliation ¶ Users who lose affiliation with the university (e.g., graduating or leaving their position) will lose access to HPC. They may apply for DCC status to reinstate their credentials if HPC access is needed. For more information, see our policy page on losing university affiliation . Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/policies/access/",
      "title": "Access - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "b8e60bca-26c4-4880-a44b-499dfb6f30a9",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/policies/access/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Research Data Center ¶ Central Computing Facilities ¶ The University of Arizona (UArizona) has two data center facilities available to assist researchers on campus: Research Data Center (RDC): 1200 ft2 raised floor data center designed for water-cooled racks dedicated to centrally managed research computing systems Co-location Data Center: 1900 ft2 of raised floor data center space for air-cooled research co-located equipment These campus data centers are managed by the UArizona’s central computing organization, University Information Technology Services (UITS). Other than installation costs no bandwidth or other recurring charges will be levied for co-location of research systems in these facilities. Power and Cooling ¶ The UITS data centers are both located in the Computer Center with 1192 kW of battery backup and a 1750 kW generator for backup power. Cooling in the RDC is both in-rack cooling with chilled water heat exchangers and Computer Room Air Conditioning (CRAC) units. The co-location Data Center is cooled with chilled water CRAC units and dual cool CRACs. Both data centers are equipped with 18” raised floors that allow for full coverage of cooling to all the equipment, and leak detection systems in the subfloor. Fire Suppression ¶ The fire suppression system is a multi-tiered defense with clean agent compressed gas, dry pipe pre-action sprinkler and EPO (Emergency Power Off) systems zoned to deploy in affected areas. For prevention storage of combustible materials such as cardboard, flammable liquids and other hazardous materials is prohibited within the data centers. Security ¶ UITS data centers have badge swipe access with two-factor authentication and video surveillance in data center and surrounding building. The data centers are monitored by a co-located 24/7 Operations and dedicated infrastructure team. With automated environmental and system monitoring to assist with issue triaging and escalation. All personnel with swipe access to the data centers have undergone background checks and are required to be US Citizens. Network and Connectivity ¶ In addition to direct connections to commodity Internet carriers, the UArizona connection to Internet2 is through the Sun Corridor Network – an Arizona regional network established through a collaborative effort sponsored by the Arizona Board of Regents’ (ABOR) three state universities – Arizona State University (ASU), Northern Arizona University (NAU), and the University of Arizona (UArizona). The Sun Corridor Network provides advanced networking services beyond those available from the individual Arizona Universities and builds an environment essential to leading-edge education, research, and the sharing of digital communications resources, network services, and applications among eligible members. The UArizona manages and operates the Sun Corridor Network. The current connection from UArizona to Sun Corridor is dual 10 Gb, while Sun Corridor is connected to Internet2 via dual 100 Gb connections in Tucson and Phoenix. Network traffic to Internet2 is automatically routed via the Internet2 infrastructure; no action or configuration by the user is required to take advantage of Internet2 connectivity. The UArizona’s Research Data Center has 40 Gb/s connections to the UArizona core with all the servers connected by 1 Gb/s or 10 Gb/s connections. In-rack switching is enabled with Cisco FEX switches used in a top of rack configuration in both data centers with servers connected to two different switches for (N + 1) redundancy. In addition to direct connectivity to the campus network at the building level, researchers have an opportunity to use a Science DMZ for fast and high volume data transfers to outside collaborating institutions. The Science DMZ is deployed at the University of Arizona network perimeter, outside border firewalls, and is directly connected to Sun Corridor via 10 Gb link. It is secured via static access lists deployed at the Sun Corridor router without impact to performance. There are two high-performance Data Transfer Nodes (DTNs) deployed in the Science DMZ. DTN’s are dedicated servers with hardware and operating system optimized for high speed transfer. We recommend using these DTNs for large transfers Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/resources/data_center/",
      "title": "Research Data Center - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "ba8db7dc-72b3-4bb0-8443-c4f41a6ea1d9",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/resources/data_center/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Committees ¶ The Research Computing Guidance Committee (RCGC) is a cross-departmental group of researchers and IT professionals at the University of Arizona with oversight of central research computing resources. The charge to the committee is to design and implement the policies and procedures; the oversight of the operations; and promotion and recommendations for the centrally funded and administered research computing resources. The policies and procedures will be monitored and updated by this steering committee and related task forces created by this committee. The resources will be administered, maintained, and supported by UITS Research Computing, Systems, and Operations. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/policies/committees/",
      "title": "Committees - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "bb784107-729f-4f6c-90ca-c28122723c87",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/policies/committees/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content System Access ¶ Overview ¶ Logging into the HPC supercomputers starts with your UArizona NetID and password with two-factor authentication enabled. This section is intended to provide you with instructions on getting terminal access to the system from your specific OS, how to log into the system from our web interface (Open OnDemand), how to set up X11 (image) forwarding, and how to configure your account to allow for a password-less login with SSH keys. If you experience any problems, refer to our FAQ page which provides some solutions to common problems. Web Access ¶ Open OnDemand Browser Terminal Virtual Desktop Users can gain command line access to HPC through our OOD web interface as an alternative to using a local SSH Client. To use this interface: Log into Open OnDemand Go to the dropdown menu at the top of the screen and select Clusters Click >_Shell Access This will put you on the command line on one of the login nodes where you may perform regular housekeeping work, submit jobs, or request an interactive session. By default, you will automatically be connected to Puma. To navigate to a different cluster, use the displayed shortcuts. Users may also interact with a cluster using a virtual desktop interface. To do this: Log into Open OnDemand and, under My Interactive Sessions, select Interactive Desktop under Desktops on the left-hand side of the page. A form will appear where you will select the target cluster, enter the amount of time you'd like to be allotted (in hours), the number of cores you need, your PI Group (if you are unsure what your group name is, you can check in https://portal.hpc.arizona.edu/portal/ ), and the queue. Once you've filled in your request, click Launch. A window will appear with the status of your request. It will start in a Pending state and will switch to Running when your desktop session is ready. Click Launch Interactive Desktop to access your session. That's it! You can now use the cluster with a Desktop interface Command Line Access ¶ Tip Credentials : To log into HPC, you will need NetID+ enabled, an HPC account, and internet access. Because we require Duo-authentication to access the system, no VPN is required. Password Visibility : When entering your password in the terminal at the prompt, you will not see any characters appear on the screen while typing during this step. This is normal and everything is working as it should. Linux/Mac Windows Tip Mac systems provide a built-in SSH client, so there is no need to install any additional software. You will find the terminal application under Applications → Utilities → Terminal. Open the terminal and enter: ``` ssh @hpc.arizona.edu ``` where <netid> is your UArizona NetID. When you press enter, you will be prompted for your university password. After successfully entering your password, you will be prompted to Duo Authenticate. If everything is successful, you will be connected to the bastion host. Windows systems do not have any built-in support for using SSH, so you will have to download a software package to do so. There are several available for Windows workstations. Free SSH clients are available for download from the University of Arizona's Site License website . PuTTY MobaXterm PuTTY is the most popular open source SSH Windows client. To use it: download, install, and open the Putty client . Next, open a connection and enter hpc.arizona.edu under Host Name and press Open This will open a terminal. At the prompt, enter the following, replacing <netid> with your own NetID: ``` Login as: ``` You will then be prompted to Duo-Authenticate. If the process is successful, you will be connected to the bastion host. MobaXterm is another available SSH Windows client. To connect to HPC, download and install MobaXterm , open the software, select Session From there, select SSH and enter hpc.arizona.edu under Remote host . Next, select the box next to Specify username and enter your UArizona NetID. To connect, click OK at the bottom of the screen: This will open a terminal and will prompt you for your UArizona password. You will then need to Duo-authenticate. If everything is successful, you will be connected to the bastion host. Once you reach the bastion host, regardless of method, you should see the following: ``` Success. Logging you in... Last login: This is a bastion host used to access the rest of the RT/HPC environment. Type \"shell\" to access the job submission hosts for all environments ``` From there, type shell to connect to the login nodes that will provide access to our three clusters. On the login nodes, you should see: ``` The default cluster for job submission is Puma Shortcut commands change the target cluster Puma: $ puma (puma) $ Ocelote: $ ocelote (ocelote) $ ElGato: $ elgato (elgato) $ ``` X11 Forwarding ¶ X11 forwarding is a mechanism that allows a user to start up a remote application (e.g. VisIt or Matlab) and forward the application display to their local machine. The key to make forwarding work successfully is to include the -X flag at each login step. To check whether X11 forwarding is active, you may run the command: ``` echo $DISPLAY ``` If it comes back blank, X11 forwarding is not enabled. Mac/Linux Windows Tips Mac users will want to install the additional software package XQuartz onto their machines to use X11 forwarding with HPC. On a Mac, if you get a blank response to echo $DISPLAY , you might need this line in your ~/.ssh/config file: ForwardX11Trusted yes Be aware forwarding X traffic does not work with the DEPRECATED menu interface enabled. You should disable the menu option and use the hostname shortcuts instead. Start a terminal session and connect as you typically would with an additional flag -X in your ssh command. Once you're connected to the bastion host, enter the name of the cluster you want to access, including the additional -X flag again. An example of this process is provided below: ``` $ ssh -X netid@hpc.arizona.edu Password: Duo two-factor login for netid Enter a passcode or select one of the following options: Duo Push to XXX-XXX-8969 Phone call to XXX-XXX-8969 Phone call to XXX-XXX-0502 SMS passcodes to XXX-XXX-8969 Passcode or option (1-4): 1 Success. Logging you in... Last login: This is a bastion host used to access the rest of the RT/HPC environment. Type \"shell\" to access the job submission hosts for all environments [netid@gatekeeper ~]$ echo $DISPLAY localhost:13.0 [netid@gatekeeper ~]$ shell -X The default cluster for job submission is Puma Shortcut commands change the target cluster Ocelote: $ ocelote (ocelote) $ Puma: $ puma (puma) $ (puma)[netid@junonia ~]$ echo $DISPLAY localhost:18.0 ``` To use X11 forwarding on a Windows system, you will need to download an X11 display server such as Xming. PuTTY MobaXterm To enable X11 forwarding in PuTTY, go to SSH → X11 and select the box next to Enable X11 forwarding. Once you've connected to the bastion host, connect to the login nodes with the an additional flag -X : ``` shell -X ``` To enable X11 forwarding in MobaXterm, open a new session, select SSH , and open Advanced SSH settings . Select the option below called X11-Forwarding . Once you've connected to the bastion host, connect to the login nodes with the an additional flag -X : ``` shell -X ``` Once you're connected to the login nodes, you'll need to include an X11 forwarding flag when you start an interactive session . When using the interactive command, use the flag -x . When using salloc directly, use --x11 . Port Forwarding ¶ Port forwarding is a technique used to redirect network traffic from one network address and port number to another. In the context of HPC systems, port forwarding allows users to access remote resources or services that are not directly accessible due to network configurations. Common use cases for port forwarding include accessing remote desktops, running graphical applications, or accessing web-based interfaces of applications. Some examples of this might include using Jupyter Lab or a containerized RStudio instance. The steps to set up port forwarding are the following: Start a job This can either be done using an interactive session or an Open OnDemand job (e.g. an interactive desktop session). Once your job starts, make note of the node name. For example, in an interactive session, you can use the command hostname ``` (elgato) [user@wentletrap ~]$ interactive -a hpcteam -t 5:00:00 [user@cpu37 ~]$ hostname cpu37.elgato.hpc.arizona.edu ``` Connect to the HPC VPN The HPC VPN can be used with Cisco AnyConnect using vpn.hpc.arizona.edu . This is different from the standard university VPN and will allow you to connect directly to a compute node, bypassing the bastion and login nodes. Use the HPC VPN Note that it's always safer and more efficient to connect directly to your compute node rather than tunneling through the bastion and login nodes. If you tunnel through the bastion/login nodes, you may inadvertently use the same port as another user causing unwanted interference. Additionally, tunneling will result in reduced performance. SSH to your compute node Once you're connected to the HPC VPN, ssh into your compute node with the additional arguments -L <port>:localhost:<port> where <port> is the port you want to use. For example: ``` ssh -L 1234:localhost:1234 user@cpu37.elgato.hpc.arizona.edu ``` 4. Start your application and launch it in a web browser In your new terminal window, start your application, specifying the port number. For example: ``` [user@cpu37 ~]$ jupyter lab --port 1234 ``` Your application should give you a URL that you can then use to access your session in a local web browser. SSH Keys ¶ Why Use SSH Keys? ¶ The Bastion Host uses two-factor authentication and will, by default, prompt you for a password and 2nd factor when you attempt to log in. As an alternative, you can use PKI (Public Key Authentication). This means you will not have to provide a password or Duo-authenticate for any future sessions. In brief, you will need to create an SSH Key on your local workstation and copy the public key to the ~/.ssh/authorized_keys file in your HPC account on the bastion host. More detailed explanation & instructions below. Setting Up SSH Keys On Linux/Mac ¶ The proper use of SSH keys involves creating a public/private keypair, and configuring a couple of files on each system. Both Source (e.g. your laptop) and Destination (e.g. the HPC Bastion Host) systems need a directory in your home called .ssh . This is a hidden folder that will store the keypair and configuration files. Note that you will need to enter your password to access the system until this process is complete and all files are set up correctly. Important Note on Account Security Do not store a backup of these keys on any other system! If you lose the keys, you will still be access the HPC using your UA password. If a third party obtains your SSH key, they will gain access to your account. 1. Setting up on Source The following files need to be present in ~/.ssh on Source private key: id_rsa -- Do not share this with anybody! It is analogous to your password; anybody who has this file will gain access to your account. public key: id_rsa.pub -- Upload this onto any servers that you wish to automatically login to. It is recommended to use different keys for different servers. configuration file: config The keypair is generated on Source with the command ssh-keygen -t rsa . You will be prompted to enter a passphrase. This is optional but recommended. You may need to enter a name other than the default id_rsa if you already have a keypair with that name on your system, or if you wish to use mutliple SSH keys to access different servers. 2. Setting up on Destination In this case, we will be treating the Bastion Host as the Destination. This is necessary because it serves as the authentication host, meaning users are required to pass through it before accessing the rest of the HPC environment. Note that the Bastion Host has very limited storage space and a different file system than the main HPC. Do not put any files on the Bastion Host other than what is necessary to set up SSH Keys. Files that you place on the Bastion Host will not be present on the main HPC. You can read more about the system layout here . A file containing a list of accepted public keys called authorized_keys needs to be present in ~/.ssh on Destination . You will then need to copy the contents of id_rsa.pub from Source into this file. This can be done with the command ssh-copy-id netid@hpc.arizona.edu . If your computer does not support the this command, or if this process does not yield desired results, you will need to copy it manually: ``` scp ~/.ssh/id_rsa.pub netid@hpc.arizona.edu: ssh netid@hpc.arizona.edu mkdir -p ~/.ssh && cat ~/id_rsa.pub >> .ssh/authorized_keys && rm ~/id_rsa.pub ``` 3. Configuring the SSH Agent Sometimes the ssh agent does not associate the right key with the right server, and you may still have to enter your password. If this occurs, setting up a config file can correct the error. On Source , run touch ~/.ssh/config . Then, copy the following code block into the new file, making sure to replace <netid> with your correct UA net id. You can change the contents of Host to any name you like. Do not change HostName . Make sure IdentityFile matches the key you generated in step one, in particular if you gave it a different name. ``` Host uahpcbastion HostName hpc.arizona.edu User IdentityFile ~/.ssh/id_rsa ``` This will associate the identity file with the HPC server, and will also allow you to replace netid@hpc.arizona.edu with uahpcbastion in the command line, e.g. ssh uahpcbastion Now, logout and attempt to login to the server again. You should not be prompted for a password! 4. Direct Access to Login Nodes You can set up a proxy jump in order to access the login nodes without having to type \"shell\" from the Bastion Host. First, you will need to copy the contents of id_rsa.pub from Source into the ~/authorized_keys file on the main HPC filesystem in a similar manner to step 2. Then, put an empty line after the last entry in ~/.ssh/config and add the following contents, again making sure to replace <netid> with you correct UA net id. You may change the Host entry as you prefer, and make sure the name after ProxyJump matches the name you gave to the Bastion Host in the previous entry. ``` Host uahpclogin HostName shell.hpc.arizona.edu User IdentityFile ~/.ssh/id_rsa ProxyJump uahpcbastion ``` Now you should be able to run ssh uahpclogin from Source to directly access the login node. 5. Accessing the File Transfer Node SSH Keys can also be used to avoid entering a password and two-factor authentication when transferring files to or from the cluster via filexfer.hpc.arizona.edu . Put an empty line after the last entry in ~/.ssh/config on Source and add the following contents, again making sure to replace <netid> with you correct UA net id. ``` Host uahpcfxfr HostName filexfer.hpc.arizona.edu User IdentityFile ~/.ssh/id_rsa ``` You should now be able to use scp , sftp , and the like from your local computer without entering your password. You may also wish to access the file transfer node from the login node without entering your password, for example to copy data from /rental . In this case, you will need to perform steps 1-3 but treating Source as the login node and Destination as the file transfer node. It may be helpful to name this new key something to indicate its association with the file transfer node, for example fxfr and fxfr.pup . Since both Source and Destination share access to your home folder, your public and private keys will both be in the ~/.ssh folder on the HPC, as will the authorized_keys and config files. Make sure to create these and put the contents of fxfr.pub into authorized_keys . If multiple keys are being added to authorized_keys , make sure they are on separate lines. Then, add the following code block to ~/.ssh/config : ``` Host uahpcfxfr HostName filexfer.hpc.arizona.edu User IdentityFile ~/.ssh/fxfr ``` Now, you should be able to perform ssh , scp , sftp , and the like from the HPC login node to/from the HPC file transfer node without having to enter your password. For example: ``` ssh uahpcfxfr scp -r /rental/netid/data /xdisk/netid/project ``` Setting Up SSH Keys On Windows ¶ To set up SSH keys on Windows with the PuTTy client, refer to the official PuTTy documentation . To set up SSH keys on Windows for file transfers using WinSCP, refer to the official WinSCP documentation . If you are a Windows user and would like to set up SSH keys to access the file transfer node from a login node without entering your password, please read through the above section on setting up SSH Keys on Linux, since the HPC is a Linux system. Some information in steps 1-4 may be relevant, but you should not perform those actions on your local computer. Then, refer to step 5 for specific directions on setting this up. Do so from an active SSH session on an HPC login node. Learn More ¶ If you would like to learn more about SSH keys and more, please refer to this in-depth guide created by our friends at Digital Ocean. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/registration_and_access/system_access/",
      "title": "System Access - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "ce1d9b68-dbc3-4caa-a33a-3778ff429a07",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/registration_and_access/system_access/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Buy-in ¶ Overview ¶ The University of Arizona's High Performance Computing (HPC) clusters are servers (computing nodes) and associated high performance storage. There are additional nodes to meet specific needs like high amounts of memory or GPUs. All UArizona research faculty can sign up for free monthly allocation following these directions . For researchers who need compute resource beyond the free standard allocation, and who have funding available, we encourage 'buy-in' of additional compute nodes. Benefits of Buy-in ¶ | | | | --- | --- | | Dedicated Research Compute | Research groups can 'Buy-In' (add resources such as processors, memory, etc.) to the base HPC systems as funding becomes available. Researchers receive 100% of the CPU*hour time their purchases create as a monthly high-priority allocation. This time receives the highest priority queue on the HPC systems. | | Quality Environment | The Buy-In option allows research groups to take advantage of the central machine room space that is designed for maintaining high performance computing resources. The UITS Research Technologies group physically maintains the purchased nodes, applies updates and patches, monitors the systems for performance and security, and manages software. Additionally, Research Technologies staff is available for research support . In short, essentially all costs associated with maintaining compute resources are covered by UITS rather than individual researchers. | | Flexible Capacity | Buy-in research group members also benefit from their resources being integrated into a larger computing resource. This means the buy-in resources can be used in conjunction with the free allocation and resources provided to address computational projects that would be beyond the capacity of a group running an independent system alone. | | Shared Resource | The University research computing community as a whole benefits from buy-in expansions to the HPC systems. As mentioned above, researchers who buy-in receive 100% of the allocation of time for their purchase. However if the buy-in resources are not fully utilized, they are made available as windfall resources . This helps to ensure full use of all HPC resources and can be used to justify future purchases of computing resources. | | Cost Competitiveness | Lower costs included in the grant proposals (i.e. hardware only, no operations costs) and evidence of campus cost‐sharing give a positive advantage during funding agency review. | | Pricing | For the year following the award the UArizona HPC request for proposal (RFP) pricing is locked in and is often considerably less than the \"market price.\" | Buy-in Policies ¶ For Puma, the University of Arizona could only purchase whole chassis units from Penguin Computing. That is 4 CPU nodes (option 1D), 1 GPU node with 4 GPUs (option 2D), or 1 high memory nodes (option 3). Research Computing worked to match partial node buy-in requests to make full nodes. Monthly high priority time is calculated as: {Number of CPUs}×24{ hours/day}×365{ days/year}12{ months/year} Purchasing GPUs expands the limit the PI has on number of GPUs that can be used at any time. Buy-in high priority allocations will last the lifetime of the system. Puma was purchased in August 2020 and will be officially end-of-life August 2025. The HPC Buy-in program is not designed to replace or compete with the very large‐scale resources at national NSF and DOE facilities, e.g. ACCESS , the Open Science Grid, etc. National resources are available at no financial cost to most US-based researchers through competitive proposal processes. Please contact our consulting team if you are interested in applying for these resources. The HPC Buy-in program is designed to meet the needs of researchers with medium‐scale HPC requirements who want guaranteed, consistent access to compute resources. High-priority Allocation Policies ¶ Standard and high priority jobs will preempt windfall jobs when necessary. Standard jobs do not run on high priority nodes since standard jobs can not be preempted High priority jobs are run on both the buy-in nodes and the centrally-funded nodes. This is advantageous if there is a short-term project deadline. Compute Buy-in Details (Puma 2020) ¶ Hardware ¶ The buy-in process for Puma has ended. The community will be informed when the next purchase cycle is announced. | Buy-in Option | Technical Specs | | --- | --- | | CPU-Only Node Penguin Computing Altus XE2242 | There are 4 CPU nodes in an Altus XE2242 chassis Technical specs for 1 node of 4 in an Altus XE2242 chassis - 96 cores: Dual socket AMD EPYC 7642 CPU (2x48 cores, 2.4 GHz, 225 W) - 512 GB RAM, DDR4-3200MHz REG, ECC, 2Rx4 (16 x 32 GB) - 2 TB SSD local hard drive, 2.5”, NVMe, 4 Lane, 1 DWPD, 3D TLC | | GPU Node Penguin Computing Altus XE2214GT | GPU chassis have 4 GPUs in them Technical specs for the full XE2214GT chassis - 96 cores: Dual socket AMD EPYC 7642 CPU (2x48 cores, 2.3 GHz, 225 W) - 4 NVIDIA Tesla V100S-PCIe, 32 GB video memory, 5120 CUDA, 640 Tensor, 250 W - 512 GB RAM, DDR4-3200 MHz REG, ECC, 2Rx4 (16 x 32 GB) - 2 TB SSD local hard drive, NVMe, 4 Lane, 1 DWPD, 3D TLC | | High Memory Node Penguin Computing Altus XE1212 | - 96 cores: Dual socket AMD EPYC 7642 CPU (2x48 cores, 2.4 GHz, 225 W) - 3072 GB RAM, DDR4-2933 MHz LR, ECC, 4R (24 x 128 GB) - 2 TB SSD local hard drive, NVMe, 4 Lane, 1 DWPD, 3D TLC | Cost and Allocations ¶ Notice With V100S GPU's no longer available the pricing will be different. The locked in pricing expired February 28, 2022. | Option Number | CPU Cores | V100s GPU | RAM (GB) | Monthly High-priority Allocation | Cost | | --- | --- | --- | --- | --- | --- | | CPU-only Options | | | | | | | 1A - One CPU node | 96 | | 512 | 70,080 | $8,037.50 (expired) | | 1B - Two CPU nodes | 192 | | 512 | 140,160 | $16,075.00 (expired) | | 1C - Three CPU nodes | 288 | | 512 | 210,240 | $24,112.00 (expired) | | 1D - Full Altus XE2242 | 384 | | 512 | 280,320 | $32,150.00 (expired) | | GPU Node Options | | | | | | | 2A - 1/4 Altus XE2214GT | 24 | 1 | 512 | 17,520 | $8,523.75 (expired) | | 2B - 2/4 Altus XE2214GT | 48 | 2 | 512 | 35,040 | $17,047.50 (expired) | | 2C - 3/4 Altus XE2214GT | 72 | 3 | 512 | 52,560 | $25,571.25 (expired) | | 2D - Full Altus XE2214GT | 96 | 4 | 512 | 70,080 | $34,095.00 (expired) | | High Memory Node | | | | | | | 3 - Full Altus XE1212 | 96 | | 3072 | 70,080 | $42,230.00 (expired) | Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/policies/buy_in/",
      "title": "Buy-in - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "ce71e88a-12a2-4b7f-8eef-231a1bb68797",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/policies/buy_in/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Common Misconceptions ¶ Both experienced and novice users may benefit from reading through these common misconceptions. Do not run computations on the login nodes. See Running Jobs for detailed instructions on the proper way to run computationally intensive tasks. If I move my code to HPC, it will automatically run faster You might be surprised to learn that if you move code from a local computer to a supercomputer, it will not automatically run faster and may even run slower . This is because the power of a supercomputer comes from the volume of resources available (compute nodes, CPUs, GPUs, etc.) and not the clockspeed of the processors themselves. Performance boosts come from optimizing your code to make use of the additional processors available on HPC, a practice known as parallelization. Parallelization enables jobs to \"divide-and-conquer\" independent tasks within a process when multiple threads are available. In practice, this typically means running a job with multiple CPUs on the HPC. On your local machine, running apps like your web browser is natively parallelized, meaning you don't have to worry about having so many tabs open. However, on the HPC, parallelization must almost always be explicitly configured and called from your job. This process is highly software-dependent, so you'll want to research the proper method for running your program of choice in parallel. If I allocate more CPUs to my job, my software will use them Running a job with a large number of CPUs when the software has not been configured to use them is a waste of your allocation, your time, and community resources. Software needs to be designed to use multiple CPUs as part of its execution. You will need to ensure your software has the capability to make use of multiple CPUs for it to be able to take advantage of additional hardware. The job scheduler only provides the resources, the code itself is what needs to know how to make use of them. All nodes on a supercomputer are the same Navigating the HPC means being aware of the different types of nodes you can land on. For example, the login node is available to all users by default upon login, and is designed for managing and editing files. However, it is not designed to run production computations. Running jobs that are too computationally intensive on the login node can severely impact performance for other users. Such jobs will be noticed and stopped by the HPC systems team. Types of nodes on the UArizona HPC system include the Bastion Host, the Login Node, the Compute Nodes, and the Data Transfer Node. See Compute Resources for information on the compute hardware available. As a user I (am)(am not) allowed to install my own software Well, it depends. Users can create custom environments and install packages for languages like Python and R by using their built-in package managers. Users are even encouraged to download software from GitHub or other repositories and compile it themselves (provided it is done on a compute node). However, system-wide modules are generally taken care of by the HPC team. If you would like something to be installed as software available to all HPC users, you can make a request through ServiceNow . But, if you would like something to be installed for personal use, or use between members of your group, you are encouraged to install it in one of your shares on the HPC filesystem. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/quick_start/common_misconceptions/",
      "title": "Common Misconceptions - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "d70bb152-5ee5-477b-bcd6-743fb4d34590",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/quick_start/common_misconceptions/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Acknowledgements ¶ PIs should notify our HPC consultants about posters or other publications (published, accepted, submitted, or in preparation) that benefited from the use of UArizona High Performance Computing, Statistical Consulting, and/or Data & Visualization Consulting. These will be listed in the Collection of Published Results. Results Acknowledging the UArizona HPC Resources The suggested format to acknowledge University of Arizona High-Performance Computing in a paper, poster, or presentation is: This material is based upon High Performance Computing (HPC) resources supported by the University of Arizona TRIF, UITS, and Research, Innovation, and Impact (RII) and maintained by the UArizona Research Technologies department. Acknowledging Contributions from a UArizona Research Technologies Staff Member or Consultant If you wish to additionally acknowledge an individual who assisted you from University of Arizona High-Performance Computing, the suggested format is: We thank [consultant's name(s)] for [his/her/their] assistance with [describe tasks accomplished], which was made possible through University of Arizona Research Technologies Collaborative Support program. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/policies/acknowledgements/",
      "title": "Acknowledgements - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "ece357c1-b1a8-4f9d-ab77-f15a02f4be38",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/policies/acknowledgements/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Secure Services ¶ Research Technologies in partnership with the Data Science Institute is providing a secure research enclave that is HIPAA compliant called Soteria. In Greek mythology, Soteria (Greek: Σωτηρία) was the goddess or spirit (daimon) of safety and salvation, deliverance, and preservation from harm. Soteria uses the same Slurm scheduling system and software module interface as our main HPC computing clusters. For general information on HPC usage, the rest of our documentation site can be used. The information below covers everything specific to Soteria. Additional information can also be found at https://soteria.arizona.edu Prerequisites and Registration ¶ To gain access, you will need to submit a Soteria request form . Once your form has been reviewed and approved, you will receive an email with the subject UA Soteria Access Request Approved . This email will contain the next steps to take which are detailed below: Complete Required Training in Edge Learning The CRRSP (regulated research) team will register you for three required trainings listed below HIPAA Essentials Information Security: Insider Threat Awareness Information Security Awareness Certification You will receive instructions on accessing these courses in your confirmation email. Courses will also be findable here: https://uaccess.arizona.edu . Assignment to the Soteria VPN Once you have completed your required training, the CRRSP team will notify you via email when you have been assigned access to the Soteria VPN. This VPN is an important part of our HIPAA compliance and differentiates Soteria usage from the standard HPC clusters. Soteria access cannot be established when not connected to the VPN. For VPN access, use: vpn.arizona.edu/soteria . Additional Requirements The computer you will use to access Soteria services must meet the following requirements: 1. The Operating System and applications must be updated with the latest patches. 2. You must have a strong password to log into the computer (at least 8 characters and a mix of character types). 3. This must not be a shared computer with other users. 4. Up to date anti-virus software. Access ¶ VPN Required You must be connected to the Soteria VPN to access the system. Command Line Access ¶ Soteria command line access is available with SSH using the hostname shell.cougar.hpc.arizona.edu as shown below (replacing <your_netid> with your own NetID): ``` $ ssh @shell.cougar.hpc.arizona.edu Authorized uses only. All activity may be monitored and reported. Last login: Tue Nov 29 06:18:33 2022 from ans-02.hpc.arizona.edu Authorized uses only. All activity may be monitored and reported. netid@taub:~ $ ``` Taub is a login node and will provide the same functionality and have the same policies as the other HPC clusters. Modules are available on Soteria's compute nodes but not on the login node. The command interactive is available to request a session on a compute node and jobs may be submitted using the standard sbatch . More details on Slurm commands can be found in Running Jobs . Graphical Interface ¶ Similar to the other HPC clusters, we offer the service Open OnDemand to provide web browser access to Soteria. This can be used to navigate, view, and edit files as well as gain access to graphical applications. To access the Soteria-specific OOD service, open the following link in your favorite browser: https://ondemand-hipaa.hpc.arizona.edu The applications currently available are RStudio, Matlab and Python 3.9 (Jupyter). Available Compute Resources ¶ This small cluster has four standard compute nodes. Each has 94 cores and 470 GB memory available. The two GPU nodes have the same resources but there are also four V100 GPU's in each. You can use the Running Jobs documentation to learn how to use Slurm with these nodes. | Node Type | Node Names | | --- | --- | | Standard Nodes | r1u26n1,r1u27n1,r1u28n1,r1u29n1 | | GPU Nodes | r1u30n1,r1u32n1 | Compute Time Allocations ¶ All PI groups will receive a time allocation of 100,000 CPU hours per month. For general information on time allocations and charging, see our Allocations documentation Storage ¶ All users are granted a home directory. Additional communal space can be found in a /groups directory allocated to each PI. Your files can be accessed both on the HPC filexfer nodes as well as when connected to Soteria. A summary of Soteria's storage is listed below: | Storage Allocation | Availability | Quota | Location on Soteria | Location on filexfer | | --- | --- | --- | --- | --- | | Home | Allocated to each user | 50 GB | /home/u<xx>/<your_netid> | /hipaa/home/u<xx>/<your_netid> | | Groups | Allocated to each PI group | Unlimited | /groups/<pi_netid> | /hipaa/groups/<pi_netid> | Transferring Data ¶ Globus can be used for moving data in and out of the Soteria environment. For more information on using Globus, see our Globus documentation Soteria's endpoint is: UA HPC HIPAA Filesystems Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/resources/secure_hpc/",
      "title": "Secure Services - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "ef40e44d-9fe1-47de-ae9b-48a8c4552691",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/resources/secure_hpc/",
      "statusCode": 200
    }
  }
]