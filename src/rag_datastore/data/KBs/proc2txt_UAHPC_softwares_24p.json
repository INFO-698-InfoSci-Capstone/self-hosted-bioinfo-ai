[
  {
    "processed_text": "Skip to content Software Overview ¶ Overview ¶ Module availability Software modules are not available on the login nodes. To access them, you will need to connect to a compute node either via an interactive session or batch job. Our HPC systems support over 100 software applications. The first thing to know is that the compute nodes on each cluster are where you use and manage software. The software is available in three different ways: Libraries in the operating system (like fftw or screen ); Personal software that you build or download and place in your own directory space; Modules , which are external packages built and maintained by HPC team for system-wide usage. Cluster Differences ¶ It's important to note which cluster you're using for your analyses as all three do not share the same operating system, software, and libraries. Ocelote and ElGato both run on CentOS 7 and share the same filesystem. This means identical software modules, compilers, and system libraries are available on both. Puma runs on Rocky Linux 9 with different software modules, compilers, and system libraries than the other two systems. When compiling your own software or installing packages (e.g., with Python, R, Julia, etc), take note of which cluster you're using as migrating between Puma and the other clusters may result in failures. Policies ¶ Academic/Free Software ¶ There is a plethora of software generally available for scientific and research usage. We will install that software as a module if it meets the following requirements: | Requirements | | | --- | --- | | Compatible with our module environment | Some software is not written with clusters in mind and tries to install into system directories, or needs a custom environment on every compute node. | | Generally useful | Some software has to be configured to the specific compute environment of the user. You are encouraged to use our \"contrib\" environment to install your own. | | Public license | We do not install software if that would be a violation of its licensing. | | Reasonably well written | Some software takes days of effort and still does not work right. We have limited resources and reserve the right to \"give up\". Sometimes software is written for workstations and does not function correctly in a shared environment. | | Downloadable | Some software requires additional steps to download installation files, such as registering on a website or accepting a license agreement. In these cases we ask researchers to download files and put them in a directory on the HPC storage. When you submit a software installation request, let us know that you have already downloaded the files and provide path to the directory where they are located. | Commercial/Fee-based Software ¶ The University of Arizona Research Computing facility has many commercial and freeware packages installed on our supercomputers. Our approach to acquisition of additional software depends upon its cost, licensing restrictions, and user interest. | Audience | | | --- | --- | | Single user interest | The license for the software is purchased by the user and his/her department or sponsor. This software is best installed by the user. There are two main options; the first and easiest, is to install the software locally in the relevant user's account using the example procedure. The second is to use the \"contrib\" environment. The advantage is that you can share the software built here with other users. This is created through a support ticket where a consultant will create a \"contrib\" group in which you can build software and add users. | | Group interest | If a package is of interest to a group of several users, the best approach at first is for one user to act as a primary sponsor and arrange to split the procurement/licensing costs among the group. We can install the software and manage the user access according to requests from the group. | | Broad interest | The High Performance Computing team will consider acquiring and supporting software packages that have broad interest among our users. Full facility support will depend on the cost of the package and our ability to comply with any restrictive licensing conditions. | Unsupported Software ¶ Unfortunately, our HPC system is not configured to support all software use cases. We have summarized the main scenarios which cause software to be unsupported by our system below. Prior to submitting an installation request, double-check that your software requirements don't fall into one of these categories. While the HPC may not be able to support these cases, it may be possible that other campus resources are able to. We encourage you to contact services listed in our Community Resources page to see if they are able to support your workflow's requirements. The below list is not exhaustive and may be expanded as new scenarios are encountered. If you are unsure whether your desired software is supported, feel free to contact our consultants . | Software Requirements | | Examples | | --- | --- | --- | | Non-SSH External Connections | Software requiring external communications that utilize protocols other than SSH are not supported. | | | Job Management/Scheduling | The UArizona HPC uses Slurm as its task scheduler and resource manager. Software requiring a different scheduler is unsupported. | Apache Spark Sun Grid Engine | | Workstation Software | Software that is designed to be run on a local Linux workstation often requires root privileges and display management that may not be compatible with a shared, remote system like HPC | | | Persistent Databases/Servers | The UArizona HPC is not configured to support databases, server instances, or other persistent software-specific daemons. | SQL databases Application deployments SAS server | | Windows Applications | While there are plenty of excellent Windows software suites available for scientific computing, they unfortunately cannot be run on HPC. There are, however, national resources available that may support your application. One example is JetStream2 available through an ACCESS Allocation. See our Community Resources page linked at the top of this page for more details. | ArcGIS | Federal Regulations ¶ By policy, it is prohibited to use any of the facility's resources in any manner that violates the US Export Administration Regulations (EAR) or the International Trafficking in Arms Regulations (ITAR). It is relevant in this regard to be aware that the facility employs analysts who are foreign, nonresident, nationals and who have root-access privileges to all files and data. Specifically, you must agree not to use any software or data on facility systems that are restricted under EAR and/or ITAR. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/overview/",
      "title": "Overview - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "0741974d-57c3-4e4e-9e60-6e2fce2ac186",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/overview/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Mamba ¶ Mamba is a package manager to create and manage Conda environments. Following definitions will help you to understand the relevant terms that you might come across when using Mamba: CondaConda is a package management system and environment management system for installing multiple versions of software packages and their dependencies and switching between them.AnacondaAnaconda is a distribution built around Conda. It includes hundreds of packages, and access to the Anaconda repository, besides the package manager itself. The use of Anaconda on our HPC systems is now deprecated, due to recent developments in how the company behind Anaconda licenses the Anaconda repository. You can read more on that in the Anaconda FAQs .MambaMamba is a package manager which was initially developed as a faster alternative to Conda. It can create and manage Conda environments. For most cases, Mamba is a drop-in replacement for Conda. Mamba comes in multiple flavors, see the Mamba documentation for more information. The most relevant to us is the binary micromamba . For the rest of the document we will specifically focus on micromamba .EnvironmentIn the context of this document, an environment is a directory that contains a specific collection of packages that you have installed. We will only consider Conda environments in this document, i.e. environments that can be created and managed with Conda. All flavors of Mamba, including micromamba , can create and manage Conda environments. TL;DR If you were using Conda/Anaconda, use micromamba instead. Initializing micromamba ¶ You can check the available versions of micromamba with module avail micromamba . Once you have decided which version you want, you can load it and initialize it. You have to initialize it only once. In an interactive session, replacing with your desired micromamba module version: ``` module load micromamba/ micromamba shell init -s bash -r ~/micromamba source ~/.bashrc ``` This will create a directory called micromamba in your home folder in which your Conda environments and associated packages will be installed, and all that information to your .bashrc . Tip Your home directory can fill up pretty fast with environments and packages installed with micromamba . You can change the location where your environments and packages are installed to avoid that. Your PI's group directory can be a good choice for this. Run the following commands to change the location: ``` micromamba config append envs_dirs /envs micromamba config append pkgs_dirs /pkgs ``` Creating a Conda Environment ¶ Warning Always create a local Conda environment with micromamba before installing packages with in. Do not mix Conda environments with any other type of virtual environments. Tip For more information on environments, check out our Software and Environments workshop. After initializing micromamba , you can create a local Conda environment with it to install packages. If you are used to using Conda to create local environments, then micromamba is very similar. For example, in an interactive session, after loading the micromamba module: ``` micromamba create -n myenv micromamba activate myenv ``` This will create and activate a Conda environment called myenv . To view the environments available to you, run: ``` micromamba env list ``` Unlike some versions of Conda, micromamba will always create an empty environment with no packages installed. If you want to install packages, you can do so after activating the environment. You can also mention the packages you want to install when you create the environment. For example, the following command will create an environment named py312 with Python 3.12 installed in it: ``` micromamba create -n py312 python=3.12 ``` Installing packages in Conda environments ¶ After activating a Conda environment, you can install packages with: micromamba install <package-name> Language-specific package installers (such as pip for Python, or install.packages for R) You can also mix both approaches Which option you choose will depend on the package, and how the package maintainers make it available. By default micromamba downloads packages from the conda-forge repository or channel. conda-forge is a community maintained repository containing a large number of packages typically used in scientific computing, data science, and others. However, if you want to install packages from other repositories you can do so. For example, the bioconda repository provides many packages used in bioinformatics. To install a package from the bioconda repository, run the following in an interactive environment, replacing <package-name> with the name of your desired package: ``` micromamba install -c bioconda ``` If you want to install packages from any other repositories, simply replace bioconda with the name of the repository in the above command. Tip By default, micromamba does not access the Anaconda repository, and we recommend not changing that. To ensure that you do not install from the Anaconda repository, you can run the following: ``` micromamba config append channels conda-forge micromamba config append channels nodefaults micromamba config set channel_priority strict ``` micromamba in Batch Jobs ¶ Assuming that you have initialized micromamba (recommended), you will need to load the micromamba module and activate a Conda environment before you can access the packages installed in that environment. Add the following, replacing <env-name> with the name of your desired Conda environment, as your first bash commands in your batch script: ``` module load micromamba micromamba activate ``` Tip You should install packages in your Conda environment from an interactive session. Do not put those instructions in a batch script. Initialization Error Sometimes batch jobs with micromamba might fail with the following error message: ``` critical libmamba Shell not initialized 'micromamba' is running as a subprocess and can't modify the parent shell. Thus you must initialize your shell before using activate and deactivate. ... ``` If you have already correctly initialized micromamba as mentioned above and you are still getting this error message, you can do one of the following to ensure that Slurm sources your .bashrc (where the relevant micromamba information is stored): Replace #!/bin/bash at the top of your batch script with #!/bin/bash --login Add source ~/.bashrc to your batch script before you activate the virtual environment Alternatively, instead of activating a Conda environment with micromamba , you can use micromamba run -n <env-name> <command> to run your command in that Conda environment. For example, if you have the following in your batch script ``` module load micromamba micromamba activate myenv python myscript.py ``` you can replace it with ``` module load micromamba micromamba run -n myenv python myscript.py ``` In this case you will not have to source your .bashrc from your batch script. Language specific suggestions ¶ You can use micromamba for some Python or R workflows which would otherwise be non-trivial to do with the corresponding language modules. Warning Do not use the micromamba module along with the Python or R modules, such as python/3.8 or R/4.4 . Choose one or the other. Python ¶ One of the typical Python workflows on the HPC involves Jupyter through the Open OnDemand (OOD) interface. One issue of this is that if you use the Python modules then you are tied to the specific version of Python that Jupyter on OOD uses. If Jupyter on OOD uses Python 3.8, then you can only use the python/3.8 module to install your packages. With micromamba you do not have that limitation. You can use your desired version of Python. All you have to do is the following: Create and activate a Conda environment with your desired Python version Install Jupyter in it Create a Jupyter kernel For example, if you want to use Python 3.11, you can try running the following commands from an interactive environment: ``` micromamba create -n python=3.11 micromamba activate micromamba install jupyter # alternatively you can use pip install jupyter ipython kernel install --name --user ``` Once you've configured your kernel, go to OOD and start a Jupyter notebook. Once the session starts, open it and click the New dropdown menu in the upper right. If everything is working correctly, you should see your kernel. For example if the kernel's name is torch311 : R ¶ Some of the most widely used R packages on the HPC have non-trivial installation processes when installed with the R modules, see the Popular Packages for more information. Here we show how you can install these packages with micromamba . Updates and Version Changes We attempt to keep these instructions reasonably up-to-date. However, given the nature of ongoing software and package updates, there may be discrepancies due to version changes. If you notice any instructions that don't work, contact us and we will help. You have to be in an interactive terminal session and not in an RStudio session to run the commands below. To install any of the R packages, first create a Conda environment with R installed in it: ``` micromamba create -n r=4.4 micromamba activate ``` If you want to install any other version of R, then replace 4.4 with that version number in the above command. You might have to choose different versions of the packages in the examples below if you use a version of R below 4.4. Seurat & SeuratDisk Terra & Monocle3 To install Seurat, run: ``` micromamba install r-seurat ``` Seurat can also be installed with R's built-in package manager install.packages . However installing it with micromamba is way faster, since it just downloads the relevant binaries, and does not have to do any local compilation. For SeuratDisk, assuming you have already installed Seurat: Install dependencies: ``` micromamba install r-hdf5r r-remotes ``` Start an R prompt: ``` R ``` Install SeuratDisk with the install_github function from the remotes R package: ``` remotes::install_github(\"mojaveazure/seurat-disk\") ``` To install Terra, run: ``` micromamba install r-terra ``` Terra can also be installed with R's built-in package manager install.packages . However installing it with micromamba is way faster, since it just downloads the relevant binaries, and does not have to do any local compilation. For Monocle3, assuming you have already installed Terra: Install dependencies: ``` micromamba install r-biocmanager r-remotes r-ggrastr ``` Start an R prompt: ``` R ``` Install more dependencies with BiocManager: ``` BiocManager::install(c('BiocGenerics', 'DelayedArray', 'DelayedMatrixStats', 'limma', 'lme4', 'S4Vectors', 'SingleCellExperiment', 'SummarizedExperiment', 'batchelor', 'HDF5Array')) ``` Install Monocle3 with install_github function from the remotes R package: ``` remotes::install_github('cole-trapnell-lab/monocle3') ``` Jupyter ¶ Jupyter is typically thought of as belonging to the Python ecosystem, but R is one of the core languages that Jupyter supports . While R practitioners tend to gravitate towards RStudio, you might find that Jupyter is an equally capable, in not more, IDE for programming in R. Particularly, if you are using the Open OnDemand (OOD) interfaces of Jupyter and RStudio, you might find the Jupyter experience to be smoother. With micromamba you can easily set up Jupyter for your R workflow. To use your R packages from Jupyter, you have to install Jupyter in your Conda environment and then create a with the IRkernel package: Install Jupyter and IRkernel: ``` micromamba install jupyter r-irkernel ``` Start an R prompt: ``` R ``` Create a R kernel for Jupyter, replacing <kernel_name> and <display_name> with your desired kernel and display names: ``` IRkernel::installspec(name = \" \", displayname = \" \") ``` Once you've configured your kernel, go to OOD and start a Jupyter notebook. Once the session starts, open it and click the \"new\" dropdown menu in the upper right. If everything is working correctly, you should see your kernel. For example, if you had given a display name R 4.4 (the default kernel and display names are ir and R , respectively): RStudio ¶ To use the Open OnDemand (OOD) interface of RStudio with your Conda environment, do the following: Create a directory .UAz_ood under your home directory. Create a file rstudio.sh under .UAz_ood with the following contents, replacing <env_name> with the name of your environment: ``` !/bin/bash source ~/.bashrc micromamba activate ``` Version Issues Loading some packages such as terra in the RStudio Console might fail with an error message like the following: `` Error: package or namespace load failed for 'terra' in dyn.load(file, DLLpath = DLLpath, ...): unable to load shared object '/groups/sohampal/micromamba/envs/r-test/lib/R/library/terra/libs/terra.so': /lib64/libssl.so.3: version OPENSSL_3.2.0' not found (required by /groups/sohampal/micromamba/envs/r-test/lib/R/library/terra/libs/../../../.././libcurl.so.4) ``` The likely solution in such a case will be to install the correct version of the dependency, in this case, OpenSSL 3.2.0. However, doing so might require you to downgrade the version of R installed in your Conda environment. If you do not want to downgrade, then you might want to consider using Jupyter instead of RStudio as your IDE. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/popular_software/mamba/",
      "title": "Mamba - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "0af55769-15f4-49fe-8664-d00e0822cde3",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/popular_software/mamba/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Software Modules ¶ Availability Software modules are not available on the login nodes. You will need to be on a compute node to access them. Cluster Differences Ocelote and El Gato both run CentOS 7 as their operating system and share the same system libraries and modules. These two clusters can generally be used interchangably. Puma uses Rocky Linux 9 as its operating system and has different system libraries and modules than Ocelote and El Gato. This means workflows may not be transferrable between Puma and the other two clusters. Take note of which cluster you're using to prevent software failures. Software packages are available as modules and are accessible from the compute nodes of any of our three clusters. A software module is a tool used to manage software environments and dependencies. It allows you to easily load and unload different software packages, libraries, and compilers needed for computational tasks without conflicts. This ensures access to many specific tools, and even different versions of the same tools, without affecting the overall system configuration. Module Commands ¶ Default Versions If multiple versions of software are available on the system, the newest is made the default. This means loading a module without specifying the version will select the most recent. We strongly recommend including version information in your module statements. This ensures that you maintain a consistent environment for your analyses in the event of a software upgrade. | Command | Description | | --- | --- | | <br>module avail<br> | Display all the software and versions installed on the system | | <br>module avail <search_term><br> | Display all installed modules matching <search_term> | | <br>module list<br> | Display the software you have loaded in your environment | | <br>module whatis <module_name><br> | Displays some descriptive information about a specific module | | <br>module show <module_name><br> | Displays system variables that are set/modified when loading module <module_name> | | <br>module load <module_name><br> | Load a software module in your environment | | <br>module unload <module_name><br> | Unload a specific software package from your environment | | <br>module swap <module_name>/<version1> <module_name>/<version2><br> | Switch versions of a software module | | <br>module purge<br> | Unload all the software modules from your environment | | <br>module help<br> | Display a help menu for the module command | Examples ¶ Loading Modules ¶ ``` [netid@cpu39 ~]$ module avail python ------------------- /opt/ohpc/pub/modulefiles -------------------- python/3.6/3.6.5 python/3.9/3.9.10 python/3.8/3.8.2 python/3.11/3.11.4 (D) python/3.8/3.8.12 [netid@cpu39 ~]$ module load python/3.9 [netid@cpu39 ~]$ python3 --version Python 3.9.10 [netid@cpu39 ~]$ module swap python/3.9 python/3.11 The following have been reloaded with a version change: 1) python/3.9/3.9.10 => python/3.11/3.11.4 [netid@cpu39 ~]$ python3 --version Python 3.11.4 ``` Finding Executables and Libraries ¶ If you're looking for the specific paths added to your environment when loading a module, you can use the command module show . For example: ``` [netid@cpu38 ~]$ module show gromacs /opt/ohpc/pub/moduledeps/gnu8-openmpi3/gromacs/2021.5: whatis(\"Name: gromacs \") whatis(\"Version: 2021.5 \") whatis(\"Molecular dynamics for biophysical chemistry \") setenv(\"GROMACS_BASE\",\"/opt/ohpc/pub/apps/gromacs/2021.5\") prepend_path(\"PATH\",\"/opt/ohpc/pub/apps/gromacs/2021.5/bin\") prepend_path{\"CPPFLAGS\",\"-I/opt/ohpc/pub/apps/gromacs/2021.5/include\",delim=\" \"} prepend_path(\"MANPATH\",\"/opt/ohpc/pub/apps/gromacs/2021.5/share/man\") prepend_path(\"PKG_CONFIG_PATH\",\"/opt/ohpc/pub/apps/gromacs/2021.5/lib64/pkgconfig\") prepend_path{\"LDFLAGS\",\"-L/opt/ohpc/pub/apps/gromacs/2021.5/lib64\",delim=\" \"} prepend_path(\"LD_LIBRARY_PATH\",\"/opt/ohpc/pub/apps/gromacs/2021.5/lib64\") unload(\"gnu8\") load(\"gnu8\") help([[ Adds gromacs to your environment\\ ]]) ``` These paths will allow you to determine the locations of the executables, libraries, header files, etc. available to you after loading the software. Compilers ¶ Puma ¶ | Compiler | Version | Module Command | | --- | --- | --- | | gcc | 12.2.0 | <br>module load gnu12/12.2.0<br> | | gcc | 13.2.0 | <br>module load gnu13/13.2.0 # Recommended. Loaded by default.<br> | | intel | 2023.2.1 | <br>module load intel/2023.2.1<br> | | intel | 2024.0.0 | <br>module load intel/2024.0.0 # Default<br> | | intel | 2024.1.2 | <br>module load intel/2024.1.2<br> | Ocelote and El Gato ¶ | Compiler | Version | Module Command | | --- | --- | --- | | gcc | 5.4.0 | <br>module load gnu/5.4.0<br> | | gcc | 7.3.0 | <br>module load gnu7/7.3.0<br> | | gcc | 8.3.0 | <br>module load gnu8/8.3.0 # Recommended. Loaded by default<br> | | Intel | 2020.1 | <br>module load intel/2020.1<br> | | Intel | 2020.4 | <br>module load intel/2020.4<br> | Software Install Requests ¶ If you need to use a software package and it is not installed on the system, we can install it for you, provided it meets the criteria outlined in our software policies . Software can be requested by using our HPC software install request Form . There is no expected time frame for how long it takes to install software, as there are many variables that determine this. If you haven't heard back within a week, it may be appropriate to follow up. For software that doesn't meet the criteria outlined in our policies and doesn't fall into the unsupported software category, it may be possible for you to install it locally. We have instructions documented in our user installations section. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/modules/",
      "title": "Modules - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "13bfccd4-4b3d-40a5-b73d-ff3f7b892030",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/modules/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Pulling Containers ¶ Warning The Apptainer cache can fill up your home quickly. To set a different location, see our Cache Directory documentation . Pulling Docker Containers ¶ Apptainer has the ability to convert available docker images into sif format allowing them to be run on HPC. If you find an image on Docker Hub that you would like to use, you can pull it using the apptainer pull <local_image_name>.sif docker://docker_image . As an example, we could pull an Ubuntu image from Docker Hub with OS 22.04 by searching for Ubuntu, opening the Tags tab, and copying their docker pull command: Then, on HPC, we can run: ``` [netid@cpu37 pull_example]$ apptainer pull ubuntu_22.04.sif docker://ubuntu:22.04 INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob 01007420e9b0 done Copying config 3db8720ecb done Writing manifest to image destination Storing signatures 2024/02/20 09:02:02 info unpack layer: sha256:01007420e9b005dc14a8c8b0f996a2ad8e0d4af6c3d01e62f123be14fe48eec7 INFO: Creating SIF file... [netid@cpu37 pull_example]$ ls ubuntu_22.04.sif ``` Pulling Nvidia Images ¶ The NVIDIA GPU Cloud (NGC) provides GPU-accelerated HPC and deep learning containers for scientific computing. NVIDIA tests HPC container compatibility with the Apptainer runtime through a rigorous QA process. Application-specific information may vary so it is recommended that you follow the container-specific documentation before running with Apptainer. If the container documentation does not include Apptainer information, then the container has not yet been tested under Apptainer. Apptainer can be used to pull, execute, and bootstrap off Docker images. To pull images, you'll need to register with Nvidia . Once you have an account, you can view their images from their catalogue . Click on the name of the software you're interested in to view available versions If you click on the Tags tab at the top of the screen, you'll find the different versions that are available for download. For example, if we click on TensorFlow, we can get the pull statement for the latest tag of TensorFlow 2 by clicking the ellipses and selecting Pull Tag. This will copy a docker pull statement to your clipboard, in this case: ``` $ docker pull nvcr.io/nvidia/tensorflow:22.02-tf2-py3 ``` To pull and convert this NGC image to a local Apptainer image file, we'll convert this to: ``` $ apptainer build ~/tensorflow2-22.02-py3.sif docker://nvcr.io/nvidia/tensorflow:22.02-tf2-py3 ``` The general format for any pull you want to do is: ``` $ apptainer build docker://nvcr.io/ / ``` This Apptainer build command will download the app:tag NGC Docker image, convert it to Apptainer format, and save it to the local filename <local_image_name> . Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/containers/pulling_containers/",
      "title": "Pulling Containers - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "1297f5a7-fdb5-4dbd-b734-8d6805fa553f",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/containers/pulling_containers/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content What are Containers? ¶ Tip Want to learn more about containers? Check out our Intro to Containers workshop . A container is a packaged unit of software that contains code and all its dependencies including, but not limited to: system tools, libraries, settings, and data. This makes applications and pipelines portable and reproducible, allowing for a consistent environment that can run on multiple platforms. Shipping containers have frequently been used as an analogy because the container is standard, does not care what is put inside, and will be carried on any ship; or in the case of computing containers, it can run on many different systems. Docker is widely used by researchers, however, Docker images require root privileges which means they cannot be run in an HPC environment. Apptainer (formerly Singularity) addresses this by completely containing the authority so that all privileges needed at runtime stay inside the container. This makes it ideal for the shared environment of a supercomputer. Even better, a Docker image can be encapsulated inside an Apptainer image. Some ideal use cases that can be supported by Apptainer on HPC include: You already use Docker and want to run your jobs on HPC. You want to preserve your environment so a system change will not affect your work. You need newer or different libraries than are offered on the system. Someone else developed a workflow using a different version of Linux. You prefer to use a Linux distribution other than CentOS (e.g. Ubuntu). You want a container with a database server like MariaDB. The documentation in this section provides instructions on how to either take a Docker image and run it from Apptainer, or create and run an image using Apptainer only. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/containers/what_are_containers/",
      "title": "What are Containers - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "2879134f-b461-4d2b-9940-c24ffd883713",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/containers/what_are_containers/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Anaconda ¶ Anaconda modules are deprecated Anaconda on UA HPC is now deprecated, which means no new version of the Anaconda distribution will be installed, and the existing modules will be removed some time in the future. We recommend users to modify their workflow to replace Anaconda with Mamba. For more information on how to use Mamba, check our Mamba guide. Known Issues with Anaconda Anaconda is known to cause potential issues on HPC Clusters. If it is possible to design your workflow around the native Python package manager Pip, we highly encourage you to do so. If you decide to use Anaconda on our HPC system, please read this page carefully and make yourself aware of the common problems and how to best avoid them. Overview ¶ We have several versions of Anaconda installed as system modules for use. You can initialize these in your home directory for access and package management. | Version | Accessibility | | --- | --- | | 2020.02 | module load anaconda/2020.02 | | 2020.11 | module load anaconda/2020.11 | | 2022.05 | module load anaconda/2022.05 | Initializing Anaconda ¶ Initializing Anaconda in your account only needs to be performed once and is what makes Anaconda available and ready for customization (e.g., installing custom packages) in your account. Faster Reloading Conda will direct you to close and reopen your shell to complete the initialization process. You can skip this by running the command source ~/.bashrc listed in the instructions below. Turn Off Auto-Activate To ensure proper functioning of built-in system functions, turning off auto-activation is highly recommended . Do this by running conda config --set auto_activate_base false in a terminal following initialization. In an interactive session, replacing <version> with your desired Anaconda module version: ``` module load anaconda/ conda init bash source ~/.bashrc conda config --set auto_activate_base false ``` Creating a Conda Environment ¶ Once conda has been configured following the steps above, you can create a local environment. This allows you to control the version of Python you want to use, install your own software, and even create custom Juypyter kernels (making your environment accessible in an OnDemand notebook). To do this, you can use the command conda create . For example, in an interactive session: ``` conda activate conda create --name py37 python=3.7 # Build a local environment with a specific version of python conda activate py37 ``` To view the environments available to you, use the command ``` conda env list ``` Installing Conda Packages and Other Software ¶ Once you have created a conda environment, you can install the packages you need. To do this, follow the software-specific installation instructions. This may be as simple as running conda install <my_package> , or it may involve installing a handful of dependencies. If the installation instructions ask you to create a new environment, you do not have to repeat this step. Once you have performed the install, you should then be able to access your software within this environment. If you are unable to load your software, check your active environment with ``` conda info ``` and the installed packages with ``` conda list ``` Conda in Batch Jobs ¶ If you've turned off Conda's auto-activate feature (recommended), add the following as your first bash command in your batch script: ``` source ~/.bashrc ``` This will allow you to run standard conda activate and conda activate <environment name> commands. Without the source command, you may get errors indicating that conda has not been initialized. Custom Jupyter Kernel ¶ If you want to make one of your conda environments available for use in one of our Open OnDemand Jupyter Notebooks, you can do so by creating a custom kernel. To do this, start an interactive terminal session and activate your environment: ``` conda activate conda activate ``` Next, pip-install Jupyter and use it to create a custom kernel using the command ipython and replacing <your_environment> with your own environment's name: ``` pip install jupyter ipython kernel install --name --user ``` Once you've configured your kernel, go to Open OnDemand and start a Jupyter notebook. Once the session starts, open it and click the \"new\" dropdown menu in the upper right. If everything is working correctly, you should see your custom name. For example, if you created a conda environment with the kernel name py38, you should see the following: Once you've selected your environment, try checking the Python version in your notebook using the sys module. Additionally, for demonstration purposes, we'll check that a custom package installed in py38 (emoji) can be imported and is working. Loading Modules in Jupyter ¶ In OnDemand Jupyter sessions, accessing HPC software modules directly from within a notebook can be challenging due to system configurations. However, it's still possible to access these modules when needed. For instance, machine learning packages like TensorFlow or PyTorch often require additional software modules such as CUDA for GPU utilization. To access software modules in your Jupyter notebooks, follow the steps below: Step 1: If you haven't already done so, create a custom kernel for your Jupyter notebook environment. Step 2: You will then need to edit your kernel configuration file kernel.json which is what sets up your environment at runtime. This file can be found in the following location, where <kernel_name> is a placeholder for the name you gave your kernel when it was created: ``` $HOME/.local/share/jupyter/kernels/ /kernel.json ``` Step 3: Next, you will need to modify your kernel's configuration by editing this file. Start by opening it with a text editor, for example nano $HOME/.local/share/jupyter/kernels/<kernel_name>/kernel.json . The contents of this file should look something like the following: ``` { \"argv\": [\\ \" /bin/python\",\\ \"-m\",\\ \"ipykernel_launcher\",\\ \"-f\",\\ \"{connection_file}\"\\ ], \"display_name\": \" \", \"language\": \"python\", \"metadata\": { \"debugger\": true } } ``` The part you need to change is the section under argv . We will change this from executing a Python command to a Bash command with a module load statement. Make a note of the path </path/to/your/environment>/bin/python to use in the edited file. The edited file should look like the following: ``` { \"argv\": [\\ \"bash\",\\ \"-c\",\\ \"module load ; /bin/python -m ipykernel_launcher -f {connection_file}\"\\ ], \"display_name\": \" \", \"language\": \"python\", \"metadata\": { \"debugger\": true } } ``` Replace <your_modules_here> with the modules you would like to load and </path/to/your/environment>/bin/python with the path to your environment's python. Step 4: Save the kernel.json file and restart your Jupyter notebook session. Removing Anaconda From Your Environment ¶ When Anaconda is initialized, your .bashrc file is edited so that conda becomes the first thing in your PATH variable any time you're logged into HPC. This can cause all sorts of mayhem when using other system functions or software. If you are running into issues (particularly when using other package managers or compiling), we recommend removing Anaconda from your environment as part of your debugging process. Sometimes it can be as simple as turning off Anaconda's auto-activation, other times it becomes necessary to modify your environment and its variables. Below are three methods for removing Anaconda from your environment: Turn off Auto-activation Temporary Removal Permanent Removal Removing Auto-activation may not always be sufficient Sometimes turning off auto-activation won't be enough because Anaconda will still be present in your PATH . In this case, follow the instructions in the tab Temporary Removal or Permanent Removal By default, Anaconda's initialization will tell it to automatically activate itself when you log in (when Anaconda is active, you will see a (base) preceding your command prompt). This is known to cause issues, for example, this behavior breaks OnDemand Desktop sessions preventing you from making a connection. If you have not already done so, disable this behavior by running the following from the command line in an interactive terminal session: ``` conda config --set auto_activate_base false ``` This will suppress Anaconda's activation until you explicitly call conda activate and is a handy way to have more control over your environment. Once you run this, you will need to log out and log back in again. If you have already turned off Anaconda's auto-activation feature and are still running into issues, it may be necessary to modify your environment variables to fully remove Anaconda. This is because conda deactivate is insufficient and Anaconda binaries and libraries will still be accessible. To fully remove Anaconda, you can either use the command conda deactivate and then manually edit your PATH variable to remove all paths where conda is present, or you can copy the following code block and run it in your terminal: ``` conda deactivate > /dev/null 2>&1 IFS=':' read -ra PATHAR <<< \"$PATH\" for i in \"${PATHAR[@]}\" do if [[ $i == \"conda\" ]] then echo \"removing $i from PATH\" else NEWPATH=$i:$NEWPATH fi done export PATH=$NEWPATH module unload gnu8 && module load gnu8 unset NEWPATH echo \"Successfully removed conda\" ``` This is a temporary solution and will only modify your current working environment. Anaconda will still be present for all future HPC sessions. If you would like to make the change permanent, follow the instructions under the Permanent Removal tab. Be careful when editing your ~/.bashrc Your ~/.bashrc file configures your environment each time you start a new session. Be careful when editing it. You may consider making a backup in case of unwanted changes. Not sure what a ~/.bashrc is and want more information? Check out our Linux cheat sheet guide . The most permanent solution for removing Anaconda from your environment is to edit your ~/.bashrc to manually remove its initialization. This change will remove Anaconda from all future terminal sessions. Start by opening the file ~/.bashrc . This can be done using the command nano ``` nano ~/.bashrc # opens your bashrc file to edit ``` Then comment out or delete the following lines and the text in between: ``` >>> conda initialize >>> ... <<< conda initialize <<< ``` To exit use Ctrl + X , select Y to save, and hit Enter to confirm your filename. This change will not take effect right away. To make the changes live, log out of HPC and then log back in again. If you need Anaconda again in the future, you can either uncomment the initialization lines (if you commented them out), or you can initialize Anaconda again. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/popular_software/anaconda/",
      "title": "Anaconda - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "3d7c7b59-aa42-4a18-8131-677006a056e9",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/popular_software/anaconda/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Using Containers ¶ If you've gone through the previous sections, you have an idea of how to pull and create containers, but how do you actually use one? Apptainer can be used to run your analyses and applications in various ways: running a prepackaged workflow embedded in the image with apptainer run , executing commands within the container's environment using apptainer exec , or starting an interactive instance to work directly within the container using apptainer shell . Running Prepackaged Apptainer Workflows ¶ Note: Not every container may include a predefined workflow apptainer run is used without any arguments and executes a predefined workflow embedded in the image. The general syntax is: ``` apptainer run .sif ``` For example: ``` [netid@cpu38 run_example]$ apptainer run lolcow.sif / You get along very well with everyone \\ \\ except animals and people. / \\ ^__^ \\ (oo)\\_______ (__)\\ )\\/\\ ||----w | || || ``` Executing Commands within Apptainer Images ¶ apptainer exec allows you to execute commands within an Apptainer image without entering it interactively. It simplifies running tasks by providing a command-line interface to interact with containerized applications, ensuring consistency and reproducibility in your workflow. The general syntax is ``` apptainer exec app.sif ``` For example: ``` apptainer exec /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif python3 script.py ``` Using Apptainer Applications Interactively ¶ Apptainer shell starts an interactive session within the container's environment. This is optimal for testing, debugging, or using any sort of graphical interface installed in your image. The general syntax is ``` apptainer shell app.sif ``` For example: ``` [netid@cpu38 run_example]$ apptainer shell /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif Singularity> python3 Python 3.8.10 (default, Sep 28 2021, 16:10:42) [GCC 9.3.0] on linux Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. import tensorflow as tf tf. version '2.6.2' ``` Apptainer and GPUs ¶ If you're using Apptainer to execute workflows that require a GPU, you'll need to add the additional flag --nv to your Apptainer command. For example: ``` apptainer exec --nv ``` Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/containers/using_containers/",
      "title": "Using Containers - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "596c8c50-8554-45cd-8c4b-55da075f202c",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/containers/using_containers/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Containers on HPC ¶ Tip Apptainer is installed on the operating systems of all HPC compute nodes, so can be easily accessed either from an interactive session or batch script without worrying about software modules. Available Containers ¶ We support the use of HPC and ML/DL containers available on NVIDIA GPU Cloud (NGC). Many of the popular HPC applications including NAMD, LAMMPS and GROMACS containers are optimized for performance and available to run in Apptainer on Ocelote or Puma. The containers and respective README files can be found in /contrib/singularity/nvidia . They are only available from compute nodes, so start an interactive session if you want to view them. | Container | Description | | --- | --- | | nvidia-caffe.20.01-py3.simg | Caffe is a deep learning framework made with expression, speed, and modularity in mind. It was originally developed by the Berkeley Vision and Learning Center (BVLC) . | | nvidia-gromacs.2018.2.simg | GROMACS is designed to simulate biochemical molecules like proteins, lipids, and nucleic acids | | nvidia-julia.1.2.0.simg | The Julia programming language is a flexible dynamic language, appropriate for scientific and numerical computing, with performance comparable to traditional statically-typed languages. | | nvidia-lammps.24Oct2018.sif | The main use case of the Large-scale Atomic / Molecular Massively Parallel Simulator is atom scale particle modeling or, more generically, as a parallel particle simulator at the atomic, meson, or continuum scale | | nvidia-namd_2.13-multinode.sif | NAMD is a parallel molecular dynamics code designed for high-performance simulation of large biomolecular systems. NAMD uses the popular molecular graphics program VMD for simulation setup and trajectory analysis, but is also file-compatible with AMBER, CHARMM, and X-PLOR. | | nvidia-pytorch.20.01-py3.simg | PyTorch is a Python package that provides two high-level features: - Tensor computation (like numpy) with strong GPU acceleration - Deep Neural Networks built on a tape-based autograd system | | nvidia-rapidsai.sif | RAPIDS provides unmatched speed with familiar APIs that match the most popular PyData libraries. Built on state-of-the-art foundations like NVIDIA CUDA and Apache Arrow. | | nvidia-relion_2.1.b1.simg | RELION (REgularized LIkelihood OptimizatioN) implements an empirical Bayesian approach for analysis of electron cryo-microscopy (Cryo-EM). Specifically, RELION provides refinement methods of singular or multiple 3D reconstructions as well as 2D class averages. | | nvidia-tensorflow_2.0.0-py3.sif | TensorFlow is an open source software library for numerical computation using data flow graphs. TensorFlow was originally developed by researchers and engineers working on the Google Brain team within Google's Machine Intelligence research organization for the purposes of conducting machine learning and deep neural networks research. | | nvidia-theano.18.08.simg | Theano is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. | Sharing Your Containers ¶ If you have containers that you would like to share with your research group or broader HPC community, you may do so by creating a directory in the space /contrib/singularity/shared . Cache Directory ¶ To speed up image downloads for faster, less redundant builds and pulls, Apptainer sets a cache directory in your home under ~/.apptainer . This directory stores images, metadata, and docker layers that can wind up being reasonably large. If you're struggling with space usage and your home's 50 GB quota, one option is to set a new Apptainer cache directory. You can do this by setting the environment variable APPTAINER_CACHEDIR to a new location. From Apptainer's documentation: If you change the value of APPTAINER_CACHEDIR be sure to choose a location that is: Unique to you. Permissions are set on the cache so that private images cached for one user are not exposed to another. This means that APPTAINER_CACHEDIR cannot be shared. Located on a filesystem with sufficient space for the number and size of container images anticipated. Located on a filesystem that supports atomic rename, if possible. For example, if you wanted to set your cache directory to your PI's /groups directory under a directory you own, you could use: ``` export APPTAINER_CACHEDIR=/groups/pi/your_netid/.apptainer ``` To make the change permanent, add this line to the hidden file in your home directory ~/.bashrc . If you are unfamiliar with this file and would like more information, see our Linux cheat sheet guide . Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/containers/containers_on_hpc/",
      "title": "Containers on HPC - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "74a2cf37-e274-4a83-b766-7e3da02268e0",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/containers/containers_on_hpc/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content User Installations ¶ Package managers ¶ Many popular programs, in particular Python and R have built-in package managers that can be used to collect new software from the internet and easily install them to your environment. See the section corresponding to your preferred language on the left under the \"Popular Software\" heading for more detailed instructions. Manual installations ¶ You are encouraged to download and compile any desired software in your own account. Commands like git clone and wget are available to pull repositories from the internet. The installation process may involve changing install locations from system folders (e.g. /usr/bin/xyz ) to user folders (e.g. /home/ux/netid/ or /groups/pi_netid/netid/ ), and will vary substantially from software to software, so providing instructions for all cases is beyond the scope of this page. If you encounter difficulties while installing your own software, the HPC Consult team is available for assistance. While you cannot add or update system software or libraries using tools that require root privileges such as yum , many software packages can be installed locally without needing to be a superuser. Frequently linux packages make use of the \" configure , make , make install \" method which allows you to customize your installation location. An example of how to do this is shown below. Where to compile Software is not available on the login nodes. To install custom software, log into an interactive session. Installation destination For a typical Linux installation, the default settings may attempt to install files in system locations. This is not permitted, so the installation process (specifically, the ./configure step) needs to be changed so that files are installed somewhere you have write access. There is frequently done with the syntax --prefix=/path/to/software . Compiling on different clusters Be sure to note which cluster you are compiling your software on. El Gato and Ocelote run the operating system CentOS 7 and share the same system libraries and modules. Puma runs Rocky Linux 9 and has its own newer system libraries and modules. Compiling software on one operating system and trying to use it on another may result in failures. configure/make/make install example cmake Here is a typical example of installing software on a Linux cluster with the configure , make , make install method. We'll use a simple hello world example downloaded from https://ftp.gnu.org/gnu/hello/ Download and unpack the software ``` [user@cpu39 make_example]$ wget https://ftp.gnu.org/gnu/hello/hello-2.10.tar.gz [user@cpu39 make_example]$ tar xzvf hello-2.10.tar.gz [user@cpu39 make_example]$ cd hello-2.10 ``` Configure your software The ./configure command generates a Makefile tailored to the specific environment and requirements of the system where the software is being installed. The use of the --prefix option allows users to install the software to a custom directory, circumventing the standard root locations which users do not have permission to modify. In this example, we'll install the software to a directory called hello_world in our home. ``` [user@cpu39 hello-2.10]$ ./configure --prefix=$HOME/hello_install ``` Compile and install your software Tip Often, there is an additional option to test your software after compiling it and before installing it. Usually, this is something like make test or make check The make command compiles the software according to the instructions provided in the Makefile generated by ./configure . Once the software is compiled, make install will install the software in the directory you specified with the --prefix option. ``` [user@cpu39 hello-2.10]$ make [user@cpu39 hello-2.10]$ make install [user@cpu39 hello-2.10]$ ls $HOME/hello_install bin share ``` Modify your environment Different environment variables control where the system looks for executables, libraries, header files, etc. Modifying your environment variables will allow you to use your new software without specifying the full paths. These variables can either be set manually on the command line for each new session, or can be added to your bashrc to make the changes permanent. For more information, see the sections Environment Variables and Hidden Files and Directories in our Bash cheat sheet . ``` [user@cpu39 hello-2.10]$ export PATH=$HOME/hello_install/bin:$PATH ``` Use your software ``` [user@cpu39 hello-2.10]$ hello Hello, world! ``` Here is a typical example of installing software on a Linux cluster with the cmake method. We'll use the software library Eigen as an example. Download the software In this example, we'll git clone the source code from the Gitlab repository https://gitlab.com/libeigen/eigen ``` [user@cpu39 cmake_example]$ git clone https://gitlab.com/libeigen/eigen.git [user@cpu39 cmake_example]$ cd eigen/ ``` Create and set your build environment Next, create a subdirectory called build where build files will be generated and stored. ``` [user@cpu39 eigen]$ mkdir build [user@cpu39 eigen]$ cd build ``` Additionally, you'll often need to set some environment variables to point to compilers and libraries. In this case, we'll set CC (which sets your C compiler), CXX (which sets your C++ compiler), and FC (which sets your Fortran compiler). ``` [user@cpu39 build]$ export CC=$(which gcc) [user@cpu39 build]$ export CXX=$(which g++) [user@cpu39 build]$ export FC=$(which gfortran) ``` Tip The command which returns a full filepath to an executable. Running something like export FOO=$(which foo) will take the output of which foo and store it as the environment variable FOO Configure your build Use cmake to configure your build. Use -DCMAKE_INSTALL_PREFIX to set a custom installation directory that you have access to. This will prevent the installation process from trying to access default root-owned locations which users don't have permission to modify. The .. is a shortcut for the directory one level above which is where the CMakeLists.txt file lives. ``` [user@cpu39 build]$ cmake -DCMAKE_INSTALL_PREFIX=$HOME/eigen_install .. ``` Compile and install your software ``` [user@cpu39 build]$ make [user@cpu39 build]$ make install [user@cpu39 build]$ ls $HOME/eigen_install include share ``` Modify your environment Different environment variables control where the system looks for executables, libraries, header files, etc. Modifying your environment variables will allow you to use your new software without specifying the full paths. These variables can either be set manually on the command line for each new session, or can be added to your bashrc to make the changes permanent. For more information, see the sections Environment Variables and Hidden Files and Directories in our Bash cheat sheet . In this case, we'll set INCLUDE and CPPFLAGS ``` [user@cpu39 build]$ export INCLUDE=$HOME/eigen_install:$INSTALL [user@cpu39 build]$ export CPPFLAGS=\"-I$HOME/eigen_install $CPPFLAGS\" ``` Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/user_installations/",
      "title": "User Installations - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "6856ee9e-b947-442a-a927-6cac25431089",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/user_installations/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content AlphaFold ¶ AlphaFold is an AI system developed by the Google DeepMind project to predict the 3D structure of a protein from its amino acid sequence. AlphaFold needs multiple datasets for inference, the combined size of which is around several TBs. We currently host the datasets for AlphaFold 2 & 3, and the corresponding modules. The datasets are available under /contrib/datasets/alphafold . Each AlphaFold module loads an Apptainer container containing the corresponding AlphaFold. You can use the containers directly, if you want. They are available under /contrib/singularity/alphafold . The modules however provide some useful shortcuts, and you can use them without knowing anything about Apptainer containers. AlphaFold 2 ¶ We currently host AlphaFold version 2.3.0. For versions 2.3.1 or 2.3.2, you will have to build your own Apptainer containers. You can use corresponding Docker images for that purpose. For more information on building your own Apptainer containers from Docker images, see Containers . These use the same datasets as version 2.3.0. When you load the alphafold/2.3.0 module it adds the following variables to your environment: ALPHAFOLD_DIR which points to /contrib/singularity/alphafold ALPHAFOLD_DATADIR which points to /contrib/datasets/alphafold/2.3.0 You can use these variables to easily access the container and the datasets. You can use the the module from either a batch script or an interactive session, the invocations are the same. For optimum performance, you should use no more than 8 CPU cores. While you can run this without a GPU, we do not recommend that. The following batch script demonstrates how you can use this module: ``` !/bin/bash SBATCH --job-name=alphafold2-run SBATCH --time=04:00:00 SBATCH --gres=gpu:volta:1 SBATCH --cpus-per-task=8 SBATCH --ntasks-per-node=1 SBATCH --partition=gpu_standard SBATCH --account= Uncomment the following two lines to make predictions on proteins that would be too long to fit into GPU memory. export APPTAINERENV_TF_FORCE_UNIFIED_MEMORY=1 export APPTAINERENV_XLA_PYTHON_CLIENT_MEM_FRACTION=4.0 module load alphafold/2.3.0 alphafold --nv \\ --use_gpu_relax \\ --uniref90_database_path=/data/uniref90/uniref90.fasta \\ --uniref30_database_path=/data/uniref30/UniRef30_2021_03 \\ --mgnify_database_path=/data/mgnify/mgy_clusters_2022_05.fa \\ --bfd_database_path=/data/bfd/bfd_metaclust_clu_complete_id30_c90_final_seq.sorted_opt \\ --pdb70_database_path=/data/pdb70/pdb70 \\ --template_mmcif_dir=/data/pdb_mmcif/mmcif_files \\ --obsolete_pdbs_path=/data/pdb_mmcif/obsolete.dat \\ --model_preset=monomer \\ --max_template_date=2023-08-02 \\ --db_preset=full_dbs \\ --output_dir= \\ --fasta_paths= ``` You can find out the full list of options that you can specify by loading the module in an interactive session and running alphafold --helpfull . AlphaFold 3 ¶ We host AlphaFold version 3.0.0. Unlike AlphaFold 2, AlphaFold 3 is not completely free. We host the datasets it needs for inference. However, you need to obtain the model parameters from Google directly. Fill and submit this form to request access to the model parameters from Google. Please ensure that you have read the terms and conditions mentioned in the form and that you can comply with them. After Google approves your request, they will send you an email with a link to download a file containing the model parameters. You will likely download the file on your local computer, it is little less than 1 GB in size. Use one of our file transfer options to transfer this file to your HPC storage. The model parameters file is in a compressed format. You will need to decompress it to use it. From an interactive session, run: ``` module load zstd unzstd af3.bin.zstd ``` We recommend that you put the decompressed af3.bin file in a directory named models . When you load the alphafold/3.0.0 module it adds the following variables to your environment: ALPHAFOLD_DIR which points to /contrib/singularity/alphafold ALPHAFOLD_DATADIR which points to /contrib/datasets/alphafold/3.0.0 AlphaFold 3 requires 1 GPU for inference. Officially only NVIDIA A100 and H100 GPUs, with 80 GB of GPU RAM, are supported. However, you can use older GPUs, such as V100 and P100, or GPUs with less memory, such as one of our A100 slices with 20 GB of GPU RAM. The input size will be limited in these cases. The AlphaFold documentation provides strategies that you can adopt to make runs with larger inputs. To run it on node with P100 or V100 GPUs, add --flash_attention_implementation=xla to the alphafold command in the example below. For optimum performance do not use more than eight CPU cores. The following batch script demonstrates how you can use this module: ``` !/bin/bash SBATCH --job-name=alphafold3-run SBATCH --time=04:00:00 SBATCH --gres=gpu:nvidia_a100_80gb_pcie_2g.20gb SBATCH --cpus-per-task=8 SBATCH --ntasks-per-node=1 SBATCH --partition=gpu_standard SBATCH --account= module load cuda12 module load alphafold/3.0.0 alphafold --db_dir=/data --output_dir= --model_dir=models --json_path= ``` You can find out the full list of options that you can specify by loading the module in an interactive session and running alphafold --helpfull . Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/popular_software/alphafold/",
      "title": "AlphaFold - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "8a353040-9fcb-4f44-9312-095e70345bb7",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/popular_software/alphafold/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Perl ¶ Accessibility ¶ Perl is installed on the operating system of each compute node: ``` [netid@compute_hostname ~]$ perl --version This is perl 5, version 16, subversion 3 (v5.16.3) built for x86_64-linux-thread-multi (with 44 registered patches, see perl -V for more detail) ... ``` Perl Module Policy ¶ We provide a version of Perl through modules or the operating system. Installation of additional user libraries can be done in a Perl environment using perl-virtualenv . For a helpful Perl tutorial, see: http://www.tutorialspoint.com/perl/perl_modules.htm . Additionally, O'Reilly Media is a well regarded source for Perl Installing Perl Packages Using perl-virtualenv ¶ One of the best things about Perl is the number of packages provided by the user community. Installing packages generally requires root access but that is not a viable solution in the HPC environment. An easy solution is to use perl-virtualenv on a compute node to create a consistent personal Perl environment. An example of usage: ``` [netid@i0n1 ~]$ perl-virtualenv my_project # Create virtual environment perl path: /usr/bin/perl venv path: /home/uxx/netid/my_project [netid@i0n1 ~]$ source my_project/bin/activate # Activate virtual environment (my_project)[netid@i0n1 ~]$ cpanm -i Config::Trivial --> Working on Config::Trivial Fetching http://www.cpan.org/authors/id/A/AT/ATRICKETT/Config-Trivial-0.81.tar.gz ... OK Configuring Config-Trivial-0.81 ... OK ... 4 distributions installed (my_project)[netid@i0n1 ~]$ ``` Once your environment is created, it can be activated for any future jobs or interactive sessions using the source </path/to/environment> command. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/popular_software/perl/",
      "title": "Perl - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "8ef2f38f-a82d-4866-a2d5-180686b3d84b",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/popular_software/perl/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Building Containers ¶ Warning The Apptainer cache can fill up your home quickly. To set a different location, see our Cache Directory documentation . Tip For detailed information on Apptainer recipes, see Apptainer's official documentation . Apptainer Build is a tool that allows you to create containers. With Apptainer Build, you can package your application and its dependencies into a single unit, making it easier to deploy and share across different computing environments. Two useful options are to build your container by bootstrapping off a container hosted locally on HPC or bootstrapping off an existing container hosted on Dockerhub. We'll cover both cases below. Bootstrapping off a Local Image ¶ One common case users run into is using a Python container hosted on HPC (say, one of our Nvidia machine learning images) but finding they need additional packages installed in the image. To do this, it's possible to bootstrap off the local image and pip-install a new package in a section called %post which is executed during build time. For example, say we want to use the HPC container nvidia-tensorflow-2.6.0.sif located in /contrib/singularity/nvidia/ but we need it to have the package astropy installed which is currently missing. We can create a recipe file that takes this image, bootstraps off it, and pip-installs astropy. Our recipe file would look like the following: ``` Bootstrap: localimage From: /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif %post pip install astropy ``` We'll call this recipe file something descriptive, e.g. tf2.6-astropy.recipe. Then, to build, all we need to do is use the syntax apptainer build <output_image> <recipe_file> . In this case: ``` [netid@cpu43 build_example]$ apptainer build tf2.6-astropy.sif tf2.6-astropy.recipe INFO: User not listed in /etc/subuid, trying root-mapped namespace INFO: The %post section will be run under fakeroot INFO: Starting build... INFO: Verifying bootstrap image /contrib/singularity/nvidia/nvidia-tensorflow-2.6.0.sif . . . INFO: Creating SIF file... INFO: Build complete: tf2.6-astropy.sif [netid@cpu43 build_example]$ ``` Bootstrapping off a Docker Hub Image ¶ Tip Not sure how to install your own software? Check our our section on User Installations . Bootstrapping off Ubuntu images is a great way to create a very customizable container where you can install your own software. Instead of pulling an Ubuntu image (see: Pulling Containers for a tip on how to find an Ubuntu image), we can bootstrap directly off the image in our recipe file. Let's say as an example, we want to install Python 3.11 with a custom library. We can create a recipe file called python3.11_astropy.recipe with the following contents: ``` Bootstrap: docker From: ubuntu:22.04 %post apt update -y apt install build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev -y wget https://www.python.org/ftp/python/3.11.3/Python-3.11.3.tgz tar -xf Python-3.11.3.tgz cd Python-3.11.3 ./configure --enable-optimizations make make altinstall python3.11 -m pip install astropy ``` Then to execute the build process and create our image, we can use: ``` [netid@cpu38 pull_example]$ apptainer build python3.11_astropy.sif python3.11_astropy.recipe INFO: User not listed in /etc/subuid, trying root-mapped namespace INFO: The %post section will be run under fakeroot INFO: Starting build... . . . INFO: Creating SIF file... INFO: Build complete: python3.11_astropy.sif [netid@cpu38 pull_example]$ ``` Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/containers/building_containers/",
      "title": "Building Containers - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "90446c74-d4c7-47cc-83ad-0dca593ff6bc",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/containers/building_containers/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Gaussian ¶ Access ¶ In order to access Gaussian and Gaussview, you will need to belong to a special group called g03 . You can request to be added by submitting a help ticket . This is a constraint specific to Gaussian that other modules do not have. GPU Notes ¶ When reading these notes, keep in mind that the GPU nodes on Ocelote have one P100 GPU, 28 cores and 256 GB RAM. Gaussian 16 can use the NVIDIA P100 GPUs installed on Ocelote. Earlier GPUs do not have the computational capabilities or memory size to run the algorithms in G16. Allowing larger amounts of memory is even more important when using GPUs than for CPUs, since larger batches of work must be done at the same time in order to use the GPUs efficiently (see below). When using GPUs it is essential to have the GPU controlled by a specific CPU and is much more efficient if the CPU is physically close to the GPU it is controlling. The hardware arrangement can be checked using the nvidia-smi utility. For example, this output is for a machine with 2 16-core Haswell CPU chips and 4 K80 boards, each of which has two GPUs: ``` GPU0 GPU1 GPU2 GPU3 GPU4 GPU5 GPU6 GPU7 CPU Affinity GPU0 X PIX SOC SOC SOC SOC SOC SOC 0-15 GPU1 PIX X SOC SOC SOC SOC SOC SOC 0-15 GPU2 SOC SOC X PIX PHB PHB PHB PHB 16-31 GPU3 SOC SOC PIX X PHB PHB PHB PHB 16-31 GPU4 SOC SOC PHB PHB X PIX PXB PXB 16-31 GPU5 SOC SOC PHB PHB PIX X PXB PXB 16-31 GPU6 SOC SOC PHB PHB PXB PXB X PIX 16-31 GPU7 SOC SOC PHB PHB PXB PXB PIX X 16-31 ``` The important part is the CPU affinity. This shows that GPUs 0 and 1 (on the first K80 card) are connected to the CPUs on chip 0 while GPUs 2-7 (on the other three K80 cards) are connected to the CPUs on chip 1. So a job which uses all the CPUs (24 CPUs doing parts of the computation and 8 controlling GPUs) would use input ``` %cpu=0-31 %gpucpu=0-7=0-1,16-21 ``` or equivalently but more verbosely ``` %cpu=0-31 %gpucpu=0,1,2,3,4,5,6,7=0,1,16,17,18,19,20,21 ``` This pins threads 0-31 to CPUs 0-31 and then uses GPU0 controlled by CPU 0, GPU1 controlled by CPU 1, GPU2 controlled by CPU 16, etc. Normally one uses consecutive numbering in the obvious way, but things can be associated differently in special cases. For example, suppose on the other machine one already had one job using 6 CPUs running with %cpu=16-21 . Then if one wanted to use the other 26 CPUs with 8 controlling GPUs one would specify: ``` %cpu=0-15,22-31 %gpucpu=0-7=0-1,22-27 ``` This would create 26 threads with GPUs controlled by the threads on CPUs 0,1,22,23,24,25,26, and 27. GPUs are not helpful for small jobs but are effective for larger molecules when doing DFT energies, gradients and frequencies (for both ground and excited states). They are not used effectively by post-SCF calculations such as MP2 or CCSD. Each GPU is several times faster than a CPU but since on modern machines there are typically many more CPUs than GPUs, it is important to use all the CPUs as well as the GPUs and the speedup from GPUs is reduced because many CPUs are also used effectively (i.e., in a job which uses all the CPUs and all the GPUs). For example, if the GPU is 5x faster than a CPU, then the speedup from going to 1 CPU to 1 CPU + 1 GPU would be 5x, but the speedup going from 32 CPUs to 32 CPUs + 8 GPUs would be 32 CPUs -> 24 CPUs + 8 GPUs, which would be equivalent to 24 + 5x8 = 64 CPUs, for a speedup of 64/32 or 2x. The GPUs can have up to 16 GB of memory and one typically tries to have most of this available for Gaussian, which requires at least an equal amount of memory for the CPU thread which is running each GPU. 8 or 9 GB works well if there is 12 GB total on each GPU, or 11-12 GB for a 16 GB GPU. Since the program gives equal shares of memory to each thread, this means that the total memory allowed should be the number of threads times the memory required to use a GPU efficiently. For example, when using 4 CPUs and two GPUs each of which has 12 GB of memory, one should use 4 x 12 GB of total memory, i.e. ``` %mem=48gb %cpu=0-3 %gpucpu=0-1=0,2 ``` (or whatever specific CPU and GPU numbers are appropriate to the machine). GPUs on nodes in a cluster can be used. Since the %cpu and %gpucpu specifications are applied to each node in the cluster, the nodes must have identical configurations (number of GPUs and their affinity to CPUs); since most clusters are collections of identical nodes, this is not usually a problem. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/popular_software/gaussian/",
      "title": "Gaussian - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "9252397d-c3bf-42af-9e75-a00257fbffce",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/popular_software/gaussian/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content VSCode Remote Connection ¶ Overview ¶ Visual Studio Code (VS Code) can be used to edit source code and other files on the HPC systems. VS Code is available to run directly on HPC through the Open OnDemand system. VS Code can also be run locally on laptop or desktop computers and used to make a remote connection to the HPC systems. This documentation is intended to detail the steps that must be taken to allow such a connection. For more detailed information on establishing SSH connections, refer to the VS Code documentation here: https://code.visualstudio.com/docs/remote/ssh-tutorial . There is also a general example shown in the section Connection Example below. General Method ¶ Remote VSCode sessions should connect to a compute node. The reasons for this are: Connecting to the bastion host (hostname: hpc.arizona.edu ) will generate files that may overfill your 10 MB quota. This can cause unpredictable connection issues for any future SSH sessions. Additionally, the bastion host is not connected to the shared storage array which means your HPC files will not be accessible. Connecting directly to the filexfer nodes is possible and will allow you to edit files. However, running applications for testing or debugging is not permitted on these machines, so anything beyond editing will need to be done with dedicated compute resources. Briefly, the general procedure is as follows: Specifics with Example ¶ Step 1: Set up SSH key authentication This step needs to be performed one time only. Once you've set up SSH keys, they will persist in your environment for subsequent sessions. The hostname for the file transfer nodes is filexfer.hpc.arizona.edu . Once this process is complete, it will allow VS Code to directly connect to the HPC systems without using passwords or Duo authentication (which may cause connection issues). We have detailed documentation for setting up SSH keys on the bastion host here: SSH Keys . Follow the procedure documented on that page, but replace hpc.arizona.edu in any commands with filexfer.hpc.arizona.edu . Step 2: Connect to the HPC VPN ( vpn.hpc.arizona.edu ) Connect to the HPC VPN, preferably with Cisco AnyConnect. For detailed information on connecting to the HPC VPN, see VPN - Virtual Private Network . Note that The HPC VPN is needed to connect directly to a compute node. This differs from the standard UArizona VPN or campus network which are not sufficient. The HPC VPN is vpn.hpc.arizona.edu . Step 3: Start an HPC job for the length of time that you’d like to connect VS Code This can either be done directly on the command line, or you can start an Open OnDemand graphical job so you don't have to worry about timing out due to inactivity Note that starting a job either on Ocelote or ElGato will likely get you though the queue faster. As an example, from the command line we could request an eight-hour session with: Interactive session on the command line ``` (ocelote) [netid@wentletrap ~]$ interactive -a hpcteam -n 4 -t 8:00:00 Run \"interactive -h for help customizing interactive use\" Submitting with /usr/local/bin/salloc --job-name=interactive --mem-per-cpu=4GB --nodes=1 --ntasks=4 --time=8:00:00 --account=hpcteam --partition=standard salloc: Pending job allocation 3293757 salloc: job 3293757 queued and waiting for resources salloc: job 3293757 has been allocated resources salloc: Granted job allocation 3293757 [netid@i16n10 ~]$ hostname i16n10.ocelote.hpc.arizona.edu ``` After the interactive session starts, type hostname , which will give something like i16n10.ocelote.hpc.arizona.edu (shown highlighted in the code above). This is the name that you will enter in your local VS Code as the remote computer to connect to. Different jobs are allocated different nodes Note that each time you start an interactive session you will likely get a different node, and will therefore need to tell VS Code the specific host to connect to for each unique connection. Step 4: Open a remote connection in VS Code Once your session is running and you have the hostname of your compute node, you can connect VS Code directly to that machine. As an example, open a new window in VS Code on your local computer and select >< from the bottom-left side Select Connect to Host... , Select +Add New SSH Host... Then enter the ssh connection information (replacing netid with your own NetID). In this case, the command with the specific hostname would be ssh netid@i16n10.ocelote.hpc.arizona.edu . You will be prompted to select the configuration file to update, this will typically be something like ~/.ssh/config . If prompted, allow the connection by selecting Allow in the next window that pops up. In the bottom right, you should now see a tile with a button prompting you to connect. If a connection has been established, you should now be able to select Open Folder , then can enter the full path to your desired working directory. This may be under your /home , /groups , or /xdisk . If everything has gone well, you should now be able to work with your files Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/popular_software/vscode_remote_connection/",
      "title": "VSCode Remote Connection - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "b8dc8982-04a0-498c-828d-f7e066b02e91",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/popular_software/vscode_remote_connection/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Python Modules ¶ Overview ¶ Different versions of Python are available on HPC both as system modules as well as system software on each compute node. Installation and Package Policy ¶ We maintain a two tiered approach to Python packages: Tier 1: We install the basic Python packages that are required by most users (these are mostly libraries rather than packages, such as numpy and scipy). This is done for the versions of Python that we install as modules. Tier 2: For packages that we do not provide, or updates to the versions we do, we STRONGLY recommend the use of virtual environments, which is detailed below and provides a custom and easy to use personal Python environment. Available Python Versions ¶ Python 2 is no longer officially supported by the Python Software Foundation. Using the python command The command python defaults to the system 2.7.5 version. To use Python 3, use the command python3 . Multiple versions of Python are available on HPC. They are only available on compute nodes and are accessible either using a batch submission or interactive session. | Version | Accessibility | | --- | --- | | Python 2.7.5 | system version (no module) | | Python 3.6.8 | system version (no module) | | Python 3.6.5 | module load python/3.6/3.6.5 | | Python 3.8.2 | module load python/3.8/3.8.2 | | Python 3.9.10 | module load python/3.9/3.9.10 | | Python 3.11.4 | module load python/3.11/3.11.4 | Installing Python Packages Using a Virtual Environment ¶ Virtual environment tips Useful overview of virtualenv and venv: InfoWorld Article: Python virtualenv and venv do's and don'ts In the following instructions any module commands have to be run from an interactive session on a compute node HPC Python Virtualenv - YouTube University of Arizona UITS Research Technologies 98 subscribers HPC Python Virtualenv University of Arizona UITS Research Technologies Search Info Shopping Tap to unmute If playback doesn't begin shortly, try restarting your device. You're signed out Videos you watch may be added to the TV's watch history and influence TV recommendations. To avoid this, cancel and sign in to YouTube on your computer. CancelConfirm Share Include playlist An error occurred while retrieving sharing information. Please try again later. Watch later Share Copy link Watch on 0:00 0:00 / 10:59•Live • Watch on YouTube One of the best things about Python is the number of packages provided by the user community. On a personal machine, the most popular method today for managing these packages is the use of a package manager, like pip . Unfortunately, these may require root access preventing you from being able to successfully install the libraries you need. There is an easy solution, however. You can use a virtual environment to create a personal Python environment that will persist each time you log in. There is no risk of packages being updated under you for another user and allows greater control over your environment. Set up your virtual environment in your account. This step is done one time only and will be good for all future uses of your Python environment. You will need to be in an interactive session to follow along. Note: In the commands below, </path/to/virtual/env> is the path to the directory where all of your environment's executables and packages will be saved. For example, if you use the path ~/mypyenv , this will create a directory in your home called mypyenv . Inside will be directories bin , lib , lib64 , and include . Python Version < 3.8 Python Version ≥ 3.8 ``` module load python/ virtualenv --system-site-packages ``` ``` module load python/ python3 -m venv --system-site-packages ``` To use your new environment, you'll need to activate it. Inside your virtual environment, there's a directory called bin that has a file called activate . Sourcing this will add all of the paths needed to your working environment. To activate, run the following, replacing </path/to/virtual/env> with the path specific to your account: ``` source /bin/activate ``` 3. Once your environment is active, you can use pip to install your Python packages. You should first upgrade to the latest version of pip. For example, to add the pycurl package to the virtual environment: ``` pip install --upgrade pip pip install pycurl ``` 4. That's it! As long as your virtual environment is active, you will have access to the packages you have installed. Virtual environments deactivate when you log out, so for each subsequent session or in batch jobs, you will just need to reactivate the environment to get access to your packages: ``` module load python/ source /bin/activate ``` Note: 1. Always use the same <version> as the one you used to create your environment 2. Use the module load command before running your source command. If you activate your environment first, you will get a library error. Custom Jupyter Kernel ¶ Warning The default version of Python available in an OnDemand Jupyter Notebook is 3.8.2. If you would like to create a virtual environment using a standard Python module, you will need to use Python version 3.8.2. If you want to use a different version of python, you can use Mamba . If you want to make one of your virtual environments available for use in one of our Open OnDemand Jupyter Notebooks, you can do so by creating a custom kernel. To do this, start an interactive terminal session and activate your environment (if you do not have an environment, refer to the sections above on how to do so): ``` module load python/3.8/3.8.2 source /bin/activate ``` Once your environment is ready to go, pip-install jupyter and create your own custom kernel. The --force-reinstall flag will allow you to install the jupyter package in your local environment and will not affect the system version. This will create the directory ~/.local/share/jupyter/kernels/ in your account. In the following commands, replace <your_environment> with the name of your own environment: ``` pip install jupyter --force-reinstall ipython kernel install --name --user ``` Once you've successfully created your kernel, go to Open OnDemand and start a Jupyter Notebook. Once the session starts, open it and click the \"new\" dropdown menu in the upper right. If everything is working correctly, you should see your custom kernel's name. For example, if the custom kernel's name was py38-env : Once you've selected your environment, try loading a custom package you've installed to check that everything is working as expected. In this example, we'll check with the non-standard package emoji which has been installed in this environment: Loading Modules in Jupyter ¶ In OnDemand Jupyter sessions, accessing HPC software modules directly from within a notebook can be challenging due to system configurations. However, it's still possible to access these modules when needed. For instance, machine learning packages like TensorFlow or PyTorch often require additional software modules such as CUDA for GPU utilization. To access software modules in your Jupyter notebooks, follow the steps below: Step 1: If you haven't already done so, create a custom kernel for your Jupyter notebook environment. Step 2: You will then need to edit your kernel configuration file kernel.json which is what sets up your environment at runtime. This file can be found in the following location, where <kernel_name> is a placeholder for the name you gave your kernel when it was created: ``` $HOME/.local/share/jupyter/kernels/ /kernel.json ``` Step 3: Next, you will need to modify your kernel's configuration by editing this file. Start by opening it with a text editor, for example nano $HOME/.local/share/jupyter/kernels/<kernel_name>/kernel.json . The contents of this file should look something like the following: ``` { \"argv\": [\\ \" /bin/python\",\\ \"-m\",\\ \"ipykernel_launcher\",\\ \"-f\",\\ \"{connection_file}\"\\ ], \"display_name\": \" \", \"language\": \"python\", \"metadata\": { \"debugger\": true } } ``` The part you need to change is the section under argv . We will change this from executing a Python command to a Bash command with a module load statement. Make a note of the path </path/to/your/environment>/bin/python to use in the edited file. The edited file should look like the following: ``` { \"argv\": [\\ \"bash\",\\ \"-c\",\\ \"module load ; /bin/python -m ipykernel_launcher -f {connection_file}\"\\ ], \"display_name\": \" \", \"language\": \"python\", \"metadata\": { \"debugger\": true } } ``` Replace <your_modules_here> with the modules you would like to load and </path/to/your/environment>/bin/python with the path to your environment's python. Step 4: Save the kernel.json file and restart your Jupyter notebook session. Was this page informative?",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/popular_software/python/",
      "title": "Python - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "b9e6cbaa-e062-4ca4-9c97-1bbd4e9eb42f",
      "viewport": [
        "width=device-width,initial-scale=1",
        "width=device-width, initial-scale=1"
      ],
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/popular_software/python/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Matlab ¶ Overview ¶ There are three common ways to run Matlab: Using the Matlab graphical application through Open OnDemand . Using a module to start a graphical mode in an Open OnDemand virtual desktop. The command line version using modules. This is the most common as you will typically submit a job using Slurm. Like any other application, Matlab has to be loaded as a module before you can use it. To see all the installed versions of the Matlab, use the command module avail matlab . Logging Into Mathworks ¶ With some of the latest versions, it's necessary to interactively log into Mathworks to access Matlab. This only needs to be done once using your university credentials. After your credentials are validated, they will be stored, allowing you to run batch jobs To start, open an interactive session , load Matlab, and start an interactive instance: Load and start Matlab ``` [netid@cpu37 ~]$ module load matlab/r2023b [netid@cpu37 ~]$ matlab MATLAB is selecting SOFTWARE OPENGL rendering. Please enter your MathWorks Account email address and press Enter: ``` at the prompt, enter your university email address. For example: Enter your university password and go to URL ``` Please enter your MathWorks Account email address and press Enter: netid@arizona.edu You need a one-time password to sign in. To get a one-time password, follow these steps: 1. Go to https://www.mathworks.com/mw_account/otp 2. Enter your MathWorks Account email address. 3. Copy the generated one-time password. 4. Return here and enter the password. Enter the one-time password: ``` Next, go to the URL they provide (highlighted above) and enter your university email address again This will take you through the usual university WebAuth process. Once this is completed, you will be given a temporary code: Copy this code to your clipboard and paste it into your terminal: Enter one time password ``` Enter the one-time password: 12345 < M A T L A B (R) > Copyright 1984-2023 The MathWorks, Inc. R2023b Update 6 (23.2.0.2485118) 64-bit (glnxa64) December 28, 2023 To get started, type doc. For product information, visit www.mathworks.com. Using 1 thread(s) on compute node. ``` Once the process is complete, you should be able to use Matlab as usual. Running Matlab Analyses in Batch ¶ The typical procedure for performing calculations on UArizona HPC systems is to run your program non-interactively on compute nodes using a batch submission. The easiest way to run Matlab non-interactively is to use input/output redirection. This method uses Linux operators < and > to point Matlab to the input file and tell where to write the output (see the example script below). The other method is to execute a statement using the -r flag. For details, refer to the manual page for the matlab command . An example batch script might look like the following: ``` !/bin/bash SBATCH --job-name=matlab SBATCH --account=group_name SBATCH --partition=standard SBATCH --ntasks=20 SBATCH --nodes SBATCH --mem-per-cpu=5gb SBATCH --time=01:00:00 module load matlab/ matlab -nodisplay -nosplash < script_name.m > output.txt ``` The options -nodisplay and -nosplash in the example prevent Matlab from opening graphical elements. To view the full list of options for the matlab command, load the Matlab module and type matlab -h at the prompt. Alternatively, use the link above to see the manual page on the MathWorks website. Parallel Computing Toolbox ¶ Temporary Files ¶ By default, Matlab PCT will dump files to ~/.matlab/<MATLAB_VERSION> . This causes problems when multiple Matlab PCT jobs are running simultaneously. Users should always define the environment variable MATLAB_PREFDIR so each job uses a unique temporary folder. Files there will be cleaned after the job finishes. For example: ``` export MATLAB_PREFDIR=$(mktemp -d ${SLURM_JOBTMP}/matlab-XXXX) ``` Matlab and Slurm Resource Requests ¶ If you are trying to run Matlab in parallel interactively, you may encounter the following error: ``` Starting parallel pool (parpool) using the 'local' profile ... Error using parpool (line 149) You requested a minimum of workers, but the cluster \"local\" has the NumWorkers property set to allow a maximum of 1 workers. To run a communicating job on more workers than this (up to a maximum of 512 for the Local cluster), increase the value of the NumWorkers property for the cluster. The default value of NumWorkers for a Local cluster is the number of physical cores on the local machine. ``` This is caused by an interaction between Slurm and Matlab. To resolve this issue, when requesting <n> cores for your interactive job, you will need to set Slurm's --ntasks directive to 1 and --cpus-per-task to the number of cores you need. For example: ``` $ salloc --nodes=1 --ntasks=1 --cpus-per-task=6 --mem-per-cpu=5GB --time=01:00:00 --job-name=interactive --account= --partition=standard ``` External Resources ¶ If you are getting started with Matlab or think there might be a better way, check out the training resources. | Resource | Link | | --- | --- | | Self-Paced Online Courses | https://matlabacademy.mathworks.com/ | | Matlab Parallel Server | https://www.mathworks.com/products/matlab-parallel-server.html#resources | | Natural Language Processing | https://www.mathworks.com/discovery/natural-language-processing.html | | Matlab Videos | https://www.mathworks.com/videos.html | Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/popular_software/matlab/",
      "title": "Matlab - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "ce15d16e-13a7-4ddb-90e1-b6a56a6f923a",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/popular_software/matlab/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Common Datasets ¶ We host several large community datasets. It is beneficial to you and us. For you, it saves all that time downloading and filling up your storage allocation. And for us it reduces the occurrence of the same data in many places. We do not currently update them on any particular cadence. You can request updates if you feel those would be useful to the community. These datasets and databases are available on the compute nodes under /contrib/datasets in read-only mode. AlphaFold ¶ AlphaFold is an AI system developed by the Google DeepMind project to predict the 3D structure of a protein from its amino acid sequence. AlphaFold needs multiple datasets to for inference, the combined size of which is around several TBs. We host the datasets for both AlphaFold 2 & 3. You can find these datasets under /contrib/datasets/alphafold/2.3.0 and /contrib/datasets/alphafold/3.0.0 . For more information on using these datasets with the provided AlphaFold modules, see AlphaFold . If you want to use these datasets with other software, you can access these paths from an interactive session or a batch script. Llama 2 ¶ Meta has open-sourced Llama2 , a group large language models, and it is available for free for research uses, under a community license . These models range from tens of GBs to hundreds of GBs in size. We have downloaded them and made them available for you at /contrib/datasets/llama2 and /contrib/datasets/llama2-hf . The latter are compatible with the HuggingFace ecosystem. You do not need to copy these models to another location to use them. You can directly access those locations from an interactive session or an batch script. However, if you copy them to another location, then also copy the license files, present in the directories mentioned above. The following Python code shows how you can load the HuggingFace compatible models. ``` from transformers import LlamaForCausalLM, LlamaTokenizer llama_path = \"/contrib/datasets/llama2-hf/meta-llama_Llama-2-7b-chat-hf\" tokenizer = LlamaTokenizer.from_pretrained(llama_path) model = LlamaForCausalLM.from_pretrained(llama_path) ``` You can run the above code from either a Jupyter notebook (see Open OnDemand ), or a batch script. If you do not provide enough memory, then the Jupyter kernel will die, or the Slurm job will terminate with an out of memory (OOM) error message. We recommend that you request at least 60 GB of memory for the models with 7 billion parameters, and progressively more for the models with 13 billion and 70 billion parameters. For more information on using Llama2, we recommend that you check Llama2's Github repository and HuggingFace documentation . NCBI BLAST ¶ The Basic Local Alignment Search Tool (BLAST), developed by NCBI, is a program for comparing primary biological sequence information, such as amino-acid or nucleotide sequences. NCBI has several databases that you can use with BLAST. We have the nr , nt , and taxdb databases available locally under /contrib/datasets/blast . You can use these datasets with the blast module. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/common_datasets/",
      "title": "Common Datasets - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "b1b75371-698e-40ee-9393-b37f905e95ae",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/common_datasets/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content R ¶ R is a popular language for data analysis and visualization. Different versions are available as software modules and we provide the graphical interface RStudio for R through our Open OnDemand web interface. Similar to other languages that use package managers to install libraries contributed by the user community, we recommend you create and manage your own local libraries in your account. This ensures a stable global environment for all users and that you have the most control over your packages' versions and dependencies. We provide instructions below for how to create, use, and switch between libraries as well as some debugging techniques for when package installations fail. RStudio is a popular method for running analyses (and for good reason!), but for longer-running jobs (say, many hours or days) or workflows that need more flexibility in their environment (e.g., need access to software installed as system modules such as gdal), we recommend batch submissions. Creating a Custom Library ¶ R Package Debugging R packages can be finicky. See Switching Between Custom Libraries and Common Problems below to help with frequent user issues. Alternatively, you can consider using Mamba to manage your R packages. Creating your first library Make a local directory to store your packages. It's recommended to include information about your library in the name, e.g., which version of R you're using. It's important to maintain a consistent version of R with your libraries since installing packages into one library with multiple versions or R will often cause trouble. Need to switch versions of R? Creating a new library will help. ``` mkdir -p ~/R/library_4.2 ``` Tell R where your library is by creating an environment file ``` echo 'R_LIBS=~/R/library_4.2/' >> ~/.Renviron ``` The file `~/.Renviron` [is a \"dot\" file](https://hpcdocs.hpc.arizona.edu/support_and_training/cheat_sheet/#hidden-files-and-directories) which means it does not show up when you run a standard `ls`. This particular file can be used to control your R environment for each subsequent time you start a session. All the `echo` command does is append the line `R_LIBS=~/R/library_4.2/` to this file. That's it! Now you can install packages and they will be stored in the directory you just created. For example, to install and load the package ggplot2 : ``` module load R/4.2 R install.packages(\"ggplot2\") ``` Switching Between Custom Libraries ¶ If you're switching versions of R, we recommend you use a different library. See Common Problems below for more information. When creating a library, consider including pertinent information in the name such as R version. For example, if you were previously using R 4.2 and wanted to switch to using R 4.1, you could create a directory called library_4.1 using: ``` mkdir -p ~/R/library_4.1 ``` Then, to use your new library, edit your ~/.Renviron file using a text editor such as nano : ``` nano ~/.Renviron ``` Once your text editor opens, set the R_LIBS previously defined in your file to the name and location of your new library. In this case, this would look like: ``` R_LIBS=~/R/library_4.1 ``` To exit nano , use Ctrl + X and save at the prompt. Once your file is saved, you're ready to start installing files into your new library. Common Problems and How to Debug Them ¶ Working on a cluster without root privileges can lead to complications. For general information on package installations, see the r-bloggers documentation . For information on common installation problems on our clusters, see the section below with with suggested solutions: Anaconda A Corrupted Environment Library Issues Mixing R Versions Open OnDemand RStudio Issues One common reason R packages won't install is an altered environment. This can frequently be caused by the presence of Anaconda (or Miniconda) installed locally or initialized in your account from our system module. We have instructions on how to remove Anaconda from your environment in our Anaconda documentation . If Anaconda is not initialized in your account, there might be other culprits that are corrupting your environment. Look for any of the file types listed below on your account. If you find them, try removing them (make a backup if you need them) and try the installation again. Saved R sessions. If this is the case, after starting a session, you will get the message [Previously saved workspace restored] . Old sessions may be saved as a hidden file .RData in your home directory. Alternatively, they may be stored under ~/.local/share/rstudio . Where old sessions are stored is dependent on the version of R you are using. Gnu compilers Windows files Have you set up a custom library? Are you switching between custom libraries? You may want to check that everything is being loaded from the correct location and that there are not multiple or unwanted libraries being used. Double-check that you have an .Renviron file. This is a hidden file located in your home directory and should set the path to your custom R library. If you do not have a custom library name set up, R will create one for you saved as something like: ``` ~/R/x86_64-pc-linux-gnu-library ``` This directory can lead to unwanted behavior. For example, if you're trying to use a new custom library (such as when switching R version), R will still search x86_64-pc-linux-gnu-library for package dependencies and may cause installs to fail. To fix this, rename these types of folders something unique and descriptive. To set up/switch custom libraries, follow the instructions in the Creating a Custom R Library section above. Because HPC is a cluster where multiple versions of R are available, users should take care to avoid mixing and matching. Because packages often depend on one another, libraries using different versions of R can turn into a tangled mess. Common errors that can crop up include: Error: package or namespace load failed. If you're switching R versions and have a custom library defined in your ~/.Renviron file, we recommend creating a new library. RStudio is a great tool! Sometimes though, because it's a different environment than working directly from the terminal, you may run into problems. Specifically, these typically arise for installs or when using packages that rely on software modules. Package Installations If you're trying to install a package in an OOD RStudio session and you've tried all the troubleshooting advice in the other tabs without luck, try starting R in the terminal and give the installation another try. You can access an R session in the terminal by first starting an interactive session , then using: ``` $ module load R/ $ R install.packages(\"package_name\") ``` Accessing Modules RStudio does not natively have access to module load commands. This means that if you have a package that relies on a system module, the easiest option is to work through an interactive terminal session or to submit a batch script . The alternative is to create a hidden directory in your account called ~/.UAz_ood/rstudio.sh with the module load commands you need. More information on this can be found in the section Loading Modules in RStudio below. Font Issues RStudio uses Apptainer under the hood. As a result, there are some environment differences that may affect correct font formatting in images generated in RStudio. If you are experiencing this, add the following line to the hidden file ~/.Renviron in your account (you can create this file if it does not exist): ``` FONTCONFIG_PATH=/opt/ohpc/pub/apps/fontconfig/2.14.2/etc/fonts ``` Using RStudio ¶ Jupyter R is one of the core languages that Jupyter supports. You can also use Jupyter as an alternative GUI to RStudio. For more information, see Mamba . Open OnDemand Apptainer/Singularity We provide access to the popular development environment RStudio through our Open OnDemand web interface. This is a very handy tool, though it should be noted that it is a less flexible environment than using R from the command line. This is because RStudio sets its own environment which prevents easy access to third party software installed as system modules. These issues can sometimes worked around by following the guide in the debugging section above. In some circumstances, you may want to run RStudio using your own Apptainer (rebranded from Singularity) image. For example, this allows access to different versions of R not provided when using our OOD application. We have some instructions on one way to do this below. First, log into HPC using an Open OnDemand Desktop session and open a terminal. A Desktop session is the easiest solution to access RStudio since it eliminates the need for port forwarding. In the terminal, make an RStudio directory where all of the necessary files will be stored. In this example, we'll be working in our home directory and will pull an RStudio image from Dockerhub to use as a test. If you're interested, you can find different RStudio images under rocker in Dockerhub . ``` mkdir $HOME/RStudio cd $HOME/RStudio apptainer pull ./geospatial.sif docker://rocker/geospatial.sif ``` Next, create the necessary directories RStudio will use to generate temporary files. You will also generate a secure cookie key. ``` TMPDIR=$HOME/RStudio/rstudio-tmp mkdir -p $TMPDIR/tmp/rstudio-server uuidgen > $TMPDIR/tmp/rstudio-server/secure-cookie-key chmod 600 $TMPDIR/tmp/rstudio-server/secure-cookie-key mkdir -p $TMPDIR/var/{lib,run} ``` Next, create a file in your RStudio directory called rserver.sh and make it an executable: ``` touch rserver.sh chmod u+x rserver.sh ``` Open the file in your favorite editor and enter the content below. Modify the variables under USER OPTIONS to match your account if necessary. You can change PASSWORD to any password you'd like to use. Once you've entered the contents, save and exit: ``` !/bin/bash --- USER OPTIONS --- WD=$HOME/RStudio SIFNAME=geospatial.sif PASSWORD=\"PASSWORD\" --- SERVER STARTUP EXECUTED BELOW --- NETID=$(whoami) TMPDIR=$WD/rstudio-tmp SIF=$WD/$SIFNAME PASSWORD=$PASSWORD apptainer exec -B $TMPDIR/var/lib:/var/lib/rstudio-server -B $TMPDIR/var/run:/var/run/rstudio-server -B $TMPDIR/tmp:/tmp $SIF rserver --auth-none=0 --auth-pam-helper-path=pam-helper --server-user=$NETID --www-address=127.0.0.1 ``` Now, in your desktop session's terminal, execute the rserver.sh script using ./rserver.sh Next, open a Firefox window and enter localhost:8787 for the URL. In your browser, you will be prompted to log into your RStudio server. Enter your NetID under Username. Under Password, enter the password you defined in the script server.sh. This will open your RStudio session: Loading Modules in RStudio (New!) ¶ Library installations still must be performed on the command line The method detailed below is used to faciliate loading R libraries in RStudio when they depend on system software modules. R library installations will still need to take place in an interactive session on the command line. If you are using the RStudio application in Open OnDemand, it is now possible to load additional software modules into your environment. You might want to do this if your R libraries depend on modules. An example of this might be the Seurat package which depends on the modules gdal, proj, sqlite3, and geos. Method to Load Modules To load modules, you will need to create a hidden directory in your home called ~/.UAz_ood . Inside that directory, create a file called rstudio.sh . You can then add any module load statements you need to this file. The file rstudio.sh is sourced when your RStudio session initiates, so if you modify your the file, you will need to start a new RStudio session for the changes to take effect. An example of how to create this file with example contents is shown below. Example This example assumes you are working on the command line. Start by first creating the necessary directory and file: ``` mkdir -p ~/.UAz_ood touch ~/.UAz_ood/rstudio.sh ``` Next, open rstudio.sh in your favorite text editor. If you are not familiar with command line text editors, a beginner-friendly tool is nano. For example: ``` nano ~/.UAz_ood/rstudio.sh ``` Next, in the file add the modules that you would like to load in RStudio. For example: ``` module load gdal/3.8.5 geos/3.9.5 proj/9.4.0 sqlite/3.45 ``` Now, save and exit. If you're using nano, you can do this with Ctrl + X , then select Y to save your changes and exit. Once your file exists with the desired contents, start an new OnDemand RStudio session. Setting a New User State Directory ¶ When working on a large project in RStudio, it is possible for your R session's data to fill up your home directory resulting in out-of-space errors (e.g. when trying to edit files, create new OOD sessions, etc). With the newest version of RStudio, you can find these saved session files under ~/.local/share/rstudio . To preserve space in your home, you can specify a different directory by setting the environment variable RSTUDIO_DATA_HOME . To do this, open the hidden file ~/.bashrc and add: ``` export RSTUDIO_DATA_HOME= ``` where </path/to/new/directory> is the path to a different location where you have a larger space quota. For example, /groups/<YOUR_PI>/<YOUR_NETID>/rstudio_sessions . Setting Your Working Directory ¶ Current Session All Non-Project Sessions If you'd like to change your working directory in an RStudio session, one option is to use setwd(\"/path/to/directory\") in your terminal. Alternatively, if you'd like to see the contents of your new workspace in your file browser, you can navigate to the Session dropdown tab, navigate to Set Working Directory , and click Choose Directory... From there, either navigate to the desired subdirectory, or click the ellipsis ... in the upper right to enter the full path to a directory. Once you click OK and then Choose in the main file navigation window, R will change its working directory and you should see the contents of your new space under the Files browser in the lower right. If you'd like to permanently set a different default working directory for all non-project RStudio sessions, navigate to the Tools dropdown tab and select Global Options... This will open a menu where you can set your default working directory under General . Click the Browse... button to open a file navigator To select a new working directory, either navigate to the subdirectory of your current working space, or select the ellipsis ... in the upper right to allow you to enter the full path. The ellipsis option allows for more flexibility such as pointing to an /xdisk or /groups space. Next, click OK , then Choose in the Choose Directory window, then Apply in the Global Options menu. This will set your working directory for your current session as well as all future sessions. Popular Packages ¶ Below, we document some installation instructions for common R packages. We attempt to keep these instructions reasonably up-to-date. However, given the nature of ongoing software and package updates, there may be discrepancies due to version changes. If you notice any instructions that don't work, contact our consultants and they can help. Alternative installation The instructions below show how you can install these packages with the R modules described above. An alternative is to install these packages in a Conda environment with a package manager like Mamba. For more information, see Mamba . Seurat and SeuratDisk Monocle3 Terra R Studio Version If you use RStudio for your analyses, make sure that you load the same version of R when working with modules on the command line. Anaconda must be removed from your environment You will need to make sure Anaconda is completely removed from your environment prior to the install. If you have Anaconda initialized in your account, see the code block on our Anaconda page under Removing Anaconda From Your Environment --> Temporary Removal . Installs must be done in a terminal To install Seurat and SeuratDisk, you'll need to be in an interactive terminal session and not in an RStudio session. Once your installation is successful, it is possible to load these modules in RStudio. More details are provided below. Seurat SeuratDisk ``` (elgato) [netid@junonia ~]$ interactive -a [netid@cpu38 ~]$ module load R/ [netid@cpu38 ~]$ module load gdal/3.3.2 glpk/5.0 libpng/1.6.37 # software modules that are needed for Seurat's dependencies [netid@cpu38 ~]$ R install.packages(\"Seurat\") ``` If you want to load this software in an RStudio session, you will need to create the file ~/.UAz_ood/rstudio.sh . See the Loading Modules in RStudio section above for more information. SeuratDisk is similar to Seurat with a few more dependencies. It also includes the line unset CPPFLAGS due to a reported issue with the dependency hdf5r : ``` (elgato) [netid@junonia ~]$ interactive -a [netid@cpu1 ~]$ module load R/ gdal/3.3.2 geos/3.9.1 hdf5/1.10.5 libpng/1.6.37 glpk/5.0 libgit2/1.8.1 [netid@cpu1 ~]$ unset CPPFLAGS [netid@cpu1 ~]$ R install.packages(\"Seurat\") install.packages(\"remotes\") remotes::install_github(\"mojaveazure/seurat-disk\") ``` If you want to load this software in an RStudio session, you will need to create the file ~/.UAz_ood/rstudio.sh . See the Loading Modules in RStudio section above for more information. R Studio Version If you use RStudio for your analyses, make sure that you load the same version of R when working with modules on the command line. Installs must be done in a terminal To install Monocle3, you'll need to be in an interactive terminal session and not in an RStudio session. Once your installation is successful, it is possible to load these modules in RStudio. More details are provided below. Anaconda must be removed from your environment You will need to make sure Anaconda is completely removed from your environment prior to the install. If you have Anaconda initialized in your account, see the code block on our Anaconda page under Removing Anaconda From Your Environment --> Temporary Removal . Monocle3's documentation includes steps that you can use for a successful installation. ``` (elgato) [netid@junonia ~]$ interactive -a your_group [netid@cpu1 ~]$ module load R/ gdal/3.3.2 [netid@cpu1 ~]$ R install.packages(\"BiocManager\") BiocManager::install(c('BiocGenerics', 'DelayedArray', 'DelayedMatrixStats', 'limma', 'lme4', 'S4Vectors', 'SingleCellExperiment', 'SummarizedExperiment', 'batchelor', 'HDF5Array', 'terra', 'ggrastr')) install.packages(\"remotes\") remotes::install_github('cole-trapnell-lab/monocle3') ``` Then, to load Monocle3 in RStudio, you will need to create the file ~/.UAz_ood/rstudio.sh as detailed in the Loading Modules in RStudio section above. R Studio Version If you use RStudio for your analyses, make sure that you load the same version of R when working with modules on the command line. Installs must be done in a terminal To install Monocle3, you'll need to be in an interactive terminal session and not in an RStudio session. Once your installation is successful, it is possible to load these modules in RStudio. More details are provided below. Anaconda must be removed from your environment You will need to make sure Anaconda is completely removed from your environment prior to the install. If you have Anaconda initialized in your account, see the code block on our Anaconda page under Removing Anaconda From Your Environment --> Temporary Removal . To install the R package terra , you will need to load the module gdal which will pull in other dependencies ( geos , proj , and sqlite ). In this example, we'll use the modules R/4.3 and gdal/3.8.5 ``` (elgato) [netid@junonia ~]$ interactive -a [netid@cpu1 ~]$ module load R/ gdal/3.8.5 [netid@cpu1 ~]$ R install.packages(\"terra\") ``` Then, to load Terra in RStudio, you will need to create the file ~/.UAz_ood/rstudio.sh as detailed in the Loading Modules in RStudio section above. Example Jobs ¶ Specify your R version Note that it's always a good idea to explicitly specify the version of R you want to use when loading an R module in a batch script. When modules are updated, the default changes to the most recent version. This may create conflicts and library issues. Always specifying your R version is a good way to ensure a consistent environment. Batch job basics The examples below assume a basic familiarity with batch scripts. If you've never submitted a batch script before, check out our Introduction to Batch Jobs for a comprehensive walkthrough. Below are some examples on how to submit R analyses as batch jobs. R jobs may also be run using interactive terminal sessions or using RStudio through Open OnDemand . Basic R Example Plotting in a Batch Script R Array Jobs Download Example To run an R job as a batch submission, you can include all of your R commands in a single file and execute it with the command Rscript . R Script To start, we'll create a simple R script that will print the line Hello World! . We'll call this script hello_world.r and can create it using the command touch : ``` [netid@wentletrap ~]$ touch hello_world.r ``` Now, open the file in your favorite text editor and add the R commands that you want to run in your job. In this case, we'll use: ``` !/usr/bin/env Rscript hello_string <- \"Hello World! \" print (hello_string, quote=FALSE) ``` Slurm Script To call this R script in a batch submission, we'll load the R version we'd like to use and then execute our workflow with the command Rscript hello_world.r . Create a batch submission file using the same method as before, touch submit_r_script.slurm , and open it in your favorite text editor. Then, in your Slurm submission file, add the following: ``` !/bin/bash SBATCH --job-name=R-Plotting-Job SBATCH --ntasks=1 SBATCH --nodes=1 SBATCH --time=00:01:00 SBATCH --partition=standard SBATCH --account=YOUR_GROUP module load R/4.0.0 Rscript hello_world.r ``` Submitting your job To run the batch job, submit it to the scheduler using the command sbatch followed by the Slurm file’s name. This will return a job ID that you can use to track the status of your job. In this case: ``` [netid@wentletrap ~]$ sbatch submit_r_script.slurm Submitted batch job 53341 ``` Output An output file will be generated by the scheduler containing any output that would have been printed to the terminal had you run your batch script interactively. ``` [netid@wentletrap ~]$ cat slurm-53341.out [1] Hello World! ``` Download Example Creating and saving figures as a part of your workflow is possible in a batch script. You can save your figure to a specified directory (the default is your working directory), give it a custom name, control your image quality and dimensions, and choose your output format (e.g., pdf, png, jpg, etc.). An example is included below. RScript To start, we'll write an R script that will create a sinusoidal plot. We'll call this script example.r and can create it using the command touch : ``` [netid@wentletrap ~]$ touch example.r ``` Now open the text file in your favorite editor and add the following: ``` print (\"In R Script. Plotting...\") x <- seq(-pi,pi,0.1) png(\"rplot.png\") plot(x, sin(x)) dev.off() ``` In the example above, we're using png(\"rplot.png\") to save our output figure to png format rather than displaying the image interactively and the dev.off() closes the file after it is generated. Batch Script Next, we'll create our submission file using touch submit_r_script.slurm and add the contents: ``` !/bin/bash SBATCH --job-name=R-Plotting-Job SBATCH --ntasks=1 SBATCH --nodes=1 SBATCH --time=00:01:00 SBATCH --partition=standard SBATCH --account=YOUR_GROUP module load R/4.0.0 Rscript example.r ``` The command RScript is used to execute our R script in batch mode allowing it to be run non-interactively. Job Submission Next, we'll submit our job using sbatch : ``` [netid@wentletrap ~]$ sbatch submit_r_script.slurm Submitted batch job 53337 ``` Output Once our job has completed, we should see both a Slurm output file which contains any text that would have been printed to the terminal had we run our commands interactively, as well as the image file we specified. ``` [netid@wentletrap ~]$ ls slurm-53337.out rplot.png example.r submit_r_script.slurm [netid@wentletrap ~]$ cat slurm-53337.out [1] \"In R Script. Plotting...\" null device 1 ``` The contents of rplot.png should be: Array Jobs Intro Unsure what an array job is? See our Array Jobs documentation which provides in-depth information on their function and implementation. Download Example In this example, we’ll create an R script that generates 1000 randomized 1s and 0s, stores them as a dataframe, then saves the dataframe to an output file. We’ll run this R script as an array job to simulate what a researcher might do if they were performing multiple independent simulations using the same R script. R Script This script is designed to accept command line input using the following syntax: ``` Rscript r_array_example.R $SLURM_ARRAY_TASK_ID ``` Since we’re using the same file for each simulation, if we were to create a single static output filename for our save() command, each simulation would overwrite the one that came before it. To deal with this issue, we’ll make use of the Slurm environment variable $SLURM_ARRAY_TASK_ID to differentiate them. The use of commandArgs() is to pull in that task ID to our R script so that we can use it in our output filenames. Start by creating an R script by using the touch command ``` [netid@cpu4 ~]$ touch save_example.R ``` Next, open the file in your favorite text editor and add the contents: ``` We'll pull in any command line arguments used in executing this script. This is to grab the SLURM_ARRAY_TASK_ID that's associated with this particular array subjob. We'll use this integer to differentiate save files so multiple simulations don't overwrite one another. args<-commandArgs(TRUE) An example of a workflow is included below Generate sample of 1000 random 0s and 1s a <-sample(0:1, 1000, rep = TRUE) Save as data.fame df <- data.frame(replicate(10,sample(0:1,1000,rep=TRUE))) Now we'll save our dataframe to a unique filename. If we run our job with: Rscript r_array_example.R $SLURM_ARRAY_TASK_ID then args[1] will be set to the job's unique integer. Using paste, we can add this into our save filename. filename <- paste(\"random_sample_df_run\", args[1], \".rda\", sep = \"\", collapse = NULL) For demonstration purposes, we'll print out the expected filename sprintf(\"Simulation complete. Saving dataframe to filename: %s\", filename) save(df, file = filename) ``` Submission Script Our submission script includes the usual directives as well as the additional --array=1-2 directive. This tells the scheduler to submit two jobs with array indices 1 and 2. ``` !/bin/bash SBATCH --account=your_group_here SBATCH --partition=standard SBATCH --time=00:01:00 SBATCH --ntasks=1 SBATCH --nodes=1 SBATCH --array=1-2 module load R/4.1.0 Rscript save_example.R $SLURM_ARRAY_TASK_ID ``` Job Submission ``` [netid@cpu4 ~]$ sbatch r_array_example.slurm Submitted batch job 260764 ``` Output We can now see that two jobs were submitted, each with their own Slurm output file as well as their own distinct R output. ``` [netid@cpu4 ~]$ cat slurm-260764_ | grep Simulation [1] \"Simulation complete. Saving dataframe to filename: random_sample_df_run1.rda\" [1] \"Simulation complete. Saving dataframe to filename: random_sample_df_run2.rda\" [netid@cpu4 ~]$ ls random_sample_df_run random_sample_df_run1.rda random_sample_df_run2.rda ``` Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/popular_software/R/",
      "title": "R - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "f54f09de-229f-4b8b-a77a-00b938bcefba",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/popular_software/R/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "�\b/fJa�\u0007TTM��A��i�M��d$�$9J\u0006�(� ��\u0016\u001a\u0004A�� HF�\u0012$\u0007�A2� �%��\u0004A�I$ ���?�μ�ޝ��ܵ.߂��v��S��޻v�S �\bW8\u0012������߅������(�������0�..$\"&x!�3X�\"�B\u0018����\u0018FOXTD *��6���剄y@�X�0\u000f�}'WW'Ͽ�爰E�����!��x�u��}`n\bW{>��5.&XLD� ο ��O�/ *\\|�'*,&�\u0005\u0015�W\u000f���\\|�\u0011 N�H('��;�\u0016���� �����$������}�ҼPO�{��\b' ̟��u0�݁����v�0E��P�E�Ӈ\u0007�����s�:��ޛ\u000f~�.�u�� /�����=�?/� ��B�+W����/&,&v\u0019��\b����ŷ��k��㿠���_⿐��8����x\u0019��\b��tԈ�4\u00171�H���-,,칋\u0010\u000eF�\u0001�)�º���e���^�r\u0005\u0007\u0007\u0007�@<<<\u0010\bD@@@HHHLL\\|��U\b\u0004BBBBJJJFFFNNNAAAIIIEEu��5\u001a\u001a\u001aZZZ:::zzz\u0006\u0006\u0006FFF(\u0014�����������������������������ˋYz���/�C\u0010c\u0018B���\"\"\"bb\u0018�\u0010����������qㆴ�����������������������������������͛7���544455������utttuu������oݺe```hhhdddlllbbbjjjfffnn~���;w�XXXXZZZYYY[[�`0\u001b\u001b\u001b[[[;;;{{��w�:888:::999;;��������������p\u0004q��=\u000f\u000f\u000fOOO$\u0012������}��} __�\u0007\u000f ������?\\|�0 000((�ѣG(\u0014�������!!!���aaaO�< \u000f\u000f���x��idddTTTtttLLLlll\\\\\\|\\|\\|BB³g�\u0012\u0013\u0013������SRRRSS�?��ŋ�����􌌌��̗/_feeegg������������\u0017\u0014\u0014\u0014\u0016\u0016\u0016\u0015\u0015\u0015\u0017\u0017�z�����������������������������������uuu���\\ \\ ���MMM���---���mmmo޼A����� o߾���������~��]OO����{{{������\u0007\u0006\u0006\u0006\u0007\u0007������?\\|�0222:::666>>>11�������O�>MMMMOO���\\|��yvvvnnn~~~aa�˗/���KKK_�~�������������������������������������������~�����pppxxxttt\\|\\|\\|rr������ӳ����sg7�j,,�\u0019ue\u0005C���t_yr��7��v\u0007��/�\u0017K��>���r[�\u00077�6T \u0010�k\u0018�\u0004�*�r_�\u000f/\u0017s6�0'@��\u0006�/\u0012AȌx˽�U����*��Zxnu`��5H��Q�\u0019�٭\u0018�������\u0001��^^Fx�6�86��\u0015�$ ��\u0018��\u000e�' M�\u0017�\u0016d=��N�� ِ#��!-��ꪯ�؜K��#�2\"KS��\\|�u\u0013�'�Q-\u0004��JN�P��l�QE���Y�\u0013f���PS2NJi���Ow,h�?\b�\\�t�2�k�+�������+�\u0018n�\u0007F�I�\\ ��~i�����������\u0016[\\�w̆s �s�J��}\u001a�[xe�\u0001R?�\u0015�_��QR壑0�c [+�\\ m�)����wo/^v��M���α�O:8�3�� ��\u0011v�>���'��!\b�\u0018�3�;�\b�z�� �l��W���~��\u001b\u0017���4\u000e��\\|\u0005 '�������gx2MI��\\|�2�n;O�ܧ�M+�R����!�Ѝy~r\u0011������TL�b\u0016#v��y�Y �Z%G2������i-���\u0014���\"e�# {o\u0005��� L�F�i��@�6�kP���c����sj�/�\u000e�C��\u001b��(�Ư��6jn�&��X׉߯k� �fq @Y`�\u0018aC�L�<��z��Ş\u0005HPN�%�pzM��iG\u000f}'�� �m�^��6\u000f�\u000f�o �\u0017\\|\u0014\u0019�x�to�ǲ�&2� �D�����U�������5�xl��\\ +�n�� Y�q��,H����L�\u000fk�&�Ԟt��g\\\u0017q_7\u0013�����kL_vۼ�H��w�\u0012E�6'\u0018�� �\u0007�9\u001b��K�\u0010�:\u0018�[ɍ����S��³ ���Ы0^�������+.-El�Q����P�Ui�]�iQ�\u00069�T�5��Պ��Ӊ�O��J�2F�ˈ?S�7\u0017/l1\\��sI/�\\ �U�kI�*ɨ�B^q��X���}0GƂјf���\u0004�\u0016���c��W.�8� ̡q\u0016\u000e2�K\u0012�����\u0007����+h���\\�=d�7�/�ˮ���\\�}ԭ7�\b�\"���JDv�Fs!����ݭBjS��^&U���_A\u0019Yoi19�\u001b��'��(4�/�\u0006�\u0019��%K:*pPcs�g�OC�Z�dဌC���Lö� %^}�\u0012d��\u0018qQ�Ѣ�# V'�\u0007���?�\u0007�+�5�c :��.4�/Q�+�`ˠ\u0004Q��j���/Y\\ ��\u0013�� ��)}!�\u0012�/\\ Y��w���u�C�\u0011\u001a�}�om7%+\\\u0010�,\u0001��4MEh��]��\u0017�\b�$#\u0011z�5�C\u001b]S� `8��+� �u���Ҳ\u0015*�\u000e'\u0011�\u000f��`#By���\u0015���x��\u0010I�! �҆�Y��֡�#��o��ɡ.�D�s���f����\u0018��\u0013�l�����V�$��\u0010-��9�1\\��O�bkr�\\|��� ��h�+����\\ ����տ*�K\u0004��L7� \u0014��e�\u0014��\u0013W�R��[n5܆��硦jo#F\bc�^\u0011���\u0014\\ Rg��^עӌ��bD�q/�\u00125ev�\u0013ڶ�Wl�JgQ�zU:V�c�Z �q��R�FH�\u0018��\\|�@� ���C�؟��,x`�#�+�xxl�FP����E\u0013��zk��H��������ǝV� �ԋAq��\\ ^[N�'B�Yr��=��r�s곛U��\u001b�L�D�.Qw\u0013�!�ي7_��D�֊X\u000e�D�S�y� ��jrњ��\u0017%�ycM`:�\u0014!�M\u000e\u0019!��q�'�A 09ZL�\u0001�U\u0010wГ\u000fdoe�D\u0012k\u0006�5+\b\u0014�ʱ\u001bЎ�6�ƽ\u0010�\\|ݼ�L�'C���=��#lO���h�\u0012:%\\ \u0001\u0013RM���֔�Q����5��=+��Q�B\u0014���W��Y�X�8��4�L�V�n�r\u0017kҩ���B\\ 6j,W���`~�\u0011O}7�I�BF4u3��:;��o-k̝��g�x���2���L���m� KPI�Nw-\"B�G�K��\\ W� \u0018�yɊ����׻���\u0014�ƾ\u0013�L�`1s\u0016����j�\u0007/�3�B6��홀���{Y���R���h;\u0011� ���_������d�q\u000e��������^��fӈ\\|E�\u0019uט�n :'�ZƀM�`/}�L\u0004R�?\b1�\u0013'R�p5O�dK��W�N~�\"��0\u0005\u0004�ޝ��\\ É���N�\u0018Y<�2��u\u0005�t��X�X��.��gENÃ��\bj�\u0007\\Q�\u0019� ��$��(V��{]�\u000f���9\u0013�9'^�D`�w%�\u001bTn?�o���L�ȳ\u0006:&\"��u��i\u0005>߮\\|��Qe 0��Jr\u0010�>v\u0018�aj\u0010��e\u0007%u���.�*\\|\u0015�\u0015\\ ^ ~:`�y� \u0014r �/\u00128��%\u001a\u0017��w��2!�\u001b\u001b^bi��>j\\ E��� Ț�F\\�2�a~\u0005��SN����6�!Yd��Uw�\u0013�\u0017�.\u0016ZH�K\u0018A�\u0006���i��H���:R\u0016Ԡ�!�\u0006�=?o�� }\u0004��!�^�_�b����=uE�/\u0014\u0004\u0018�hڒ�\b7͘��\u0006f\u0016�\u0001����M_v��DT ���e�Ф��\\ .i%�\u0016��R�s�<�\\ \u0012\u0019v�[�/�b\u0001lI<��\u0007�v��� �Ц�����l���k1.ԙ\u0019\u000e�8$�1M�zV���b���ן�n�Y�P\u0013�&Hj�%���!\u0007���\\ ����3\u000e��\u0019 �I �\"@�A� Tn���j��{\\ +�����ό3 �\b*tr~TϘ.\u00072^���KFF�7$�;u�dg���#�� �\u0011�m R`׀�#����\u0018�7'Dg]\u0019eqc� ([>\\|�=�q��A����y\u0014G�� ���� �ڷ��'�s�*���խ\u0010��/\u0018h���\\8nF́R{R�7�N%6,�����\b��)��o��T\u0017���+���H\u0007`����E����8�\u000f\\ ���Pk��\\ ��f���Ѕb~�゜X��ׇ��i �1\u0005�\u001b�<\u0016t�>$%�\b��tG=T����}��_�( ��b�`-tj �('Ox��A�m-E׃��Z�O�2�;yh\u000e��a~!c�}��G�\\ \u0013[t��R��ǳ\u0019%5D�B\\ �^������� yJ � �ڟJ� \"mއ��i����<�[�[�EMfaY\u0010:t�f����&�\\|?�h�d7���)\u000f�w�ci�޸��,?t�{Op�b�\u0011���-kq�<*Ψ� 艝1'wjw:�P�`?F�\u0012�� ��O)f5��\u0007\u0013\u0016�\u0006��\u0014�3��2��.�Vf�<�C�}y�v���\u0018�\u0007ΰִ��� %e&� t\u0018T-�~��� +�gwg��m�DR9G9il \u0007pjʼ��� \u0019�\u0001�\\ wRN&\u0014b� �2���3\u000242\u0003�PxQ�\\ �\\ 9�@���\u0006c!�Bf�5�iέ} !�*��`�G���\u0010V?�5�B,T:f gu�lO�K!_A&V��Өk\u001a����\u0019�b-N��4)W> ��\u0018�O�1��\u0015I㚮� \\|n�ɩ�= e�\u0004�u�:���\u0001��$\u0016���3'��҇sR�&\u0011�_u~\\|�����6�̃�瘅)�C~\u0006���8��\u0001 b\"�� A��[)�ՠ9+��%\\ �0�5�B T`�ї\\ q�*b��o�P��\u0019���{Q\u001bk��x��\u0004��ɚ/��F�!��K�����nX\u001b톈IS��rO\u0005M\u0001�i\b� \u0005W��î!�����\u0006@�8�T]\u000e�l=�E\u0005:��׵��Yd�� �H-f�\u001a�\u0013/��ÇD��G� �\"��l�Һ�fm�C�������TQ﵂�\b�Ae B^�\u0001�@����7\b�ԲH#�j�ը����'FS �Djt�RVD+��>��bAK+)!8�\u0015� �\u0017Ae��[\u0004�\u0014\u001b����W��-腷��{�# \u0014� �\u0019\"�x�3�\u0010r.u�N�hrd�2��/x���0�s\\|6ٱ���\u0013~=�A\u0010�H\u0019�\u0012_�]�6��\"�v\u0005�*�@�--�\b�b?��F� �e����\u0006M�B ( T�\u0017oD2\u0004ߞ3u�زA�:�\u0005\u0004Ōy�QTA��x�V:���>� ���h�������������K �\u0018\u001a\u0016��P�N+ Z�����\u0012\\ _�ҴU������� � c��\\ �mкue�Θ\\+ \u000f �\u0010&�ó&0���\u0017\u0001�\\ �Iu���\b 8���y%\u0010�����&\\�� Œ�pR{ɞ\u0016\u0007�Ϫ���P�g���\u0004�\u000e\u001b��,�C ��\"8��cU0 1\u0015+����6>���\u0010 f� \u001a��ΟK� Y\u0004�&����G\u0016�Iea����ÓP\u0011F�zx N���D\b)��\\�8���\u001b�~]�ba�8w\u001a\u0014�&���}n\u000e�\"4*��c�SH��y�0���L�m`L $�v`�L��6��Etl� \u0010s�1u`U��hA�\u000e=�r���$�Jb4���vK���B���\b�a����j�\u0004q�D� z���wu���@����Gu!vD��b�J����5�h^68\u0005B�\u0016\u0005?�ח\u0012�n֧+i87g����!�t\u0016�w5>�����fkg����\u0010Ĝ�D�\b��p?����~(\b�X�ߪ$�0�ɜ�� lG���Z&�_,�W2cê[�Pw�!J_s ���a��\\ �y� \u0019=����);���Ʊx��*=�\u0004��ӶX�/���x��a�l�Z�\u0014n�R�L\u0013�k4�V?_�K ��yN�?%!8�����{��\u000e�=/��9�kmw}��=vi��n��n�Ue�p��W�n��5�\u0014���6�\u000eI��\bUR�O��� �B>U�\"�YG�m����\u0013a����Jo}w4��aYy\u000f�La��S b�H� r)7�J�64�\u0007tϲ,Х�IL5��-_d^���,5\\|\\ 0��\u0012��P׊���C�\u000e�V �#����5P��ve4�_�'�yb]��Ń�w 2ݘOJj\u000f�&�\u0007���)ɱ~�\u0015(>@Ll\"k��d�pnh̭$�� �\u0017\b+� ���Jb�v�y� �!p\u0005\u000e\u0014<\u0010B^�Y������Dv=4[x��~���{0Y�M/��۫�\u0016t��J����\u0017;a{\\{ {L>Շ�\u0001��I�_iw)�[��3�ʃ�9>�\\|}�]�5�����Z\b��P��3\\ !\u000e&���\u0015-��w�}\u0005�EH�\\ A\u000fs{�طj�\\Ҭ&ٖ���W3���f鍦FY\"�Vr\u0007Xݑw���� �@���\\|��l\u0013M56�\u0011���\u0017��\u0011���;�7��3 ����g\u0015�ڟ�\u0010� ���Q��S\\|��a��xu �\u0016&1K�\u0012}� Y jJ6D��͎\u00164\u000e@��~\u00150iï�O���%z\u0016.\u0012�U��?\u000e \u000e�'[�$�ݳ I��� �m[��Ud \b? ��6K\u0013�\\ �����\u0017ZӜG\"Z�\u0004�\u000fu �JO�v\\ \u0011\u0004YCx� Ӳ��1�#\u0005�\u001a�\u0017��Ŕ\u0001�� �,�M��\\ �$��R\u000f�����ixס=�Q��\u0010]���t1�����sߎ�Rl\u0017ݴ8/�.�3��Z��ￋ_&� �3`���\u0005\u0006n�q��zg��\\|擘��B�֓)^�hэ���*�7S�`�Jh���$�mJ��>;��\u0005C�4]أ�����[��n�z�X%_���Az f\u000e�\u0013\u0001�Zi՚���&R�t\u001b�&\u0010.߸\\�\u0005��f�x�W���t�\\|\\ e��+ȧ���RY5��j\u001bHq�B��=�E���\u0018�V�9O8$� ����IJ��\u0007\b�t�\u000fd\u0001�\\ �*�$�B�֯\\ Rx�q?\u0010��#��\u001a �}�d\u001b���pC�᩟\u0014\\ �I� >+\u001a�� ��J�\u0018m͟L�I}�{5�:\u000f̉�mL�\u0016\bt����3e\u0015��O�� ���_��-g�=�E�̲�T�Ӧ}����W�Mn\u0015�\\bV��D-y��g\u0004\u0019�x[ ?�w���?��d�.zR�n�! �!~�G�\u0005� �V\u0010+�5� p��&����іSF��,������=�9��\"t�辰���\u0014�\u0005�(\u001b,�m��\u001b�/�\u0014'�\u0007�W\u0007s\u0014\u0018Q\u001a\\܁����ۯ���r��5������/��������\u001b�����8 F��\\�?;�',&���\u0010\u0017\u0015�<��Gp[���\u000f��ݽ\\]�v��N��P0�/\bB���H���� \u0014a�q\u0017��\u0006sǨ��#=�l=�\u0018\u0001\u0014��� u��@/�*�� \\ ��t�ٸbJH�#\u0012�����S\u000f>���y8=��������Y0=������{��l�Npw\u0019OG�}v\u000f{�������-\u0015mmL�\u0013��ako�d'#��\\ ���Z]t�TY.��\u0017*\u0001s��ݡBB����BB\\|�\u0007\u0012�������� i�a���%����k���/�����\u0017�/ t��\b,L�6N��60OG0������M(//Ƈx�an�2�x� \u000ex5�6��pG�<]",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/popular_software/R/files/r-plotting.tar.gz",
      "scrapeId": "0c011491-e555-49e7-98a8-9ca21596181d",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/popular_software/R/files/r-plotting.tar.gz",
      "statusCode": 200
    }
  },
  {
    "processed_text": "�\b�jJa��k�0\u0014��s��� ��m����a���!���CD��m�Eۦ&�\u000e��M�9z� �� �}~j^����}_Rp#�@\u0007_T\u0011yw\u0004!�M��#�MI ���%\u0013:��ф2\u0017L��O�4� �w�Aczc�\u0006� �\\|!�Z�ߏ�w�\u0015�Y�'��b���MW�@���� ��V��A�I�NP�up����u\u0013�q��Ѕ�b�A�I�\\�L'W��t�\u0011F�1��/����G� �E]+x�t]m��RX.kQA'�g�\u001bޖ\u001aa�, �ع4��\u0005��B?s\u0013xQ�' sk;�\u0013E�3B�o!��jy(�>�r3�3)\u0016B?㥕���\\-���u� � ��\u000e\u000e\\˨^�\"�U�lת�u>Lt���;<� �\u0016\u0018a4\"O\"�B��w��c��}�H��ܔZv6<�+X�F��d��gC ��\u000e�6�B��K���u�\\|�݋}\b�l\bZވl\u0016 ��Zٞ\u0006oT1\u001a�Zn����C�\u0012.���� ��\u0010�C\b������ �[�V\\W�N^��om���d��� � �~��ޕ�Z�\\ fQ\u0012�����L��P���P�B�U�\u0016�����+\\�&�Sg���M�� � � 8��F��ў���\\|�\"\u0017�;��Sx\u001a���%a��\\��\u0007�a��[���^��=�{�.D\u0010\u0004A\u0010\u0004A\u0010\u0004A\u0010\u0004A\u0010\u0004A\u0010\u0004�\u0017~�;��(",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/popular_software/R/files/basic-r-job.tar.gz",
      "scrapeId": "3a142218-0eab-410e-ba32-85ee98901ea4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/popular_software/R/files/basic-r-job.tar.gz",
      "statusCode": 200
    }
  },
  {
    "processed_text": "�\bX��a�Y XSW��;��H���ւ��>@-��$�W��* /%\u0004T �\u0005�\u0010̃ܛ\u0010^���� X��\u0004\b� \u0001��~��a\u0015�\u0005�,���\u0012\u0004 �8\u0001y�snP;��lw�S����/_�s����������\u0010\u0005� 0�B \u0007Y�L��_��h�\u0019\u0001�_�Ƙ\u0011�X,�'���1��g�\u0006��\u0010\bX M@�cp�X�\u001a�k8�c��<\u0010�50\b�\u0006�ɀ�/84�G\u0005Y/å�IЏ�\u000f3�\u0010��� M0�Q�P >���\u0004� ����Ǚ�\u0010���\u0019�G������韧���`���$\u0006��\u0004y \u000e\u001b\b�hҀGE\u0001$0���A \u000f�A � �8������\u0015�� \u000e��f %�F��l3\u0014\u0014\u0004n��C�\b_\u0010\u0010J�h �\u0005���ʃ\u0018�0� �`\u0006 �p�z\u0001T C\u0004�&��\u0001�<^(lej�T�,< �\u0018� 6��\u0006�M�!��0\u0006U@���@��D8�#0�� �E$�\u000fx�uuEz0�\u000f\u0005R)� \"ΐ� \u0004�\u0014� �}��@@\u0004`\u0001�d�fh33S4�ã�~�x�l�q��͸�\u001b?S�x�9���7����^��ˠ-�x��]Q�� d�\u0013�����oT{B�d�q���E���F��!}���D^U#ܷ� �ޮ0�* 4���\u0005݃��t�X:\u00105�x��_�/8�Z��\"lDя\u001b霘hd���\\b����{��RgS�\u0012;=��`W�R�0$��.U.�h]^Qk��p��e�v��)��\u000f>\u0012�kؤ��; ��~. K*޼���b��rØ6�P�ȥy X\u000e�孰�r\u0011wr��*�_��\"���8J���>��-z-E�X\u0019fu��j���v�p� ���ط��_pK�мN��q /y1\u0019g�=I�J��'��I� 꺧�\\���c��\u0015�� ��9� ��l��Ⱥ�[::u�M^�'����\u001b��I1�4]Q�\u000e��鞞\u0013��{j.����e��`�\u0010 +�\u001bww��������fh�\"��ne���\\|M � �\u0010G\u0007�-Lu�S% &���2e�'����\u0006R�&\u0013Yj �\u0016� ��bd.0J\u0001�d_ş\u0017%���9�L\u0010w��t\u001a\u0018\\T��[1#�>�����(GG\\|�7{8y�ڰ�\u0014��/Q�w �`���K0\"\b<��bmH� \u0015e w\\ �\u0005������φ�\u001a����N�\\|�\u000e��ן<`=[�h7�\u0018r^���L�\u0019܎?�\u0014�.�\u00013��d�\u0011e\u0018�y\u000eA��4\\)��\u0017�0���\u0004�a� ��WY�ա�+;�]Z�\u0004�vՏ'np��}��=�:�tq�B\u0006]/q�Nl����5��(�r���3C�^�z�V9���s�%f��\"��[}w�0�\u001bŒ&��\u0015��uI�E\u000e͙k��^�K5�\u0017����{ �>3�K���ĖhϦӆ·��i)����\\ c Eq\u00079I1��d�ʔ! ��� ��lr5ܜ�\u0012�Z�:-�\u0017�_�L�9���\\ V\\�t� ����[�>�!Wٛ.8�M�����:ے \u0014�{/)��z�}�GT�.�՜��3eH�\\|�w���(ys������.������\u0017üƪ[�Ԯ[�mu�߷\u0012W5��\u0014�\u0010�\\�\u0010Qs=֨͌� -V:%�P�e:�\u0013��ό'�\\ �\u0015�a�����߷�G�l>�\"��+�X� 4��b�\u0011/����~����dͤU��x�k��ź%��lp�⃒z�\bw氜aQ����+���+L�d��������=����\u0019�� oV��\u001an���n�s�����\u0016f�:p9\\|�� \u0014�Go�`��4z��E��c_^)K78� \\|�ň���\\|s����\u0015 �� ��7�0\\g��\u0019 ֧IR᠔ٺ\u0017���,�\u0016��}\u0015¶$e�X��[C�>'��\u0014��W�9\b�h��K \u0019dq�Izc(�s�\"�~!�HB�e���xH��Y��I�N��rhΣ(�~��]gi\u0017㒕�r�8�tU^b� Ms��k\\A�E\u001bd�ʮ��6W� \u0018�@�N`�[m��[��\u0013���u�DuM }PfX�A�� ��d�\u0017� }=�}��O4�4g��X�:O��S������Yq\u0004��� M/\u0017?>�����q�k�.ng���_�s�ru�m�'��wqa(��hH���+ �}�z��\u0018�'O����P�\u0011�('��Q�J��[�Ҧ2x�\u0018#\u0015,_�Ud\b��N �?�lq�$Ex�\\ ^�\u0015^��v�3g�~mT�ס�S�7\u0014&\u0019Y�;mC��z��n����u�� �p�w�>]^��!0܇w���\\ \u0012�G�\\ �s�֨�& ��� =t5�V��\u000ee��L�ᬘz�������N�X�/n�(���\u001a���g\u0006�/T�Z�@ťu����\u0015Y�6�C�:wB�ı���F[X�\u0018���K\u0016�cu7�-��]S��m��ٮ7Ɖ\\N\bn�u\u000e��O� ��&��3��^���d����^� �\u0001i\u0011;jJ�Y�� ������{�\u000e\\��Ńܞuڥ}5����\u0007��F��\u0018�h�-e!h\u0013e���AB�Q�\u0006���m����� �(j ���uH2vVoeW;*��.դP��ȩ&K?2��(t\u001a���_v�G�gqy��j����N�fo���1c\u0014#+��6��\u0013���D�w\\ ��0�W�T� d�44�\u000f�:Q�n� n�R; �\b���%m�\\?<��f.����lk��R�TI��L� ��$��ܯ�:V�X�n�B���Y()E?������($�{ :խM�%��/�[���$�ַ��\\ \\ٸ�}�c��[\u0005���ӄ1����˓=�� �2! �J�ݱ��G�H��d����\u0010�B\\�m������ڻ�p��U����E��\u0018��k׭;U��΢}<�VlЍm̰q�9P�<�z0<���=,Q�쟣\\ ��\u0007�\u001b�D�!'� ���!�� ��y\u0016��Ƈ[gx\u0010\u0005[6\u0018��j֕��X=\\|�Z� \\|NU\u0018���;��\u00158�@Ļ���$�w��\\ �E��� :z���\u0016uf~���;\u0016e���C;�_�X��`ռi�Y����+uLU{H)v?=)6ɔ�6�1w����f�5�Hm���XtS\\|H�\u001a��6mA�n��T\u0006g7�Z8s\\2�a���bK#}��b�q0����d�<0\u00151U��v�U�'U�.��[��X*ݖ\u000f��ll9ط�Mra�p�\u0015۔%5��\u0005܊�m��������'��~ߖ\\|e$���\u0011�o��#^� <�-�=�vX<���m�]���������_U��O\u0011\u0015W�ItǕf�:��g��{_}�q{ٹ�7��#'��9Wc�v�R�K�/ѻ��A\\|��.��ƫ�g냑�O)��?gQ�˸�w�YM��A����̻�_:�R �ZԈ��E�\u0015qS^ɪ����O��B9j�\"ݣG}��V\\�jl� �B���\u0004���^�펹��'[2t��\u0019�T_��Nw�st'EJw+\u000f���k=���\u0005O�iN��\u0007!ҥ�놢�b��i��.��s�� r�p��m'�\u0006�R }��!\"��HN�;u\u0011a��3�2�\\ :[�宇��dK�Q�\u0017ڰm��&�6������_�\u0013\u0004ܫ�Q8\u0007%~����Y4����Xd#�k�8���#\u0016���\u000f������U�W��\u000465���\u0007\u001bg\u0007\u0016�#�'&G���h�\u000eD�G�\u000e�՜\u0016�n\u0011ن���yp,��� �y<;va�a5Q7zZ\u0011��P���cKʔ��c �'� c�;�������)��U�C%u��C�S���b��=�7/㷅���追���\u0016�?�W�?/����?��G�?�W�?��_�^�V��_h��� �üx�o�A�c_��ˠm �\u00060ئ\u0001 ����秎�� \u0018\u0018��y�\b��(t��\u000f�\u0004S!�s � �c<)> �\u0015�P�s\u000fy \u0016��F[�1Vh�s�l \b ����8A�\u0017GV�11��bq��Ȧ��A��)\u000e�A��� �@�\u0011�~�� �v� �Õb��a��iOr�8���+迓~��=�\u00016~��1渿��x 2n�F����\u0019�\\ �\u001a!�p( �c�\u0001� ��ȡ \u00040\u0019l�D��\\|\u0016�̓\u0001 ���\b 5�\u001a��i���A�� QZ�OM\u0017�!@�\u000e�\u0001�c*�\u0013%���<#\u0018a�\u0013�y�V\u0001�\u0017�D��\u0012��\u0011\bQ����\u000f@Nn�SG\u0011/�p2�<*�\\ i�\u00051h4d{b�4�\u0018 ��\"\u0015�7\u000e� \u001b �dD �́� \u000eۈ\u0007p¨�b B �T� �e\b���\u0019�1y� �Hg��\u0007y�.-D��f���\u0001ph\b\b8�Q\u001a�#V=\\ d��\u0004P�\u0011\\ ��M����L\u0004�d�S�\u0004��E@�\u0019k����\u0013��h+��*�1QC\u0001\"���6\\ C\\|�W\u0011\u0019���0� \u001a\"����h;\u0011\u0001&#\u00101�\u0013�6~Nߪ:�!qU׮ՙ�!N V�tu����E�\u0007\\|6�˧��P�\u0013\\ �\u0010����\u0004��\u0010e禼M�\u001blMn�\u001a� \u0007�T�Ʋ&o\u0010�H�<��i�Q\u0019֤](\b��\u001aWA�&M��~H Ϊk?����Z�Z��Z\u0015޹����V�g \u0019[W��1�&\u0004[�V ��\u0004C��#nd\u0017�]Zp(�\u0018������ 0��Y\u0017Q���3��d\b1��'�v��O��a~��84���?���W�/�������#�}B�Լ�_^���r�vj��5\u0007/�޸�Q,�N�4�� 0��V4�L!g�\u001b>�/�&�\u0005�:8u���s�j��vb\\|�8�ֆ�Y�nE�v���Ը�y�fH\u0019Λ�]���-�wv��;�2�b �WI?�}(�\\���F���'��vn��d��R���DM�\u0006yqG�R\u000e��w�}�J�Nrx�B�g�Yu\u0019o�\\ e�׬\\|�\\|�]�\u0014�F�ѱ�1���Y�#�(�9�9�w\u0010�,{�/�)Z��\u000f��o-쟭(u\u0014e\\ �t-֗P��uF㵕�P \\|2�W6O���C�Dv�\u0004A��9��\"`���alY�\u0018�\u0001Y\\|l!��=�0�M��Y���/�����9�\u0014�_\\|�8C]��r�a�_�l˨�S�\u000flwM�N�A�����g/LI'\b�g��}��ޭu�j�v[�Z�;�<\u0014ݽ&5Qȣr�ո�J�h\\ ����l�j*%�ʳ] ����\u0005*�d�Y\"�# \\|\u0014 \b%��\u0017*�縙��k�*A_�����K�o,m�.D���S >(�<��n�;4�{�'I=\\|ܴ��'� BmC\"5�3Qs�5���]�\\�u*�m��V�[�1A���;�����6���cFmŐ>=& X*��.\u001a~\u0014��\u0013� F�� ���\u0017� ����� ��H��_ US�\u0005�?\u0016ԟ�jR7��Ͽ��\\ \u00054��ת+I���kH��R vS8ċ,e�A����-��<������?�� \u0016N4X�m��\u000e�=� R��@%7�������W�~\u0018AD�n��V�5��ܢ\u0012s�*�k�m)ѧ� -\u0007ד���ϻ�����F#��n8fo� )�6��,����� ��+4���r�\u000e��xߺ5����Cfq.�4�\u0007y���]C����ar��\u0004e��^[�}��U��\\ �3�oI�Pa�z����Qm�����%)��i� ��ܧ��\u000f�I}�Zf2f8E���$v �feq����Rށ�\\ .m\\ T=0�+�/�55hTN����)I ���]���k��3�\u0013_�����\u001b1xI-�7\u0004�Ȼ��n� 7��x}�\u0004al��\\|^{n�\u0006���Ii��e^�:��u�惃gy\\ ��ﳯ����ϰ��\u0010̵7����}j鑸�Ƨ�� nY�Y{�A� S����q�xKI\u0001�U\u0019��iRW��yy,\u0014t�\u0013�a��\u0006r�m B� �\u0016\\ O*�\u0019��/~wa� �\u000fnN�\u0010�+�\\� �Q:�iY�̸��*�\u000f�~�M\u0011?��?�E�)��hSt�f�b֗���hn�lpn�(�\u0019��U Ʃee��gL�ګ������UTS�d�f\\�&�\u0014��'\u0014��uv���\u001a�{�$���[����Օ��U�M�4���lu?�M\u0016@�k\u0007͕��K��]�q]�<�\u000f�0��0c��)m�̡��\u0006!�\u0010��{I�r�]���������삽'z����qm�p(jB�� �5\u0011��\u001b\u0005'Ƀgpv/�<��qH\\ ���\u0011h)�t��f0�>1{\u000e\\ 9\u00169d��~+��Zd��:&��\u00196\u0004������d�Jw�n�s\\ hWF��x8\u001b&\u0010)[ ߕ�XUQ�d���ɦ�Bu�\u000e#�c\u0019�\\� �ϊ Z�\\���3\u0015�w �\u0019Q*� W��i�J1�`Eܮ�l�oa}�%\u0004�4\u0005-jլ��!��79Ӛa3ѫ��'�a�=���� \u0012�SaG����iW{H�tW�\u0017I�����g�t=�vJ �5�q��� 7x�9=�N\u0005E=t�7���l��g�W��\"�o� E��7Л#S������\u0014=��%�����\u0013ii�N_� �wj\u0014�\\ �N�9�I\u0011V\u0011�K���Ƭ\u0016����5�\u000ea5�\u0012\u0012t�i�l(��k�bK$�\u00134&��zh���D��:)�� ��}�6��ꄡ��Yndiɦ�\u0016 p�E��b��Jͭ\\|�����ŲZ���1.S?Ѵ��,Z�5\u000f�\\ �\u0006�j9\\�4��cFP����e]\\Q9�\u0017+h� +T��2V\u0017f\u0014E����oZ@�z��W�q���\u0015\u000f.)Sau��ű�� 㞺�\\ Դ-��yW���9�Q4;4G����H� k2��\u0016 �\u0007���M\\|VF����g�w&��\u0014�\u001bڻGp�읦?�\u0014��þZ5\u0018I�����\u0011���Z\u0004��A�51�\u0005�B_��+p��Vho7�9zQ\u0011yʆp?���ѺO��@wǾ���Ӡ\u0012��l1K�x�ƢW��I�kg8�x�3a�\" ^P���a���d3 \u0018���f�&�����(���TJ����\u001bv�3�8�A\u0012;c) �g��ഥ�9lj/�F�\u0017�� E\u0007,�A�+9��͡�G� u�0��8%���\u0001�� � �a�\u0017�k\u0006�+߭��[ǔL�*����Ϧ \\|��\u0011\u000eOٮ!C� @�����,�ɹ\\ 0Tٰ���n!䜕\u001a�l�,�f�Vt��\u0010߯<��J�\\ ˒u�쎢���\u001a ��c��SY���O�W��nӇ �����e7�\b��\u001bϜ#��+�6<�E�� T\u0015\\ ��t�q��� �+F]�Ѩ��?�:�V\\ �g��H�Tl��b��'W0R�1���֖�v!t��\u0015�Ϲ�x�o�7���10�s\u0001'\b����U\u0016S P",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/software/popular_software/R/files/r_array_example.tar.gz",
      "scrapeId": "f13a59ea-7dcc-4b3b-b42c-324b5ab5ab6f",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/software/popular_software/R/files/r_array_example.tar.gz",
      "statusCode": 200
    }
  }
]