[
  {
    "processed_text": "404 Error: It appears your data packet took a detour through a black hole. Don't worry, our astrophysicists are on it! Meanwhile, feel free to explore the rest of our galaxy. Home Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/",
      "error": "Not Found",
      "title": "UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "a6287686-1536-4e77-9c75-2e62a8b5ab8a",
      "viewport": [
        "width=device-width,initial-scale=1",
        "width=device-width, initial-scale=1.0"
      ],
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/",
      "statusCode": 404
    }
  },
  {
    "processed_text": "Skip to content Overview ¶ Log In Before Transferring To make transfers to/from HPC, you will need to have logged into your account at least once. If you have not, you may encounter \"directory does not exist\" errors. This is because your home directory is not created until you log in for the first time. See our System Access page for information on logging in. Designated Data Transfer Node for File Transfers ¶ For efficient file transfers to and from the HPC system, utilize the designated data transfer node, hostname: filexfer.hpc.arizona.edu . This node is optimized for handling large data transfers and is equipped with a high-speed 100 Gb interconnect. Why Use the Data Transfer Node? Optimized Performance. The dedicated data transfer node ensures efficient transfer speeds, particularly for large datasets. Network Stability. Utilizing the data transfer node helps prevent network congestion and potential disruptions on other components of the HPC system. Do not use hpc.arizona.edu Using the hostname hpc.arizona.edu for transfers will move your data to the HPC bastion host. The bastion host is not connected to the shared storage array (meaning files stored here will not be accessible on login/compute nodes) and has limited storage capacity. Users are restricted to 10 MB of space on this node and may experience login issues if this is exceeded. Data Transfers By Size ¶ Transfers ≤ 64 MB: For small data transfers, the web portal offers the most intuitive method. Transfers <100 GB: we recommend SFTP, SCP or Rsync using filexfer.hpc.arizona.edu . Transfers >100 GB, transfers outside the university, and large transfers within HPC: we recommend using Globus (GridFTP). Best Practices ¶ Use the file transfer nodes for large data transfers Login and compute nodes are not designed for large file transfers and transfers initiated here may result in network problems. The data transfer nodes (DTNs) are specifically set up for moving large amounts of data and are accessible via the hostname filexfer.hpc.arizona.edu . Limit file copy sessions You share bandwidth with others. Two or three SCP sessions are probably ok; >10 is not. Consolidate files If you are transferring many small files, consider collecting them in a tarball first. Transfer Software Summary ¶ | Software | CLI Interface? | GUI Interface? | Access to Cloud Services? | Notes | | --- | --- | --- | --- | --- | | | | | Google Drive | AWS | Box | Dropbox | | | Globus | ✅ | ✅ | ❌ | ✅ | ❌ | ❌ | | | SFTP | ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | | | SCP | ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | On Windows, WinSCP is available as a GUI interface | | Rsync | ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | Grsync is a GUI interface for Rsync for multiple platforms. | | Rclone | ✅ | ❓ | ✅ | ✅ | ✅ | ✅ | Rclone has recently announced they have an experimental GUI . | | Cyberduck | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | | | iRODS | ✅ | ✅ | ❌ | ❌ | ❌ | ❌ | | File Transfers and SSH Keys ¶ Several of the file transfer methods listed below use authentication based on the SSH protocol, including SCP, SFTP, and Rsync. Therefore, adding your SSH Key to the filexfer.hpc.arizona.edu node can allow one to avoid entering passwords when using those methods. See the documentation for adding SSH Keys . Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/transfers/overview/",
      "title": "Overview - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "03979348-72f1-47e6-a87f-be405e067d17",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/transfers/overview/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Storage Overview ¶ Where Should I Store My Data? ¶ Data undergoing active analyses should be stored in HPC's local High Performance Storage . Large amounts of data not requiring immediate access from our HPC compute nodes can be stored at reasonable rates on our Rental Storage . RDAS is a research data service which supports the mounting of SMB shares. The supported operating systems are MacOS, Linux, and Windows. It provides 5 TB of free storage. Research data not requiring immediate access should be stored in General Research Data Storage (Tier 2) . For example: Large datasets where only subsets are actively being analyzed. Results no longer requiring immediate access. Backups (highly encouraged!). Data that require HIPAA-compliance can be stored on Soteria (currently in the pilot phase). Storage Option Summary ¶ | | Purpose | Capacity | Cost | Restricted Data | Access | Duration | Backup | | --- | --- | --- | --- | --- | --- | --- | --- | | Primary HPC Storage | Research data. Supports compute. Directly attached to HPC | /home : 50 GB /groups : 500 GB /xdisk : 20 TB | Free | ❌ | Directly mounted on HPC. Also uses Globus and DTNs. | Long term. Aligns with HPC purchase cycle. | No | | R-DAS | Research Desktop Attached Storage - SMB shares | 5 TB | Free | ❌ | Mounted to workstations as shares | Long term | No | | Rental Storage | Research data. Large datasets. Typically for staging to HPC | Rented per TB per year | $47.35 per TB per year | ❌ | Uses Globus and DTNs. Copy data to Primary | Long term. Aligns with HPC purchase cycle | No | | Tier 2 | Typically research data. Unused data is archived | 15 GB to TBs | Tier-based system. First 1 TB of active data and archival data are free. Active data > 1 TB is paid. | ❌ | Uses Globus and AWS command line interface | Typically long term since use of Glacier is free and slow | Archival | | ReData | Research data . Managed by UArizona Libraries | Quota system | Free | ❌ | Log in and fill out fields, then upload | Longer than 10 years | No | | Soteria HIPAA | Secure data enclave | Individual requests | Free upon qualification | Restricted data; HIPAA, ePHI | HIPAA training required, followed by request process | Long term | No | | Box | General Data | 50 GB | Free | ❌ | Browser | Long term | Cloud | | Google Drive | General data | 15 GB | Free. Google rates for amounts > 15 GB | ❌ | Browser | Unlimited usage expires March 1, 2023 | Cloud | NIH Data Management and Sharing Policy ¶ The NIH has issued a new data management and sharing policy , effective January 25, 2023. The University Libraries now offers a comprehensive guide for how to navigate these policies and what they mean for you. What's new about the 2023 NIH Data Management and Sharing Policy? Previously, the NIH only required grants with $500,000 per year or more in direct costs to provide a brief explanation of how and when data resulting from the grant would be shared. The 2023 policy is entirely new. Beginning in 2023, ALL grant applications or renewals that generate Scientific Data must now include a robust and detailed plan for how you will manage and share data during the entire funded period. This includes information on data storage, access policies/procedures, preservation, metadata standards, distribution approaches, and more. You must provide this information in a data management and sharing plan (DMSP). The DMSP is similar to what other funders call a data management plan (DMP). The DMSP will be assessed by NIH Program Staff (though peer reviewers will be able to comment on the proposed data management budget). The Institute, Center, or Office (ICO)-approved plan becomes a Term and Condition of the Notice of Award. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/storage/overview/",
      "title": "Overview - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "7640fdba-3451-41bd-90dd-20fa00a3c2dd",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/storage/overview/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Globus ¶ Overview ¶ Globus provides a graphical web application that employs GridFTP to transfer data between pre-configured endpoints. GridFTP is an extension of the standard File transfer Protocol (FTP) for high-speed, reliable, and secure data transfer. Because GridFTP provides a more reliable and high performance file transfer (compared to protocols such as SCP or rsync), it enables the transmission of very large files. GridFTP also addresses the problem of incompatibility between storage and access systems. (You can read more about the advantages of GridFTP here ). A list of endpoint names managed by HPC are shown below for reference. For more information on usage, see HPC-Managed Globus Endpoints below. | Endpoint Name | Data Storage | | --- | --- | | UA HPC Filesystems | HPC's main storage array (access to home, xdisk, and groups) | | UA Rental Storage Filesystem | HPC rental storage | | Tier 2 AWS Storage | HPC-managed AWS S3 buckets | | UA HPC HIPAA Filesystems | Soteria | Accessing Globus ¶ Globus can be used as a web application. To access it, navigate to https://www.globus.org/ . Next, click Log In in the upper right-hand corner On the next page, enter The University of Arizona in the search field and click the result. This will take you through the standard university WebAuth login process. Once you successfully log in, you will be placed in a File Manager window. The various steps for setting up endpoints, initiating transfers, and viewing a transfer's progress can be found in the sections below. Globus Connect Personal ¶ To transfer files to/from your personal computer with Globus, you'll need to have a local endpoint set up. This can be achieved using Globus Connect Personal. Official documentation on how to install the relevant software and configure a local endpoint can be found in Globus' official how-to documentation . An overview is shown for Mac, Linux, and Windows below. To start, regardless of operating system, go to https://www.globus.org/ , log in, navigate to the Collections tab, and select Get Globus Connect Personal From there, choose your operating system to proceed with the download and setup process Mac Windows Linux Once you've downloaded the .dmg file, open it and drag/drop the Globus icon into your Applications directory Next, open the application. This will prompt you to log in via the university WebAuth process in a browser session. Once you've logged in, enter an identifying label for your local machine and grant Globus access This will bring you back to your local Globus Connect Personal installation. You will fill out your local display name for your endpoint and click Save . Once your installation is complete, open the .exe file to initiate the install. Click Yes to allow Globus Connect Personal to make changes to your device Next, select the install location, and click Install . Once the install is complete, make sure the Run Globus Connect Personal box is checked and click Finish . Globus Connect Personal will then open and begin the configuration process. Click Log In to continue. This will open a web browser where you will go through the typical UArizona WebAuth login process. Once you're logged in, give your local endpoint a descriptive name and click Allow . This will bring you back to your local install. Enter a descriptive local name for your endpoint and click Save . Once your installation is complete, open a terminal, navigate to your Downloads directory, and unpack the tar archive. Next, change into the unpacked directory and execute the globusconnectpersonal binary: ``` [user@ubuntu ~]$ cd Downloads [user@ubuntu Downloads]$ tar xzvf globusconnectpersonal-latest.tgz [user@ubuntu Downloads]$ cd globusconnectpersonal-3.2.0 [user@ubuntu globusconnectpersonal-3.2.0]$ ./globusconnectpersonal ``` This will bring up a graphical application. Click Log In to continue This will open a web browser where you will need to go through the typical university WebAuth process. Once you're logged in, give your endpoint a name and click Allow . This will bring you back to your local installation. Give your machine a descriptive name, then select Save . Your setup should now be complete and your endpoint will now be usable to initiate transfers. You can find your endpoint by navigating to the Collections tab and checking the box Administered by you . For example: HPC Managed Globus Endpoints ¶ HPC managed endpoints allow you to connect to HPC-affiliated storage to initiate transfers. Transfers can be made between any two endpoints; for example, allowing you to make transfers between your own personal computer and HPC storage, between HPC storage ( /home , /groups , or /xdisk ) and a rental option (such as Tier 2 AWS buckets or HPC rental storage), or between HPC and another institution's endpoint. Below are a list of HPC managed endpoints and how to configure them: HPC Storage Rental Storage Tier 2 AWS Storage The endpoint for HPC can be found by searching UA HPC Filesystems under the Collections tab. Click the result, then click Open in File Manager to access your HPC files. The default location is your /home on HPC. You can navigate through by double-clicking directories, or by entering a full path in the Path search bar and hitting enter. This method can be used to access any /xdisk or /groups directories you have access to. The endpoint for rental storage (found on the filexfer nodes under /rental ) can be found by searching UA Rental Storage Filesystem under the Collections tab. This will open details on the endpoint. Click Open in File Manager to view the contents. The root for this endpoint is /rental. Faculty members who have rented storage will have a directory with their NetID in this space. Find the one relevant to you and double-click to access. To access a Tier 2 AWS S3 bucket , in the Collections tab, enter UA AWS S3 in the search bar. In the results, you should see the name UA AWS S3 show up with the description Subscribed Mapped Collection. Click the endpoint's name to proceed Next, select the Credentials tab and select Add Credential. If you are prompted for Authentication/Consent, click Continue If requested, authenticate by selecting your Arizona email address, then Allow . You will then be returned to the Credentials tab. From there, link to your AWS S3 Bucket by entering your public and private keys in the provided fields and click Continue . Once you've added your keys, navigate back to the UA AWS S3 collection, go to the Collections tab, and click Add a Guest Collection on the right Under Create New Guest Collection, click Browse next to the Directory field to find your group's AWS bucket. You will find it under /ua-rt-t2-faculty_netid/ where faculty_netid is the NetID of the faculty member who requested the bucket. Under Display Name , enter a descriptive name that you can use to identify your bucket. Once you've completed the process, click Create Collection at the bottom of the page. Tip If you encounter Authentication/Consent Required after clicking Browse, click Continue, select your university credentials, and click Allow. That should bring you back to the Browse window. To find and use your new collection, navigate to the Collections tab and select the display name you assigned to your bucket. That will open your collection in the File Manager window allowing you to view the contents and initiate transfers. If you click the display name, this will open the bucket in the Globus file manager window allowing you to see the contents Making Transfers ¶ Transfers can be made between any two endpoints of your choosing using the File Manager window in the Globus web application. In this example, we'll make a transfer between a Globus Connect Personal endpoint and the primary HPC storage array. To start, go to the File Manager tab in the Globus web application. Make sure you have the dual-panel mode enabled (upper right-hand corner shown with the red arrow below) to allow you to open two endpoints. Start with opening your first endpoint by clicking the Search bar on the left-hand side. This will open a window where you can search for your first endpoint. In this example, we'll use UA HPC Filesystems . Click the result. Now, on the left-hand side you should see the contents of your home directory on HPC. You can navigate through the various directories by double-clicking the folder icons, or can enter a full path in the Path search bar. To open a second connection, click the Search bar on the right-hand side. You can search for your next endpoint in the same way as we searched for UA HPC Filesystems. You can also find recently used endpoints and your collections (e.g., a Google Drive collection or personal endpoint) under the Recent and Your Collections tabs. In this example, we'll go to Your Collections, find a personal endpoint, and click the result. Now you should be back in the File Manager window with two endpoints open. A transfer can be made from one endpoint to another by selecting the item(s) you want to transfer, then clicking the Start button. Monitoring Your Transfers ¶ When you initiate a transfer following the instructions in the Making Transfers section above, a green box will pop up confirming the request. You can get additional information the Activity panel on the left-hand side of the page. This will show you active and past transfers as well as their status. You can view additional details about your transfers by clicking the > shown on the right-hand side next to the target task. You can also cancel a transfer by clicking the × on the right. Once your transfer has completed, you should receive an email with its status. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/transfers/globus/",
      "title": "Globus - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "1ce8912d-d724-4bda-8c77-298ee7d831c2",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/transfers/globus/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content SCP ¶ Secure Copy, or SCP, uses Secure Shell (SSH) for data transfer and utilizes the same mechanisms for authentication, thereby ensuring the authenticity and confidentiality of the data in transit. Mac/Linux ¶ You will need to use an SSH v2 compliant terminal to move files to/from HPC. For more information on using SCP, use man scp . Copying to HPC In a local terminal, you can move a file or directory to a designated subdirectory in your account on HPC using the following syntax: ``` scp -rp /path/to/file/or/directory netid@filexfer.hpc.arizona.edu:/path/to/remote/destination ``` Copying from HPC In a local terminal, you can copy a remote file from HPC to your current directory using the syntax: ``` scp -rp netid@filexfer.hpc.arizona.edu:/path/to/file/or/directory . ``` Shorthand Note that the trailing period above refers to the current directory. See our Linux Cheat Sheet for more tips like this. Wildcards can be used for multiple file transfers (e.g. all files with .dat extension). Note the backslash \\ preceding * ``` scp netid@filexfer.hpc.arizona.edu:subdirectory/*.dat . ``` Windows ¶ Windows users can use software like WinSCP to make SCP transfers. To use WinSCP, first download/install the software from: https://winscp.net/eng/download.php To connect, enter filexfer.hpc.arizona.edu in the Host Name field, enter your NetID under User name , and enter your password. Accept by clicking Login . You'll be prompted to Duo Authenticate: Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/transfers/scp/",
      "title": "SCP - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "2df2355c-e9fe-4a59-8cc6-9262a2526442",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/transfers/scp/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Cyberduck ¶ Cyberduck is a graphical file transfer application that can be used to connect to and transfer files between your local computer and various remote servers and cloud storage services. To get started, you can download the application onto your local workstation from their website here: https://cyberduck.io/ Initiating Transfers ¶ Once you have Cyberduck installed, open the software and select New Browser from the toolbar In the window that opens, select Open Connection This will give you a number of options to choose from. Some Connection Options ¶ HPC Google Drive To connect to HPC, select SFTP (SSH File Transfer Protocol) from the top dropdown, enter filexfer.hpc.arizona.edu under Server , and your university credentials under Username and Password . Once you click Connect, you will be prompted to duo-authenticate If your connection is successful, you will see a window open with the contents of your home directory. To connect to Google Drive, select the Google Drive option from the dropdown tab and select Connect This will open a browser where you will be prompted to log into your Google Drive account. Once you have successfully logged in, grant access to Cyberduck where prompted. If this process is successful, you should see a connection window where you can navigate through the contents of your Google Drive. To initiate transfers, simply drag and drop your files between the Cyberduck window and your local computer. If you have multiple connections open, you can also initiate transfers between two remotes by dragging and dropping files between two connection windows. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/transfers/cyberduck/",
      "title": "Cyberduck - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "71d35f09-75b3-4dc1-b060-507b59ce8647",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/transfers/cyberduck/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Research Desktop Attached Storage (R-DAS) ¶ Overview ¶ R-DAS not an HPC filesystem R-DAS storage is not mounted on HPC compute or login nodes. Data stored in R-DAS will need to be copied over to the HPC filesystem in order to be accessible to jobs. Instructions on how to access R-DAS from HPC are included below. Group Sharing Faculty members/PIs can share their allocations with group members. To do so, in step 6 in the Accessing Your R-DAS Allocation section below, group members will choose the allocation with their faculty member's/PI's NetID. No Controlled Data This service is not intended for HIPAA or otherwise controlled data. Please see Secure HPC for more information. On October 16, 2023, we went live with the Research Desktop Attached Storage Array (R-DAS). R-DAS provides up to 5 TB of no-cost storage capacity for each PI group. Our requirement was to enable our users to easily share data with other research group members. You can treat the allocation as a drive mounted on your local computer. R-DAS is intended for storing open research data, but not controlled or regulated data. Technical Requirements ¶ R-DAS is a storage service backed by a Qumulo branded storage array. It supports the mounting of SMB shares for SMB 3.1. The supported operating systems are MacOS (Monterey or higher), Linux (kernel 3.7 or higher), and Windows (Windows 10 or 11). Performance ¶ The storage array is located in the Research Data Center to benefit from the network infrastructure in the Computer Center. The performance you experience will depend on your network connectivity. The best case is likely wired ethernet in a newer building. Off campus usage requires connection to the VPN, and so performance can be variable. Our testing off campus regularly reached 3 MB/s. Requesting an Allocation ¶ PIs can request an allocation on R-DAS from https://portal.hpc.arizona.edu/portal Go to the Storage tab Select Create Shared Desktop Storage under Research Desktop Storage Select Open Submission Form from the window that opens. This will open the MOU agreement. Review it and, if it is acceptable to you, select Submit Request . Note: you must scroll to the bottom of the MOU agreement to be able to submit the request. You can now select the View Shared Desktop Storage option from the main Storage page in the user portal Accessing Your R-DAS Allocation ¶ Tip UArizona IP Address Required: To access your R-DAS allocation you need to be connected to either the UArizona campus network, or the UArizona SSL VPN. For information about connecting to a VPN, see VPN - Virtual Private Network . If you are accessing your R-DAS allocation from an HPC cluster, then you are already on the UArizona campus network and do not need to connect to the UArizona SSL VPN. Please note that the correct VPN to access R-DAS is the UArizona SSL VPN (vpn.arizona.edu), and not the HPC VPN (vpn.hpc.arizona.edu). R-DAS can be accessed from Linux, MacOS, or Windows. The screenshots are intended to be visual aids, but they include information from the consulting team. When you proceed, please enter your own information. Choose your operating system Linux Mac OS Windows No sudo on HPC Do not attempt to run sudo commands on HPC, these are only meant for your personal Linux machines. To transfer data between R-DAS and HPC see Transfer data between R-DAS and HPC storage . First, install the necessary software packages to access your allocation Choose your distribution Debian/Ubuntu Fedora/CentOS Other Linux Distributions ``` sudo apt install samba gvfs-backends smbclient ``` ``` sudo yum install samba gvfs-samba samba-client ``` Please check the documentation of your distribution. Next, access your allocation Choose your connection method GUI CLI On a desktop environment, such as MATE, GNOME, KDE, you can mount your R-DAS allocation as a local drive with the corresponding file manager (Caja on MATE, GNOME Files, Dolphin on KDE). On HPC, you can use a virtual desktop . Open the file manager (Caja, GNOME Files, Dolphin) Press Ctrl + L . This makes the location bar editable. Enter smb://rdas.hpc.arizona.edu in the location bar, and press Enter . A few moments later a window opens, prompting for your Username ( BLUECAT\\ followed by your UArizona NetID) and Password (UA NetID password). After entering the details, select Connect (on other file managers this may be OK ). Some file managers, such as Caja and GNOME Files, also have a Domain field, whereas others, like Dolphin, do not. Either way, you do not need to modify its default value. Select the allocation named after your group from the list of allocations displayed. On some file managers, such as Dolphin, you can right away access your allocation by double clicking on it. On others, such as Caja and GNOME Files, double clicking on it will open another window prompting for your Username ( BLUECAT\\ followed by your UArizona NetID) and Password (UA NetID password). Select Connect as user , enter the details, and select Connect . Your allocation will be mounted as a local drive. You can interactively browse your R-DAS allocation with smbclient : ``` smbclient \\\\rdas.hpc.arizona.edu\\ -U BLUECAT\\ ``` The <share> is the PI group that you belong to, and <username> is your UArizona NetID. The command will prompt for a password where you will enter your UArizona NetID password. This will start an smb shell. For example: ``` ~ $ smbclient \\\\rdas.hpc.arizona.edu\\sohampal -U BLUECAT\\sohampal Password for [BLUECAT\\sohampal]: Try \"help\" to get a list of possible commands. smb: > ``` Try help to get a list of possible commands: ``` smb: > help ? allinfo altname archive backup blocksize cancel case_sensitive cd chmod chown close del deltree dir du echo exit get getfacl . . . ``` Use the -L flag to get the list of shares on the Array. For example: ``` smbclient -L \\\\rdas.hpc.arizona.edu -U BLUECAT\\sohampal Password for [BLUECAT\\sohampal]: Sharename Type Comment --------- ---- ------- Q$ Disk Default root share for SRVSVC. ipc$ IPC Named Pipes upgrade Disk for qumulo upgrades tmerritt Disk Desktop share for tmerritt created on 09/12/2023 12:24 PM . . . ``` Any command that you can run interactively from the smb shell, you can also run non-interactively with the -c flag. For example, to list the files and directories in your share, run: ``` smbclient \\\\rdas.hpc.arizona.edu\\ -U BLUECAT\\ -c 'ls' ``` You can also combine multiple commands with ; . For example to list the contents in a directory in your share, run: ``` smbclient \\\\rdas.hpc.arizona.edu\\ -U BLUECAT\\ -c 'cd ;ls' ``` To copy a file from your local system to your R-DAS share use put , and from your R-DAS share to your local system use get : ``` smbclient \\\\rdas.hpc.arizona.edu\\ -U BLUECAT\\ -c 'put ' ``` To learn more about smbclient, run man smbclient . If you are on a Mac, then you can mount your R-DAS allocation as a local drive with the following steps: Go to Finder Select Go from the top menu bar. From the drop-down menu, select Connect to Server . In the window that opens, enter smb://rdas.hpc.arizona.edu in the address bar, and select Connect . After a few moments a window opens prompting for your Name (UA NetID) and Password (UA NetID password). After entering the details, select Connect. A window will open with the list of allocations on the array. Select the allocation named after your group, and then select OK . If you are on Windows, you can mount your R-DAS allocation as a local drive with the following steps: Open Windows Explorer. Enter \\\\rdas.hpc.arizona.edu in the location bar, and press Enter . A few moments later a window will open, prompting for your Username ( BLUECAT\\ followed by your UArizona NetID) and Password (UA NetID password). After entering the details, select OK . Select the allocation named after your group from the list of allocations displayed. You can directly open the allocation by double-clicking on it, or mount it by right clicking on it and selecting Map network drive . Transfer data between R-DAS and HPC storage ¶ The simplest way to transfer data between your R-DAS share and HPC storage is to first transfer data to your local machine, and then from local machine to the destination. For more information on transferring data from local machine, see Transfers . However if you do not want to store the data to your local machine as an intermediate step, then you can transfer data between R-DAS and HPC storage with the following steps: Mount the R-DAS share as a local drive following the steps above. Transfer the data using rsync , see rsync for more information. For example, if your local machine is a Mac, then you can transfer the data from R-DAS to HPC storage with the following: ``` rsync -ravz /Volumes/ / @filexfer.hpc.arizona.edu: ``` The above steps assumes that you know the mount point of the R-DAS share on your local machines: On Linux, it might take some amount of sleuthing to find out where it is mounted. File managers dependent on gvfs will typically mount it under /run/user/<uid>/gvfs . On a Mac, it will typically be mounted at /Volumes/<share-name> . On Window, you will have to map it to a drive. We recommend that you use rsync to transfer the data from your R-DAS share to HPC storage. However, if you do not know the mount point of the R-DAS share, or if you do not want to use rsync , then the other alternative to transfer data between R-DAS and HPC storage is: Start an virtual desktop on Open OnDemand. See Virtual Desktop for more information. Mount the R-DAS share following the Linux GUI steps. Transfer the data graphically, or using your favorite command line tool from the virtual desktop terminal. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/storage/rdas_storage/",
      "title": "R-DAS Storage - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "51a17531-bccd-4c15-837e-aa9f2674e01e",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/storage/rdas_storage/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Tier 2 AWS Storage ¶ Overview ¶ No Controlled Data This service is not intended for HIPAA or otherwise controlled data. Please see Secure HPC for more information. Research Technologies in partnership with UITS has implemented an AWS rental storage solution. This AWS option is called Tier 2 which differs from Tier 1, the primary storage that is directly connected to the HPC clusters. Tier 1 is very fast, very expensive, and immediately available for active analyses. Tier 2 is intended for archival data not immediately undergoing active analyses and for backups (highly encouraged!). Researchers can use the software Globus to move data to Tier 2, and can also move data from other sources (called endpoints). The data in Tier 2 will not be mounted on HPC, and so Globus will be used to move it back to Tier 1 if needed. AWS storage is organized in \"buckets.\" One S3 intelligent tiering bucket is supported per KFS account. A PI could sponsor multiple buckets by submitting separate requests each with a unique KFS number, and then provide permissions as they see fit. Data Lifecycle ¶ Avoid large numbers of files Because AWS is set up for automatic archiving, files are moved into tiers where restore requests need to be submitted for each individual file that needs to be downloaded after a period of time. We strongly recommend archiving directories (.zip, .tar.gz files, etc) prior to moving them to AWS. This will significantly speed up your data transfers as well as reduce the complexity of file restorations. If you transfer hundreds or thousands of files to AWS, restore requests may take days or weeks to process. Small files Warning: Very small files (less than 128KB ) are not subject to intelligent tiering and are not migrated to Glacier/Deep Glacier. This means they are permanently stored in the paid storage class. If you have many small files, we recommend making archives of your directories (.tar.gz, .zip, etc) prior to uploading them to AWS. This will also reduce transfer times significantly. Tier 2 AWS buckets use intelligent tiering to determine the archival status of files. When data are first uploaded to a group's bucket, they are in the standard access class. This essentially means they are stored on higher performant storage and are available for immediate download. After three months of inactivity , data are automatically migrated to Glacier storage. This is less performant and data are no longer instantly downloadable. Users will need to request a restore before downloading their files. Restore requests can be submitted either in the user portal or using a command line tool available on our compute nodes (more details below). After three months of inactivity in the Glacier access tier, data are automatically migrated to Deep Glacier. Deep Glacier utilizes very slow storage technology and requires a restore request to be submitted prior to downloading files, similar to Glacier. Deep Glacier restore requests typically take more time than Glacier files. Pricing ¶ Storage Costs ¶ Part of this service is paid for by researchers and the rest is either subsidized or covered by UITS. The data stored in S3 will be billed monthly by AWS to the KFS account used when this is set up. Data in archival storage will be stored at no cost to the researcher. You will receive an email with detailed billing information when charges are made to your account. | Tier | Cost to Researchers | Duration | Data Retrieval | | --- | --- | --- | --- | | Standard | $0 (First TB) $23/TB/Month 1 (data > 1 TB) | Three months (if data not downloaded). After three months, untouched data automatically migrate to Glacier. | Data may be immediately downloaded. | | Glacier | $0 | Three months (if data not downloaded*). After three months, untouched data automatically migrated to Deep Glacier. | A restore request must be submitted. Restores may take a few minutes to hours. Data may be transferred once restored. | | Deep Glacier | $0 | Unlimited (if data not downloaded) | A restore request must be submitted. Restores may take a few hours to days. Data may be transferred once restored. | Data Transfer Costs ¶ Data movement costs are subsidized by UITS, researchers are not charged any AWS transfer fees. Request a Bucket ¶ Who can submit a request? A group's PI is responsible for submitting a storage request unless they have a group delegate who may submit requests on their behalf. First, log into the User Portal and navigate to the Storage tab at the top of the page. Select Submit Tier 2 Storage Request . This will open a web form. Add your KFS number under KFS Number and the email address for the Department's financial contact under Business contact email . There will also be two optional fields: Subaccount and Project . These are used for tagging/reporting purposes in KFS billing. You can safely leave these entries blank if you're not sure what they are. Once you have completed the form, click Send request . Submitting this form will open a ServiceNow ticket. Processing time may take up to a few days. Once your request has been completed, you will receive a confirmation email with a link to subscribe for account alerts (e.g., notifications for a sudden spike in usage). Checking Your Usage ¶ Tip AWS runs a batch update every night with the results being reported the following day. This means that if you have made any modifications to your allocation, your usage information will not be accurately reflected until the next batch update. User Portal CLI You may check your storage usage at any time in the User Portal . Navigate to the Storage tab, select View Tier 2 Storage , and click Query Usage . To view the size and storage classes of individual objects, you will need to use the CLI interface. A command line tool is available on our compute nodes that will allow you to view the size and storage classes of the contents in your bucket. You will need to generate access keys to use this tool (see the next section). This can be accessed using: ``` (elgato) [netid@junonia ~]$ interactive [netid@cpu37 ~]$ module load contrib ; module load bjoyce3/sarawillis/tier2-viewer ``` For information on usage: ``` tier2-viewer --help ``` To play a tutorial in your terminal, use: ``` tier2-viewer --example ``` Generate Access Keys ¶ Access keys will allow you to connect your AWS bucket using tools such as Globus which will enable you to make transfers directly between HPC and your Tier 2 storage allocation. Access keys should be treated as passwords and should only be shared with trusted group members and collaborators. To generate an access key, log into the User Portal , navigate to the Storage tab, and select Regenerate IAM Access Key under the Tier 2 Storage heading. This will generate a KeyID and Secret Access Key used to establish the connection. Save these keys somewhere safe since once the window is closed, they cannot be retrieved . If you forget your keys, you can regenerate a new pair. Transferring Files ¶ Files must be in the Standard tier to be downloadable Any files that are in archival storage (Glacier or Deep Glacier) must first be restored to the Standard tier to be retrievable. See the next section for more information. The easiest way to transfer files from AWS to HPC is using Globus. We have instructions in our Transferring Files page on how to set up an endpoint to access your AWS bucket as well as how to initiate file transfers. Some other file transfer programs include rclone and Cyberduck . Restoring Archived Data ¶ Data that are not touched for at least 90 and 180 days are automatically re-tiered to archival storage (Glacier and Deep Glacier, respectively). Files stored in an archival state cannot be transferred out of AWS until they are restored. Restore requests can be submitted either via the User Portal or using a command line utility available on our compute nodes. The time it takes for an object to be retrieved is dependent on its storage class. Objects in Glacier may take a few hours while objects in Deep Glacier may take up to a day or two. Once an object has been restored, it will move back up to the frequent access tier and can be downloaded using any transfer method you prefer. User Portal CLI File count Warning: If you are restoring a directory, the portal will only support restore requests for directories containing up to 50 files. If you need to restore a large directory, use the CLI. In the User Portal , navigate to the Storage tab by clicking Restore Archived Tier 2 Storage Object : This will open a box where you can enter the path to a file or directory in your bucket. Enter the path to the object you would like to restore, then click to initiate the process. A command line tool is available on our compute nodes that will allow you to view the size and storage classes of the contents in your bucket. You will need to generate access keys to use this tool. This can be accessed using: ``` (elgato) [netid@junonia ~]$ interactive [netid@cpu37 ~]$ module load contrib ; module load bjoyce3/sarawillis/tier2-viewer ``` For information on usage: ``` tier2-viewer --help ``` To play a tutorial in your terminal, use: ``` tier2-viewer --example ``` The --restore flag can be used to either restore a file or a full directory. More up-to-date pricing information can be found on AWS's website . ↩ Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/storage/tier2_storage/",
      "title": "Tier2 AWS Storage - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "79ca5ebb-2072-485f-acb3-beeaeea1bdb3",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/storage/tier2_storage/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content HPC Storage ¶ Overview ¶ The University’s Research Data Center provides data storage for active analysis on the high-performance computers (HPCs). Using central computing storage services and resources, researchers are able to: Share research data in a collaborative environment with other UArizona affiliates on the HPC system. Store large-scale computational research data. Request additional storage for further data analysis. All clusters share access to the same mounted HPC storage, so your files are available regardless of which cluster you’re using. Every user has access to individual and shared storage on the system where they can host data for active analyses. A summary of these locations is shown below: | Path | Description | Quota | Duration | | --- | --- | --- | --- | | /home/uxx/netid | An individual storage allocation provided for every HPC user | 50 GB | Accessible for the duration of user's account | | /groups/pi_netid | A communal storage allocation provided for every research group | 500 GB | Accessible for the duration of a PI's account | | /xdisk/pi_netid | Temporary communal storage available for every group on request. See xdisk section below for details. | 200 GB to 20 TB | Up to 300 days | | /tmp | Local storage available on individual compute nodes. | < 800 GB to 1.4 TB | Only accessible for the duration of a job's run | Managing permissions If you're working with other members of your group and need to make your files more accessible, see our bash cheat sheet . This offers an overview on Linux file permissions, ownership, and some tips for working with collaborators. Best Practices ¶ The shared file system on HPC is the location for everything in /home , /groups , and /xdisk . The /tmp directory is also available to users, and refers to the local disk on each node. Your I/O activity can have dramatic activity on other users. Extreme read/write activity can cause bottlenecks and may be cancelled without warning. It is generally best to limit I/O whenever possible to avoid straining the system. The HPC consult team is available to help optimize workflows that may be impacted by I/O. Be aware of I/O load. Running multiple instances of jobs performing significant I/O activity may be detrimental to the system, especially if these occur within the same subdirectories. It may be best to read in data at the beginning of a workflow, perform the entire analysis, then write at the very end. Reconfiguring your workflow to limit I/O may cost some time up front, but will most likely be made back through faster job completion. If you are running array jobs , please be cognizant of your I/O activity. Use /tmp for working space If you have multiple jobs that will use the same data, consider copying it to /tmp and run multiple jobs. This can increase performance and reduce I/O load. Avoid storing many files in a single directory Hundreds of files is probably ok; tens of thousands is not. Avoid opening and closing files repeatedly in tight loops If possible, open files once at the beginning of your workflow/program, then close them at the end. Watch your quotas You are limited in capacity and exceeding your storage quotas may have unintended side effects (e.g., login issues, data loss, or failed jobs). See the section below on checking your storage usage. Avoid frequent snapshot files This can stress the storage. Use parallel I/O Some modules enable parallelized file operations, such as phdf5 . Checking Your Storage Usage ¶ Command Line User Portal To check your storage usage, on a compute node, file transfer node, or login node, use the command uquota . This will show you all the spaces you have access to, their quotas, and current usage. ``` (puma) [netid@junonia ~]$ uquota used soft limit hard limit /groups/pi_netid 6.6G 500.0G 500.0G /home 37.1G 50.0G 50.0G /xdisk/pi_netid 12.9G 9.8T 9.8T ``` You can check your storage allocation through our online user portal by navigating to the Storage tab and clicking Check Disk Quotas : xdisk ¶ What is xdisk? ¶ xdisk is a temporary storage allocation available to all faculty members (PIs) and offers up to 20 TB of usable space for their group for up to 300 days. PIs may only have one active xdisk at a time. A PI can request an allocation either via the command line or through our web portal (no paperwork necessary!). Only PIs may request, alter, or delete an allocation from the command line. However, members of their research group may be delegated management rights allowing them to manage a group's xdisk on their PI's behalf through our web portal. Once an xdisk allocation is created, it is immediately available for use. Groups can find their allocations under /xdisk/pi_netid . By default, a subdirectory is created for each group member under /xdisk/pi_netid/netid . If a group member is added after the allocation is created, a directory is not automatically created for them. To add one, reach out to our consultants . Because xdisk allocations are temporary, they will be removed as soon as their time limit is reached. Warnings will be sent to every group member at their netid@arizona.edu addresses beginning one month before the expiration. It is the group's responsibility to renew xdisk allocations or copy files to an alternate storage location prior to the expiration date. Once an xdisk allocation expires, the contents are deleted. PIs may request a new xdisk allocation immediately after their previous one has expired. This ensures groups will always have access to increased storage on HPC on a rolling basis with the requirement that housekeeping be done once per academic year. Requesting, Modifying, and Deleting an Allocation ¶ XDisk management is limited to PIs and delegates Only PIs and trusted delegates can create, delete, and modify xdisk allocations. The CLI interface is restricted to PIs only. Delegates may manage their groups allocation through the user portal. For more information on adding group delegates and how they can use the portal on their PI's behalf, see: Delegating Group Management Rights . User Portal ¶ Requesting an Allocation Modifying an Allocation Deleting an Allocation Warning If a group has an active xdisk allocation, a new one cannot be created until the active allocation expires or is deleted. PIs or delegates can request an xdisk allocation at any time through the user portal. Under the Storage tab, select Create XDisk This will open a web form where you can enter your size and duration requirements in GB and days, respectively. The maximum size that can be requested is 20000 GB and the maximum duration is 300 days. In addition, specify the desired group ownership for the allocation from the Group dropdown menu This will determine file permissions and who has access. Once you click , your allocation should immediately be available. PIs or delegates may manage their xdisk allocation at any time through the user portal. Under the Storage tab, either select Update XDisk Size or Update XDisk Duration , depending on the property you would like to update. This will open a form which will allow you to modify the size and duration of your xdisk. Xdisk allocations cannot be increased beyond 20000 GB and the maximum duration of 300 days. Note: the Group field may only be modified at the time of the allocation's creation. PIs or delegates may delete their xdisk allocation at any time through the user portal. Under the Storage tab, select Delete XDisk Clicking this link will open a window with a prompt. Type confirm and then select to complete the process. If you would like to request a new xdisk, you may do so as soon as the request is processed. Note: sometimes processing the request can take a few minutes, depending on the number of files and the size of the allocation. CLI Commands (PIs only) ¶ Warning The xdisk CLI commands are usable by PIs only. Group delegates can manage allocations via the user portal after switching to their PI's account. xdisk is a locally written utility for PI's to create, delete, resize, and extend xdisk allocations. Any PIs who wish to utilize the CLI to manage their allocations can do so using the syntax shown below: | xdisk Function | Command | Examples | | --- | --- | --- | | Display xdisk help | <br>xdisk -c help<br> | <br>$ xdisk -c help<br> | | View Current Information | <br>xdisk -c query<br> | <br>$ xdisk -c query<br>XDISK on host: ericidle.hpc.arizona.edu<br>Current xdisk allocation for <pi_netid>:<br>Disk location: /xdisk/<pi_netid><br>Allocated size: 200GB<br>Creation date: 3/10/2020 Expiration date: 6/8/2020<br>Max days: 45 Max size: 1000GB<br> | | Create an xdisk | <br>xdisk -c create -m [size in gb] -d [days]<br> | <br>$ xdisk -c create -m 300 -d 30<br>Your create request of 300 GB for 30 days was successful.<br>Your space is in /xdisk/<pi_netid><br> | | Extend xdisk Expiration Date | <br>xdisk -c expire -d [days]<br> | <br>$ xdisk -c expire -d 15<br>Your extension of 15 days was successfully processed<br> | | Resize an xdisk Allocation | <br>xdisk -c size -m [size in gb]<br> | <br>$ # Assuming an initial xdisk allocation size of 200 gb<br>$ xdisk -c size -m 200<br>XDISK on host: ericidle.hpc.arizona.edu<br>Your resize to 400GB was successful<br>$ xdisk -c size -m -100<br>XDISK on host: ericidle.hpc.arizona.edu<br>Your resize to 300GB was successful<br> | | Delete an xdisk Allocation | <br>xdisk -c delete<br> | <br>$ xdisk -c delete`<br>Your delete request has been processed<br> | Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/storage/hpc_storage/",
      "title": "HPC Storage - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "801af64c-9a39-4674-a69f-074b81453595",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/storage/hpc_storage/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Rclone ¶ Rclone is a CLI tool installed on filexfer.hpc.arizona.edu that can be used to transfer files between HPC and Cloud-based storage sites. To use Rclone, you will need to start by configuring it. Rclone's configuration process is fairly straightforward and has a large number of options to choose from. Some configuration examples are shown below. Newer Version Available As of December 12, 2024 a newer version of rclone is available on the file transfer nodes. In the default environment: ``` [user@sdmz-dtn-3 ~]$ rclone --version rclone v1.58.1 ``` The command to activate the new environment is optrclone : ``` [user@sdmz-dtn-3 ~]$ optrclone Selecting version 1.68.2 [user@sdmz-dtn-3 ~]$ rclone --version rclone v1.68.2 ``` Exiting the new environment: ``` [user@sdmz-dtn-3 ~]$ exit [user@sdmz-dtn-3 ~]$ rclone --version rclone v1.58.1 ``` Configuring Rclone ¶ To configure Rclone, you'll first want to be in an OnDemand Desktop session . This will give you access to both a terminal session and browser that can be used for granting Rclone access to your target cloud service. To configure Rclone, open a MATE terminal and type rclone config some examples for configuring rclone for common cloud services are shown below. Some text has been removed for brevity. If there is nothing included after the > rclone prompt, then the option was left empty before hitting Enter . When entries are left blank, Rclone's default options will be used. Verify your input The instructions below were written on 9/6/2024. It's possible some Rclone configuration options have been updated, so verify the any options you enter match the correct cloud provider and desired configuration options. Google Drive Box ``` [netid@sdmz-dtn-4 ~]$ rclone config Name Type ==== ==== e) Edit existing remote n) New remote d) Delete remote r) Rename remote c) Copy remote s) Set configuration password q) Quit config e/n/d/r/c/s/q> n name> GoogleDrive Type of storage to configure. Enter a string value. Press Enter for the default (\"\"). Choose a number from below, or type in your own value ... 17 / Google Drive \\ \"drive\" ... Storage> 17 Google Application Client Id client_id> client_secret> scope> 1 root_folder_id> service_account_file> Edit advanced config? (y/n) y) Yes n) No y/n> n ``` The next prompt will ask if you would like to use auto config, select y . This will open a Firefox browser prompting you to log into your Google Drive account. Once you've successfully logged in, authorize rclone for access. You will then return to your terminal: ``` Remote config Use auto config? * Say Y if not sure * Say N if you are working on a remote or headless machine y) Yes n) No y/n> y If your browser doesn't open automatically go to the following link: Log in and authorize rclone for access ... Configure this as a team drive? y) Yes n) No y/n> n [GoogleDrive] type = drive scope = drive token = {\"access_token\": } y) Yes this is OK e) Edit this remote d) Delete this remote y/e/d> y Current remotes: Name Type ==== ==== GoogleDrive drive ``` ``` [netid@sdmz-dtn-4 ~]$ rclone config Name Type ==== ==== e) Edit existing remote n) New remote d) Delete remote r) Rename remote c) Copy remote s) Set configuration password q) Quit config e/n/d/r/c/s/q> n name> Box Storage> 8 client_id> client_secret> box_config_file> access_token> box_sub_type> 1 Edit advanced config? y/n> n Use auto config? * Say Y if not sure * Say N if you are working on a remote or headless machine y) Yes (default) n) No y/n> y ``` This will open a Firefox browser. To sign in, click the SSO option and sign in with your university credentials (or log into your personal account), then authorize rclone so it can access Box. Once you're done, back in the terminal: ``` y) Yes this is OK (default) e) Edit this remote d) Delete this remote y/e/d> y /n/d/r/c/s/q> q ``` Rclone Transfers ¶ The syntax to initiate file transfers will typically look something like ``` rclone copyto ``` When <source> and/or <destination> are a cloud service, the format will be: ``` :/path/to/file/or/directory ``` Where <Name> is the name you gave for the service during the configuration process. For example, if I were to access my University Box endpoint (configured in the example in the previous section), I could check its contents using rclone lsf with: ``` [netid@sdmz-dtn-4 ~]$ rclone lsf Box: HPC Documentation Site/ IntroToViz.pptx Photos/ RcloneExample/ [netid@sdmz-dtn-4 ~]$ rclone lsf Box:RcloneExample example.txt ``` If I wanted to transfer the file example.txt to my working directory on HPC: ``` [netid@sdmz-dtn-4 ~]$ rclone copyto Box:RcloneExample/example.txt ./example.txt [netid@sdmz-dtn-4 ~]$ ls -l example.txt -rw-r-----. 1 netid staff 4 Sep 5 14:14 example.txt ``` More detailed information on using Rclone can be found in their documentation https://rclone.org/commands/ . Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/transfers/rclone/",
      "title": "Rclone - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "9a413658-9c8f-4800-888b-4a0b70cd7df0",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/transfers/rclone/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content SFTP/FTP/LFTP ¶ Tip To use SFTP/LFTP, you will need to be connected to our file transfer nodes, hostname: filexfer.hpc.arizona.edu . SFTP ¶ SFTP (Secure File Transfer Protocol) ensures data encryption during transmission over the network. It offers features such as resuming interrupted transfers, directory listing, and remote file management. To perform file transfers using SFTP, you'll need an SSH v2 compliant terminal. To connect to HPC's data transfer nodes, run: ``` sftp your_netid@filexfer.hpc.arizona.edu ``` You will then be able to move files between your machine and HPC using get and put commands. For example: ``` sftp> get /path/to/remote/file /path/to/local/directory ### Retrieves file from HPC. Omitting paths will default to working directories. sftp> put /path/to/local/file /path/to/remote/directory ### Uploads a file from your local computer to HPC. Omitting paths will default to working directories. sftp> help ### prints detailed sftp usage ``` FTP/LFTP ¶ Warning Due to security risks, it is not possible to FTP to the file transfer node from a remote machine, however, you may FTP from the file transfer node to a remote machine. HPC uses the FTP client LFTP to transfer files between the file transfer node and remote machines. This can be done using get and put commands. To use lftp, you must first connect to our file transfer node using an SSH v2 compliant terminal: ``` ssh your_netid@filexfer.hpc.arizona.edu ``` Once connected, you may connect to the external host using the command lftp . For example: ``` lftp ftp.hostname.gov ``` You will then be able to move files between HPC and the remote host using get and put commands. For example: ``` get /path/to/remote/file /path/to/local/directory ### retrieves file from remote host put /path/to/local/file /path/to/remote/directory ### Uploads file from HPC to remote host ``` Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/transfers/sftp_ftp_lftp/",
      "title": "FTP/LFTP/SFTP - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "a0747fa8-6ea2-4b7d-b35c-f15c82fb911d",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/transfers/sftp_ftp_lftp/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Rental Storage ¶ Overview ¶ Accessibility Your /rental allocation is only mounted on our Data Transfer Nodes (hostname filexfer.hpc.arizona.edu ) and is not directly accessible from the HPC login or compute nodes. No Controlled Data This service is not intended for HIPAA or otherwise controlled data. Please see Secure HPC for more information. We offer a rental storage solution that has less performance than our primary SSD array making it affordable for researchers to rent. This storage array is located in the Research Data Center and is mounted on our data transfer nodes which makes it more accessible than most other options. Data in your rental space will be accessible via the command line and the graphical transfer application Globus. Pricing ¶ Cost per Year ¶ The first-year rate is $94.50 per TB, and RII will provide matching funds for first-year allocations to make the actual first-year cost to researchers $47.35. These matching funds will be applied automatically, so in practice you will see the $47.35 rate. The ongoing rate after year one is $47.35 per TB per year. Billing ¶ Researchers must provide a KFS account for this service. Charges will be applied at the end of the academic year (June). Size Modifications ¶ If the size of your allocation is modified, you will be billed for the maximum amount of space reserved during that fiscal year. Data Location ¶ Danger Your /rental allocation is only mounted on our Data Transfer Nodes and is not directly accessible from the HPC login or compute nodes. Your rental space will be on a storage array in our Research Data Center and mounted on our data transfer nodes (hostname: filexfer.hpc.arizona.edu ). Your space will be findable under ``` /rental/ ``` Where <pi_netid> is the NetID of the faculty member who requested the allocation. Data Transfers ¶ A few data transfer options are Globus, sftp , and scp which will allow you to move data external to the data center to your allocation. For data transfers between HPC storage ( /home , /groups , or /xdisk ) and your rental allocation, you may also ssh into filexfer.hpc.arizona.edu and use mv or cp . For large copies done using this method, we recommend using a screen session to prevent timeouts. For example: ``` [netid@home ~]$ ssh netid@filexfer.hpc.arizona.edu Authorized uses only. All activity may be monitored and reported. Last login: Fri Sep 15 10:53:27 2023 [netid@sdmz-dtn-3 ~]$ cd /rental/pi/netid/example [netid@sdmz-dtn-3 example]$ screen [netid@sdmz-dtn-3 example]$ cp -r /xdisk/pi/CONTAINERS/ $PWD/CONTAINERS [netid@sdmz-dtn-3 example]$ ls CONTAINERS [netid@sdmz-dtn-3 example]$ exit # exits screen session [netid@sdmz-dtn-3 example]$ exit # exits filexfer node logout Come again soon! Connection to filexfer.hpc.arizona.edu closed. [netid@home ~]$ ``` How to Request Rental Storage ¶ Warning Allocations up to 20TB in size can be requested through the user portal. For allocations larger than 20TB, contact our consulting team for help. Tip It can take a few days to process the request as it has to route through the Financial Services Office (FSO). You will receive an email confirmation once it is complete. PIs or Group Delegates can request rental storage on behalf of their group. To do so, navigate to the User Portal in your browser, choose the Storage tab, then select Submit Rental Storage Request under the Rental Storage heading. This will open a web form. Add your KFS number under KFS Number and the email address for the Department's financial contact under Business contact email . There will also be two optional fields: Subaccount and Project . These are used for tagging/reporting purposes in KFS billing. You can safely leave these entries blank if you're not sure what they are. Once you have completed the form, click . Once your space has been created, you will receive an email notification that it is ready for use. Resizing Your Allocation ¶ Warning Resizing allocations up to 20TB can be done the user portal. For allocations larger than 20TB, contact our consulting team for help. Your rental allocation can be resized through the user portal by navigating to the Storage tab and selecting Modify Rental Quota under the Rental Storage heading. Checking Your Usage ¶ You can check your allocation's size and current usage either through the user portal or on the command line. User Portal Command Line In the user portal , navigate to the Storage tab and select Check Rental Quota from under the Rental Storage heading. This option is only available to PIs and group delegates. From an HPC login node, enter the command uquota , for example: ``` [user@local_machine ~]$$ ssh netid@hpc.arizona.edu [netid@gatekeeper ~]$ shell (puma) [netid@wentletrap ~]$ uquota used soft limit hard limit /groups/pi 163.4G 500.0G 500.0G /home 13.2G 50.0G 50.0G /rental/pi 11.8G 931.3G 931.3G /xdisk/pi 9.0T 9.9T 9.9T ``` Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/storage/rental_storage/",
      "title": "Rental Storage - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "ab302dbd-7b27-4e59-8fac-86d69f7642f8",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/storage/rental_storage/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content iRODS ¶ CyVerse Support If you are looking for information on how to connect to CyVerse's data store, see their iRODS documentation for a guide . Only use on DTNs iRODS should only be used on the data transfer nodes (DTNs) , which are equipped to handle large data transfers. Use of iRODS on the compute nodes may result in system or network issues. iRODS 4 is installed as a standard package on the operating systems of the data transfer nodes. You will need to iinit the first time you use the software (see below). Initializing iRODS ¶ Running iinit for any system using iRODS 4.x, unlike its iRODS 3 counterpart, does not help you set up the environment. Instead, you need to run create_irods_env with suitable options for the iRODS host, zone, username, etc manually. | For this key | Enter this | | --- | --- | | -h | <hostname of iRODS server> | | -p | <port number of iRODS server> (1247 is default) | | -z | <Zone name of iRODS zone> | | -u | <user name on the iRODS server> (may not match your netid) | | -a | <authentication method for the iRODS server> (PAM, native,...) | For example ``` create_irods_env -a native -h someserver.somewhere.net -z MYZONE ``` will suffice to create an appropriate ~/.irods/irods_environment.json file to allow you to run iinit ; we took the default -p 1247 , -u <your NetId> in the above example by omitting -p and -u . You only need to do this step one time; subsequent times you will just run iinit and it will asked for your password. Note create_irods_env will not overwrite or alter an existing ~/.irods/irods_environment.json file. Once the ~/.irods/irods_environment.json file is created properly, you should be able to sign in to the iRODS server your selected using iinit , viz: ``` $ iinit Enter your current ... password: # enter your iRODS server password here ``` At this point you can use other iRods commands such as icp to move files. Examples ¶ Tip In the following examples: my-files-to-transfer/ is the example name of the directory or folder for bulk transfers. my-file-to-transfer.txt is the example name for single file transfers. Any filename may be used for the checkpoint-file . Bulk Files Transfer Example ``` iput -P -b -r -T --retries 3 -X checkpoint-file my-files-to-transfer/ ``` Single Large File Transfer Example ``` iput -P -T --retries 3 --lfrestart checkpoint-lf-file my-file-to-transfer.txt ``` Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/transfers/irods/",
      "title": "iRODS - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "e71cf745-ae2e-4301-9669-580df6d86d60",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/transfers/irods/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content rsync ¶ Rsync is a fast and extraordinarily versatile file copying tool. It synchronizes files and directories between two different locations (or servers). Rsync copies only the differences of files that have actually changed. An important feature of Rsync not found in most similar programs/protocols is that the mirroring takes place with only one transmission in each direction. It can copy or display directory contents and copy files, optionally using compression and recursion. It is similar to SCP in that both a source and a destination must be specified, one of which may be remote. Subdirectory Behavior ¶ How can we recursively transfers all files from the directory /src/directory-name on a local machine into /data/tmp/ on a remote machine? A trailing slash on the source changes the behavior of rsync . The inclusion of the trailing slash avoids creating an additional directory level at the destination. You can think of a trailing / on a source as meaning “copy the contents of this directory” as opposed to “copy the directory by name”, but in both cases the attributes of the containing directory are transferred to the containing directory on the destination. Trailing Slash The below command will copy all of the contents of directory-name into tmp , excluding the parent folder. ``` rsync -ravz computer-name:/src/directory-name/ user@remote.host:/data/tmp --log-file=hpc-user-rsync.log ``` The folder remote.host:/data/tmp will then contain anything it held previously in addition to the subfolders of directory-name . No Trailing Slash On the other hand, the below command will copy directory-name as a parent folder into tmp resulting in a new directory /data/tmp/directory-name . The contents of directory-name will appear exactly as they did on the local machine. ``` rsync -ravz computer-name:src/directory-name user@remote.host:/data/tmp --log-file=hpc-user-rsync.log ``` Note that including computer-name is optional when referring to the local machine. Log files are optional but recommended. Additional Options ¶ | Flag | Meaning | | --- | --- | | -r | Recursive mode; loop through contents of all subfolders | | -a | Archive mode; will preserve time stamps and other metadata | | -v | Increase verbosity | | -z | Compress file data during the transfer | | --log-file=FILE | Log everything done in specified FILE | Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/transfers/rsync/",
      "title": "Rsync - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "e903482b-0002-44a1-9400-17500767835d",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/transfers/rsync/",
      "statusCode": 200
    }
  },
  {
    "processed_text": "Skip to content Open OnDemand ¶ Tip Open OnDemand file transfers are limited to 64 MB. For larger files, see our data transfer overview for more options. A popular method of transferring files to and from the HPC is the Open OnDemand interface, which is accessed through the browser at ood.hpc.arizona.edu . To access the file browser in Open OnDemand, choose your desired share from the Files dropdown menu. From there, you should see a list of folders and files. Click on folders to open them up, or use the file path navigator to ascend the tree. Additional actions can be taken using the button ribbon on the top right. To upload or download files to/from your selected directory, select Upload or Download. To change the root directory, use the links on the left hand side of the screen. When navigating to your group's share within /groups or /xdisk , use the \"Filter\" box to quickly find your folder from the list. Was this page informative? Thanks for your feedback! Thank you for your feedback! Have ideas to improve our site? Please share them in our quick, anonymous feedback form . Your input helps us enhance our documentation! Back to top",
    "metadata": {
      "url": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/transfers/open_on_demand/",
      "title": "Open OnDemand - UArizona HPC Documentation",
      "favicon": {},
      "language": "en",
      "scrapeId": "f69afbcb-59ac-432f-950d-00ef41fbab77",
      "viewport": "width=device-width,initial-scale=1",
      "generator": "mkdocs-1.6.1, mkdocs-material-9.6.4",
      "sourceURL": "https://hpcdocs.hpc.arizona.edu/storage_and_transfers/transfers/open_on_demand/",
      "statusCode": 200
    }
  }
]